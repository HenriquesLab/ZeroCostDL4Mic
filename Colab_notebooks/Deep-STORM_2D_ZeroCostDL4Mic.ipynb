{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep-STORM_2D_ZeroCostDL4Mic.ipynb","provenance":[{"file_id":"169qcwQo-yw15PwoGatXAdBvjs4wt_foD","timestamp":1592147948265},{"file_id":"1gjRCgDORKi_GNBu4QnVCBkSWrfPtqL-E","timestamp":1588525976305},{"file_id":"1DFy6aCi1XAVdjA5KLRZirB2aMZkMFdv-","timestamp":1587998755430},{"file_id":"1NpzigQoXGy3GFdxh4_jvG1PnBfyrcpBs","timestamp":1587569988032},{"file_id":"1jdI540qAfMSQwjnMhoAFkGJH9EbHwNSf","timestamp":1587486196143}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FpCtYevLHfl4","colab_type":"text"},"source":["# **Deep-STORM (2D)**\n","\n","---\n","\n","<font size = 4>Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by [Nehme *et al.* in Optica](https://www.osapublishing.org/optica/abstract.cfm?uri=optica-5-4-458). The architecture used here is a U-Net based network without skip connections. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension).\n","\n","Deep-STORM has **two key advantages**:\n","- SMLM reconstruction at high density of emitters\n","- fast prediction (reconstruction) once the model is trained appropriately, compared to more common multi-emitter fitting processes.\n","\n","\n","---\n","\n","<font size = 4>*Disclaimer*:\n","\n","<font size = 4>This notebook is part of the *Zero-Cost Deep-Learning to Enhance Microscopy* project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories.\n","\n","<font size = 4>This notebook is based on the following paper: \n","\n","<font size = 4>**Deep-STORM: super-resolution single-molecule microscopy by deep learning**, Optica (2018) by *Elias Nehme, Lucien E. Weiss, Tomer Michaeli, and Yoav Shechtman* (https://www.osapublishing.org/optica/abstract.cfm?uri=optica-5-4-458)\n","\n","<font size = 4>And source code found in: https://github.com/EliasNehme/Deep-STORM\n","\n","\n","<font size = 4>**Please also cite this original paper when using or developing this notebook.**"]},{"cell_type":"markdown","metadata":{"id":"wyzTn3IcHq6Y","colab_type":"text"},"source":["# **How to use this notebook?**\n","\n","---\n","\n","<font size = 4>Video describing how to use our notebooks are available on youtube:\n","  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n","  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n","\n","\n","---\n","###**Structure of a notebook**\n","\n","<font size = 4>The notebook contains two types of cell:  \n","\n","<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n","\n","<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n","\n","---\n","###**Table of contents, Code snippets** and **Files**\n","\n","<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n","\n","<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n","\n","<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n","\n","<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here. \n","\n","<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n","\n","<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n","\n","---\n","###**Making changes to the notebook**\n","\n","<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n","\n","<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n","You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."]},{"cell_type":"markdown","metadata":{"id":"bEy4EBXHHyAX","colab_type":"text"},"source":["#**0. Before getting started**\n","---\n","<font size = 4> Deep-STORM is able to train on simulated dataset of SMLM data (see https://www.osapublishing.org/optica/abstract.cfm?uri=optica-5-4-458 for more info). Here, we provide a simulator that will generate training dataset (section 3.1.b). A few parameters will allow you to match the simulation to your experimental data. Similarly to what is described in the paper, simulations obtained from ThunderSTORM can also be loaded here (section 3.1.a).\n","\n","---\n","<font size = 4>**Important note**\n","\n","<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to run predictions using the model that you trained.\n","\n","<font size = 4>- If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n","\n","<font size = 4>- If you only wish to **run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n","---"]},{"cell_type":"markdown","metadata":{"id":"E04mOlG_H5Tz","colab_type":"text"},"source":["# **1. Initialise the Colab session**\n","---"]},{"cell_type":"markdown","metadata":{"id":"F_tjlGzsH-Dn","colab_type":"text"},"source":["\n","## **1.1. Check for GPU access**\n","---\n","\n","By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n","\n","<font size = 4>Go to **Runtime -> Change the Runtime type**\n","\n","<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n","\n","<font size = 4>**Accelator: GPU** *(Graphics processing unit)*\n"]},{"cell_type":"code","metadata":{"id":"gn-LaaNNICqL","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run this cell to check if you have GPU access\n","# %tensorflow_version 1.x\n","\n","import tensorflow as tf\n","if tf.__version__ != '2.2.0':\n","  !pip install tensorflow==2.2.0\n","\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime settings are correct then Google did not allocate GPU to your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi\n","\n","# from tensorflow.python.client import device_lib \n","# device_lib.list_local_devices()\n","\n","# print the tensorflow version\n","print('Tensorflow version is ' + str(tf.__version__))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnP7wM79IKW-","colab_type":"text"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. \n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","metadata":{"id":"1R-7Fo34_gOd","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run this cell to connect your Google Drive to Colab\n","\n","#@markdown * Click on the URL. \n","\n","#@markdown * Sign in your Google Account. \n","\n","#@markdown * Copy the authorization code. \n","\n","#@markdown * Enter the authorization code. \n","\n","#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n","\n","#mounts user's Google Drive to Google Colab.\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jRnQZWSZhArJ","colab_type":"text"},"source":["# **2. Install Deep-STORM and dependencies**\n","---\n"]},{"cell_type":"code","metadata":{"id":"kSrZMo3X_NhO","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Install Deep-STORM and dependencies\n","\n","# %% Model definition + helper functions\n","\n","# Import keras modules and libraries\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Activation, UpSampling2D, Convolution2D, MaxPooling2D, BatchNormalization, Layer\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import optimizers, losses\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from skimage.transform import warp\n","from skimage.transform import SimilarityTransform\n","from skimage.metrics import structural_similarity\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from scipy.signal import fftconvolve\n","\n","# Import common libraries\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy.io as sio\n","from os.path import abspath\n","from sklearn.model_selection import train_test_split\n","from skimage import io\n","import time\n","import os\n","import shutil\n","import csv\n","from PIL import Image \n","from PIL.TiffTags import TAGS\n","from scipy.ndimage import gaussian_filter\n","import math\n","from astropy.visualization import simple_norm\n","from sys import getsizeof\n","\n","# For sliders and dropdown menu, progress bar\n","from ipywidgets import interact\n","import ipywidgets as widgets\n","from tqdm import tqdm\n","\n","# For Multi-threading in simulation\n","from numba import njit, prange\n","\n","\n","# define a function that projects and rescales an image to the range [0,1]\n","def project_01(im):\n","    im = np.squeeze(im)\n","    min_val = im.min()\n","    max_val = im.max()\n","    return (im - min_val)/(max_val - min_val)\n","\n","# normalize image given mean and std\n","def normalize_im(im, dmean, dstd):\n","    im = np.squeeze(im)\n","    im_norm = np.zeros(im.shape,dtype=np.float32)\n","    im_norm = (im - dmean)/dstd\n","    return im_norm\n","\n","# Define the loss history recorder\n","class LossHistory(Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","        \n","#  Define a matlab like gaussian 2D filter\n","def matlab_style_gauss2D(shape=(7,7),sigma=1):\n","    \"\"\" \n","    2D gaussian filter - should give the same result as:\n","    MATLAB's fspecial('gaussian',[shape],[sigma]) \n","    \"\"\"\n","    m,n = [(ss-1.)/2. for ss in shape]\n","    y,x = np.ogrid[-m:m+1,-n:n+1]\n","    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n","    h.astype(dtype=K.floatx())\n","    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n","    sumh = h.sum()\n","    if sumh != 0:\n","        h /= sumh\n","    h = h*2.0\n","    h = h.astype('float32')\n","    return h\n","\n","# Expand the filter dimensions\n","psf_heatmap = matlab_style_gauss2D(shape = (7,7),sigma=1)\n","gfilter = tf.reshape(psf_heatmap, [7, 7, 1, 1])\n","\n","# Combined MSE + L1 loss\n","def L1L2loss(input_shape):\n","    def bump_mse(heatmap_true, spikes_pred):\n","\n","        # generate the heatmap corresponding to the predicted spikes\n","        heatmap_pred = K.conv2d(spikes_pred, gfilter, strides=(1, 1), padding='same')\n","\n","        # heatmaps MSE\n","        loss_heatmaps = losses.mean_squared_error(heatmap_true,heatmap_pred)\n","\n","        # l1 on the predicted spikes\n","        loss_spikes = losses.mean_absolute_error(spikes_pred,tf.zeros(input_shape))\n","        return loss_heatmaps + loss_spikes\n","    return bump_mse\n","\n","# Define the concatenated conv2, batch normalization, and relu block\n","def conv_bn_relu(nb_filter, rk, ck, name):\n","    def f(input):\n","        conv = Convolution2D(nb_filter, kernel_size=(rk, ck), strides=(1,1),\\\n","                               padding=\"same\", use_bias=False,\\\n","                               kernel_initializer=\"Orthogonal\",name='conv-'+name)(input)\n","        conv_norm = BatchNormalization(name='BN-'+name)(conv)\n","        conv_norm_relu = Activation(activation = \"relu\",name='Relu-'+name)(conv_norm)\n","        return conv_norm_relu\n","    return f\n","\n","# Define the model architechture\n","def CNN(input,names):\n","    Features1 = conv_bn_relu(32,3,3,names+'F1')(input)\n","    pool1 = MaxPooling2D(pool_size=(2,2),name=names+'Pool1')(Features1)\n","    Features2 = conv_bn_relu(64,3,3,names+'F2')(pool1)\n","    pool2 = MaxPooling2D(pool_size=(2, 2),name=names+'Pool2')(Features2)\n","    Features3 = conv_bn_relu(128,3,3,names+'F3')(pool2)\n","    pool3 = MaxPooling2D(pool_size=(2, 2),name=names+'Pool3')(Features3)\n","    Features4 = conv_bn_relu(512,3,3,names+'F4')(pool3)\n","    up5 = UpSampling2D(size=(2, 2),name=names+'Upsample1')(Features4)\n","    Features5 = conv_bn_relu(128,3,3,names+'F5')(up5)\n","    up6 = UpSampling2D(size=(2, 2),name=names+'Upsample2')(Features5)\n","    Features6 = conv_bn_relu(64,3,3,names+'F6')(up6)\n","    up7 = UpSampling2D(size=(2, 2),name=names+'Upsample3')(Features6)\n","    Features7 = conv_bn_relu(32,3,3,names+'F7')(up7)\n","    return Features7\n","\n","# Define the Model building for an arbitrary input size\n","def buildModel(input_dim, initial_learning_rate = 0.001):\n","    input_ = Input (shape = (input_dim))\n","    act_ = CNN (input_,'CNN')\n","    density_pred = Convolution2D(1, kernel_size=(1, 1), strides=(1, 1), padding=\"same\",\\\n","                                  activation=\"linear\", use_bias = False,\\\n","                                  kernel_initializer=\"Orthogonal\",name='Prediction')(act_)\n","    model = Model (inputs= input_, outputs=density_pred)\n","    opt = optimizers.Adam(lr = initial_learning_rate)\n","    model.compile(optimizer=opt, loss = L1L2loss(input_dim))\n","    return model\n","\n","\n","# define a function that trains a model for a given data SNR and density\n","def train_model(patches, heatmaps, modelPath, epochs, steps_per_epoch, batch_size, upsampling_factor=8, validation_split = 0.3, initial_learning_rate = 0.001, pretrained_model_path = '', L2_weighting_factor = 100):\n","    \n","    \"\"\"\n","    This function trains a CNN model on the desired training set, given the \n","    upsampled training images and labels generated in MATLAB.\n","    \n","    # Inputs\n","    # TO UPDATE ----------\n","\n","    # Outputs\n","    function saves the weights of the trained model to a hdf5, and the \n","    normalization factors to a mat file. These will be loaded later for testing \n","    the model in test_model.    \n","    \"\"\"\n","    \n","    # for reproducibility\n","    np.random.seed(123)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(patches, heatmaps, test_size = validation_split, random_state=42)\n","    print('Number of training examples: %d' % X_train.shape[0])\n","    print('Number of validation examples: %d' % X_test.shape[0])\n","       \n","    # Setting type\n","    X_train = X_train.astype('float32')\n","    X_test = X_test.astype('float32')\n","    y_train = y_train.astype('float32')\n","    y_test = y_test.astype('float32')\n","\n","    \n","    #===================== Training set normalization ==========================\n","    # normalize training images to be in the range [0,1] and calculate the \n","    # training set mean and std\n","    mean_train = np.zeros(X_train.shape[0],dtype=np.float32)\n","    std_train = np.zeros(X_train.shape[0], dtype=np.float32)\n","    for i in range(X_train.shape[0]):\n","        X_train[i, :, :] = project_01(X_train[i, :, :])\n","        mean_train[i] = X_train[i, :, :].mean()\n","        std_train[i] = X_train[i, :, :].std()\n","\n","    # resulting normalized training images\n","    mean_val_train = mean_train.mean()\n","    std_val_train = std_train.mean()\n","    X_train_norm = np.zeros(X_train.shape, dtype=np.float32)\n","    for i in range(X_train.shape[0]):\n","        X_train_norm[i, :, :] = normalize_im(X_train[i, :, :], mean_val_train, std_val_train)\n","    \n","    # patch size\n","    psize = X_train_norm.shape[1]\n","\n","    # Reshaping\n","    X_train_norm = X_train_norm.reshape(X_train.shape[0], psize, psize, 1)\n","\n","    # ===================== Test set normalization ==========================\n","    # normalize test images to be in the range [0,1] and calculate the test set \n","    # mean and std\n","    mean_test = np.zeros(X_test.shape[0],dtype=np.float32)\n","    std_test = np.zeros(X_test.shape[0], dtype=np.float32)\n","    for i in range(X_test.shape[0]):\n","        X_test[i, :, :] = project_01(X_test[i, :, :])\n","        mean_test[i] = X_test[i, :, :].mean()\n","        std_test[i] = X_test[i, :, :].std()\n","\n","    # resulting normalized test images\n","    mean_val_test = mean_test.mean()\n","    std_val_test = std_test.mean()\n","    X_test_norm = np.zeros(X_test.shape, dtype=np.float32)\n","    for i in range(X_test.shape[0]):\n","        X_test_norm[i, :, :] = normalize_im(X_test[i, :, :], mean_val_test, std_val_test)\n","        \n","    # Reshaping\n","    X_test_norm = X_test_norm.reshape(X_test.shape[0], psize, psize, 1)\n","\n","    # Reshaping labels\n","    Y_train = y_train.reshape(y_train.shape[0], psize, psize, 1)\n","    Y_test = y_test.reshape(y_test.shape[0], psize, psize, 1)\n","\n","    # Save datasets to a matfile to open later in matlab\n","    mdict = {\"mean_test\": mean_val_test, \"std_test\": std_val_test, \"upsampling_factor\": upsampling_factor, \"Normalization factor\": L2_weighting_factor}\n","    sio.savemat(os.path.join(modelPath,\"model_metadata.mat\"), mdict)\n","\n","\n","    # Set the dimensions ordering according to tensorflow consensous\n","    # K.set_image_dim_ordering('tf')\n","    K.set_image_data_format('channels_last')\n","\n","    # Save the model weights after each epoch if the validation loss decreased\n","    checkpointer = ModelCheckpoint(filepath=os.path.join(modelPath,\"weights_best.hdf5\"), verbose=1,\n","                                   save_best_only=True)\n","\n","    # Change learning when loss reaches a plataeu\n","    change_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00005)\n","    \n","    # Model building and complitation\n","    model = buildModel((psize, psize, 1), initial_learning_rate = initial_learning_rate)\n","    model.summary()\n","\n","    # Load pretrained model\n","    if not pretrained_model_path:\n","      print('Using random initial model weights.')\n","    else:\n","      print('Loading model weights from '+pretrained_model_path)\n","      model.load_weights(pretrained_model_path)\n","    \n","    # Create an image data generator for real time data augmentation\n","    datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range=0.,  # randomly rotate images in the range (degrees, 0 to 180)\n","        width_shift_range=0.,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.,  # randomly shift images vertically (fraction of total height)\n","        zoom_range=0.,\n","        shear_range=0.,\n","        horizontal_flip=False,  # randomly flip images\n","        vertical_flip=False,  # randomly flip images\n","        fill_mode='constant',\n","        data_format=K.image_data_format())\n","\n","    # Fit the image generator on the training data\n","    datagen.fit(X_train_norm)\n","    \n","    # loss history recorder\n","    history = LossHistory()\n","\n","    # Inform user training begun\n","    print('-------------------------------')\n","    print('Training model...')\n","\n","    # Fit model on the batches generated by datagen.flow()\n","    train_history = model.fit_generator(datagen.flow(X_train_norm, Y_train, batch_size=batch_size), \n","                                        steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1, \n","                                        validation_data=(X_test_norm, Y_test), \n","                                        callbacks=[history, checkpointer, change_lr])    \n","\n","    # Inform user training ended\n","    print('-------------------------------')\n","    print('Training Complete!')\n","    \n","    # Save the last model\n","    model.save(os.path.join(modelPath, 'weights_last.hdf5'))\n","\n","    # convert the history.history dict to a pandas DataFrame:     \n","    lossData = pd.DataFrame(train_history.history) \n","\n","    if os.path.exists(os.path.join(modelPath,\"Quality Control\")):\n","      shutil.rmtree(os.path.join(modelPath,\"Quality Control\"))\n","\n","    os.makedirs(os.path.join(modelPath,\"Quality Control\"))\n","\n","    # The training evaluation.csv is saved (overwrites the Files if needed). \n","    lossDataCSVpath = os.path.join(modelPath,\"Quality Control/training_evaluation.csv\")\n","    with open(lossDataCSVpath, 'w') as f:\n","      writer = csv.writer(f)\n","      writer.writerow(['loss','val_loss','learning rate'])\n","      for i in range(len(train_history.history['loss'])):\n","        writer.writerow([train_history.history['loss'][i], train_history.history['val_loss'][i], train_history.history['lr'][i]])\n","\n","    return\n","\n","\n","# Normalization functions from Martin Weigert used in CARE\n","def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n","    \"\"\"This function is adapted from Martin Weigert\"\"\"\n","    \"\"\"Percentile-based image normalization.\"\"\"\n","\n","    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n","    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n","    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n","\n","\n","def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n","    \"\"\"This function is adapted from Martin Weigert\"\"\"\n","    if dtype is not None:\n","        x   = x.astype(dtype,copy=False)\n","        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n","        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n","        eps = dtype(eps)\n","\n","    try:\n","        import numexpr\n","        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n","    except ImportError:\n","        x =                   (x - mi) / ( ma - mi + eps )\n","\n","    if clip:\n","        x = np.clip(x,0,1)\n","\n","    return x\n","\n","def norm_minmse(gt, x, normalize_gt=True):\n","    \"\"\"This function is adapted from Martin Weigert\"\"\"\n","\n","    \"\"\"\n","    normalizes and affinely scales an image pair such that the MSE is minimized  \n","     \n","    Parameters\n","    ----------\n","    gt: ndarray\n","        the ground truth image      \n","    x: ndarray\n","        the image that will be affinely scaled \n","    normalize_gt: bool\n","        set to True of gt image should be normalized (default)\n","    Returns\n","    -------\n","    gt_scaled, x_scaled \n","    \"\"\"\n","    if normalize_gt:\n","        gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n","    x = x.astype(np.float32, copy=False) - np.mean(x)\n","    #x = x - np.mean(x)\n","    gt = gt.astype(np.float32, copy=False) - np.mean(gt)\n","    #gt = gt - np.mean(gt)\n","    scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n","    return gt, scale * x\n","\n","\n","# Multi-threaded Erf-based image construction\n","@njit(parallel=True)\n","def FromLoc2Image_Erf(xc_array, yc_array, photon_array, sigma_array, image_size = (64,64), pixel_size = 100):\n","  w = image_size[0]\n","  h = image_size[1]\n","  erfImage = np.zeros((w, h))\n","  for ij in prange(w*h):\n","    j = int(ij/w)\n","    i = ij - j*w\n","    for (xc, yc, photon, sigma) in zip(xc_array, yc_array, photon_array, sigma_array):\n","      # Don't bother if the emitter has photons <= 0 or if Sigma <= 0\n","      if (sigma > 0) and (photon > 0):\n","        S = sigma*math.sqrt(2)\n","        x = i*pixel_size - xc\n","        y = j*pixel_size - yc\n","        # Don't bother if the emitter is further than 4 sigma from the centre of the pixel\n","        if (x+pixel_size/2)**2 + (y+pixel_size/2)**2 < 16*sigma**2:\n","          ErfX = math.erf((x+pixel_size)/S) - math.erf(x/S)\n","          ErfY = math.erf((y+pixel_size)/S) - math.erf(y/S)\n","          erfImage[j][i] += 0.25*photon*ErfX*ErfY\n","  return erfImage\n","\n","\n","@njit(parallel=True)\n","def FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = (64,64), pixel_size = 100):\n","  w = image_size[0]\n","  h = image_size[1]\n","  locImage = np.zeros((image_size[0],image_size[1]) )\n","  n_locs = len(xc_array)\n","\n","  for e in prange(n_locs):\n","    locImage[int(max(min(round(yc_array[e]/pixel_size),w-1),0))][int(max(min(round(xc_array[e]/pixel_size),h-1),0))] += 1\n","\n","  return locImage\n","\n","\n","\n","def getPixelSizeTIFFmetadata(TIFFpath, display=False):\n","  with Image.open(TIFFpath) as img:\n","    meta_dict = {TAGS[key] : img.tag[key] for key in img.tag.keys()}\n","\n","\n","  # TIFF tags\n","  # https://www.loc.gov/preservation/digital/formats/content/tiff_tags.shtml\n","  # https://www.awaresystems.be/imaging/tiff/tifftags/resolutionunit.html\n","  ResolutionUnit = meta_dict['ResolutionUnit'][0] # unit of resolution\n","  width = meta_dict['ImageWidth'][0]\n","  height = meta_dict['ImageLength'][0]\n","\n","  xResolution = meta_dict['XResolution'][0] # number of pixels / ResolutionUnit\n","\n","  if len(xResolution) == 1:\n","    xResolution = xResolution[0]\n","  elif len(xResolution) == 2:\n","    xResolution = xResolution[0]/xResolution[1]\n","  else:\n","    print('Image resolution not defined.')\n","    xResolution = 1\n","\n","  if ResolutionUnit == 2:\n","    # Units given are in inches\n","    pixel_size = 0.025*1e9/xResolution\n","  elif ResolutionUnit == 3:\n","    # Units given are in cm\n","    pixel_size = 0.01*1e9/xResolution\n","  else: \n","    # ResolutionUnit is therefore 1\n","    print('Resolution unit not defined. Assuming: um')\n","    pixel_size = 1e3/xResolution\n","\n","  if display:\n","    print('Pixel size obtained from metadata: '+str(pixel_size)+' nm')\n","    print('Image size: '+str(width)+'x'+str(height))\n","  \n","  return (pixel_size, width, height)\n","\n","\n","def saveAsTIF(path, filename, array, pixel_size):\n","  \"\"\"\n","  Image saving using PIL to save as .tif format\n","  # Input \n","  path       - path where it will be saved\n","  filename   - name of the file to save (no extension)\n","  array      - numpy array conatining the data at the required format\n","  pixel_size - physical size of pixels in nanometers (identical for x and y)\n","  \"\"\"\n","\n","  # print('Data type: '+str(array.dtype))\n","  if (array.dtype == np.uint16):\n","    mode = 'I;16'\n","  elif (array.dtype == np.uint32):\n","    mode = 'I'\n","  else:\n","    mode = 'F'\n","\n","  # Rounding the pixel size to the nearest number that divides exactly 1cm.\n","  # Resolution needs to be a rational number --> see TIFF format\n","  # pixel_size = 10000/(round(10000/pixel_size))\n","\n","  if len(array.shape) == 2:\n","    im = Image.fromarray(array)\n","    im.save(os.path.join(path, filename+'.tif'),\n","                  mode = mode,  \n","                  resolution_unit = 3,\n","                  resolution = 0.01*1e9/pixel_size)\n","\n","\n","  elif len(array.shape) == 3:\n","    imlist = []\n","    for frame in array:\n","      imlist.append(Image.fromarray(frame))\n","\n","    imlist[0].save(os.path.join(path, filename+'.tif'), save_all=True,\n","                  append_images=imlist[1:],\n","                  mode = mode,  \n","                  resolution_unit = 3,\n","                  resolution = 0.01*1e9/pixel_size)\n","\n","  return\n","\n","\n","\n","\n","class Maximafinder(Layer):\n","    def __init__(self, thresh, neighborhood_size, use_local_avg, **kwargs):\n","        super(Maximafinder, self).__init__(**kwargs)\n","        self.thresh = tf.constant(thresh, dtype=tf.float32)\n","        self.nhood = neighborhood_size\n","        self.use_local_avg = use_local_avg\n","\n","    def build(self, input_shape):\n","        if self.use_local_avg is True:\n","          self.kernel_x = tf.reshape(tf.constant([[-1,0,1],[-1,0,1],[-1,0,1]], dtype=tf.float32), [3, 3, 1, 1])\n","          self.kernel_y = tf.reshape(tf.constant([[-1,-1,-1],[0,0,0],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n","          self.kernel_sum = tf.reshape(tf.constant([[1,1,1],[1,1,1],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n","\n","    def call(self, inputs):\n","\n","        # local maxima positions\n","        max_pool_image = MaxPooling2D(pool_size=(self.nhood,self.nhood), strides=(1,1), padding='same')(inputs)\n","        cond = tf.math.greater(max_pool_image, self.thresh) & tf.math.equal(max_pool_image, inputs)\n","        indices = tf.where(cond)\n","        bind, xind, yind = indices[:, 0], indices[:, 2], indices[:, 1]\n","        confidence = tf.gather_nd(inputs, indices)\n","\n","        # local CoG estimator\n","        if self.use_local_avg:\n","          x_image = K.conv2d(inputs, self.kernel_x, padding='same')\n","          y_image = K.conv2d(inputs, self.kernel_y, padding='same')\n","          sum_image = K.conv2d(inputs, self.kernel_sum, padding='same')\n","          confidence = tf.cast(tf.gather_nd(sum_image, indices), dtype=tf.float32)\n","          x_local = tf.math.divide(tf.gather_nd(x_image, indices),tf.gather_nd(sum_image, indices))\n","          y_local = tf.math.divide(tf.gather_nd(y_image, indices),tf.gather_nd(sum_image, indices))\n","          xind = tf.cast(xind, dtype=tf.float32) + tf.cast(x_local, dtype=tf.float32)\n","          yind = tf.cast(yind, dtype=tf.float32) + tf.cast(y_local, dtype=tf.float32)\n","        else:\n","          xind = tf.cast(xind, dtype=tf.float32)\n","          yind = tf.cast(yind, dtype=tf.float32)\n","        \n","        return bind, xind, yind, confidence\n","\n","    def get_config(self):\n","\n","        # Implement get_config to enable serialization. This is optional.\n","        base_config = super(Maximafinder, self).get_config()\n","        config = {}\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","\n","# ------------------------------- Prediction with postprocessing  function-------------------------------\n","def batchFramePredictionLocalization(dataPath, filename, modelPath, savePath, batch_size=1, thresh=0.1, neighborhood_size=3, use_local_avg = False, pixel_size = None):\n","    \"\"\"\n","    This function tests a trained model on the desired test set, given the \n","    tiff stack of test images, learned weights, and normalization factors.\n","    \n","    # Inputs\n","    dataPath          - the path to the folder containing the tiff stack(s) to run prediction on \n","    filename          - the name of the file to process\n","    modelPath         - the path to the folder containing the weights file and the mean and standard deviation file generated in train_model\n","    savePath          - the path to the folder where to save the prediction\n","    batch_size.       - the number of frames to predict on for each iteration\n","    thresh            - threshoold percentage from the maximum of the gaussian scaling\n","    neighborhood_size - the size of the neighborhood for local maxima finding\n","    use_local_average - Boolean whether to perform local averaging or not\n","    \"\"\"\n","    \n","    # load mean and std\n","    matfile = sio.loadmat(os.path.join(modelPath,'model_metadata.mat'))\n","    test_mean = np.array(matfile['mean_test'])\n","    test_std = np.array(matfile['std_test'])  \n","    upsampling_factor = np.array(matfile['upsampling_factor'])\n","    upsampling_factor = upsampling_factor.item() # convert to scalar\n","    L2_weighting_factor = np.array(matfile['Normalization factor'])\n","    L2_weighting_factor = L2_weighting_factor.item() # convert to scalar\n","\n","    # Read in the raw file\n","    Images = io.imread(os.path.join(dataPath, filename))\n","    if pixel_size == None:\n","      pixel_size, _, _ = getPixelSizeTIFFmetadata(os.path.join(dataPath, filename), display=True)\n","    pixel_size_hr = pixel_size/upsampling_factor\n","\n","    # get dataset dimensions\n","    (nFrames, M, N) = Images.shape\n","    print('Input image is '+str(N)+'x'+str(M)+' with '+str(nFrames)+' frames.')\n","\n","    # Build the model for a bigger image\n","    model = buildModel((upsampling_factor*M, upsampling_factor*N, 1))\n","\n","    # Load the trained weights\n","    model.load_weights(os.path.join(modelPath,'weights_best.hdf5'))\n","\n","    # add a post-processing module\n","    max_layer = Maximafinder(thresh*L2_weighting_factor, neighborhood_size, use_local_avg)\n","\n","    # Initialise the results: lists will be used to collect all the localizations\n","    frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n","\n","    # Initialise the results\n","    Prediction = np.zeros((M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n","    Widefield = np.zeros((M, N), dtype=np.float32)\n","\n","    # run model in batches\n","    n_batches = math.ceil(nFrames/batch_size)\n","    for b in tqdm(range(n_batches)):\n","\n","      nF = min(batch_size, nFrames - b*batch_size)\n","      Images_norm = np.zeros((nF, M, N),dtype=np.float32)\n","      Images_upsampled = np.zeros((nF, M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n","\n","      # Upsampling using a simple nearest neighbor interp and calculating - MULTI-THREAD this?\n","      for f in range(nF):\n","        Images_norm[f,:,:] = project_01(Images[b*batch_size+f,:,:])\n","        Images_norm[f,:,:] = normalize_im(Images_norm[f,:,:], test_mean, test_std)\n","        Images_upsampled[f,:,:] = np.kron(Images_norm[f,:,:], np.ones((upsampling_factor,upsampling_factor)))\n","        Widefield += Images[b*batch_size+f,:,:]\n","\n","      # Reshaping\n","      Images_upsampled = np.expand_dims(Images_upsampled,axis=3)\n","\n","      # Run prediction and local amxima finding\n","      predicted_density = model.predict_on_batch(Images_upsampled)\n","      predicted_density[predicted_density < 0] = 0\n","      Prediction += predicted_density.sum(axis = 3).sum(axis = 0)\n","\n","      bind, xind, yind, confidence = max_layer(predicted_density)\n","      \n","      # normalizing the confidence by the L2_weighting_factor\n","      confidence /= L2_weighting_factor \n","\n","      # turn indices to nms and append to the results\n","      xind, yind = xind*pixel_size_hr, yind*pixel_size_hr\n","      frmind = (bind.numpy() + b*batch_size + 1).tolist()\n","      xind = xind.numpy().tolist()\n","      yind = yind.numpy().tolist()\n","      confidence = confidence.numpy().tolist()\n","      frame_number_list += frmind\n","      x_nm_list += xind\n","      y_nm_list += yind\n","      confidence_au_list += confidence\n","\n","    # Open and create the csv file that will contain all the localizations\n","    if use_local_avg:\n","      ext = '_avg'\n","    else:\n","      ext = '_max'\n","    with open(os.path.join(savePath, 'Localizations_' + os.path.splitext(filename)[0] + ext + '.csv'), \"w\", newline='') as file:\n","      writer = csv.writer(file)\n","      writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n","      locs = list(zip(frame_number_list, x_nm_list, y_nm_list, confidence_au_list))\n","      writer.writerows(locs)\n","\n","    # Save the prediction and widefield image\n","    Widefield = np.kron(Widefield, np.ones((upsampling_factor,upsampling_factor)))\n","    Widefield = np.float32(Widefield)\n","\n","    # io.imsave(os.path.join(savePath, 'Predicted_'+os.path.splitext(filename)[0]+'.tif'), Prediction)\n","    # io.imsave(os.path.join(savePath, 'Widefield_'+os.path.splitext(filename)[0]+'.tif'), Widefield)\n","\n","    saveAsTIF(savePath, 'Predicted_'+os.path.splitext(filename)[0], Prediction, pixel_size_hr)\n","    saveAsTIF(savePath, 'Widefield_'+os.path.splitext(filename)[0], Widefield, pixel_size_hr)\n","\n","\n","    return\n","\n","\n","# Colors for the warning messages\n","class bcolors:\n","  WARNING = '\\033[31m'\n","  NORMAL = '\\033[0m'  # white (normal)\n","\n","\n","\n","def list_files(directory, extension):\n","  return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n","\n","\n","# @njit(parallel=True)\n","def subPixelMaxLocalization(array, method = 'CoM', patch_size = 3):\n","  xMaxInd, yMaxInd = np.unravel_index(array.argmax(), array.shape, order='C')\n","  centralPatch = XC[(xMaxInd-patch_size):(xMaxInd+patch_size+1),(yMaxInd-patch_size):(yMaxInd+patch_size+1)]\n","\n","  if (method == 'MAX'):\n","    x0 = xMaxInd\n","    y0 = yMaxInd\n","\n","  elif (method == 'CoM'):\n","    x0 = 0\n","    y0 = 0\n","    S = 0\n","    for xy in range(patch_size*patch_size):\n","      y = math.floor(xy/patch_size)\n","      x = xy - y*patch_size\n","      x0 += x*array[x,y]\n","      y0 += y*array[x,y]\n","      S = array[x,y]\n","    \n","    x0 = x0/S - patch_size/2 + xMaxInd\n","    y0 = y0/S - patch_size/2 + yMaxInd\n","  \n","  elif (method == 'Radiality'):\n","    # Not implemented yet\n","    x0 = xMaxInd\n","    y0 = yMaxInd\n","  \n","  return (x0, y0)\n","\n","\n","@njit(parallel=True)\n","def correctDriftLocalization(xc_array, yc_array, frames, xDrift, yDrift):\n","  n_locs = xc_array.shape[0]\n","  xc_array_Corr = np.empty(n_locs)\n","  yc_array_Corr = np.empty(n_locs)\n","  \n","  for loc in prange(n_locs):\n","    xc_array_Corr[loc] = xc_array[loc] - xDrift[frames[loc]]\n","    yc_array_Corr[loc] = yc_array[loc] - yDrift[frames[loc]]\n","\n","  return (xc_array_Corr, yc_array_Corr)\n","\n","\n","print('--------------------------------')\n","print('DeepSTORM installation complete.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vu8f5NGJkJos","colab_type":"text"},"source":["\n","# **3. Generate patches for training**\n","---\n","\n","For Deep-STORM the training data can be obtained in two ways:\n","* Simulated using ThunderSTORM or other simulation tool and loaded here (**using Section 3.1.a**)\n","* Directly simulated in this notebook (**using Section 3.1.b**)\n"]},{"cell_type":"markdown","metadata":{"id":"WSV8xnlynp0l","colab_type":"text"},"source":["## **3.1.a Load training data**\n","---\n","\n","Here you can load your simulated data along with its corresponding localization file.\n","*   The `pixel_size` is defined in nanometer (nm). "]},{"cell_type":"code","metadata":{"id":"CT6SNcfNg6j0","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Load raw data\n","\n","# Get user input\n","ImageData_path = \"\" #@param {type:\"string\"}\n","LocalizationData_path = \"\" #@param {type: \"string\"}\n","#@markdown Get pixel size from file?\n","get_pixel_size_from_file = True #@param {type:\"boolean\"}\n","#@markdown Otherwise, use this value:\n","pixel_size = 100 #@param {type:\"number\"}\n","\n","if get_pixel_size_from_file:\n","  pixel_size,_,_ = getPixelSizeTIFFmetadata(ImageData_path, True)\n","\n","# load the tiff data\n","Images = io.imread(ImageData_path)\n","# get dataset dimensions\n","if len(Images.shape) == 3:\n","  (number_of_frames, M, N) = Images.shape\n","elif len(Images.shape) == 2:\n","  (M, N) = Images.shape\n","  number_of_frames = 1\n","print('Loaded images: '+str(M)+'x'+str(N)+' with '+str(number_of_frames)+' frames')\n","\n","# Interactive display of the stack\n","def scroll_in_time(frame):\n","    f=plt.figure(figsize=(6,6))\n","    plt.imshow(Images[frame-1], interpolation='nearest', cmap = 'gray')\n","    plt.title('Training source at frame = ' + str(frame))\n","    plt.axis('off');\n","\n","if number_of_frames > 1:\n","  interact(scroll_in_time, frame=widgets.IntSlider(min=1, max=Images.shape[0], step=1, value=0, continuous_update=False));\n","else:\n","  f=plt.figure(figsize=(6,6))\n","  plt.imshow(Images, interpolation='nearest', cmap = 'gray')\n","  plt.title('Training source')\n","  plt.axis('off');\n","\n","# Load the localization file and display the first\n","LocData = pd.read_csv(LocalizationData_path, index_col=0)\n","LocData.tail()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9xE5GeYiks9","colab_type":"text"},"source":["## **3.1.b Simulate training data**\n","---\n","This simulation tool allows you to generate SMLM data of randomly distrubuted emitters in a field-of-view. \n","The assumptions are as follows:\n","\n","*   Gaussian Point Spread Function (PSF) with standard deviation defined by `Sigma`. The nominal value of `sigma` can be evaluated using `sigma = 0.21 x Lambda / NA`. \n","*   Each emitter will emit `n_photons` per frame, and generate their equivalent Poisson noise.\n","*   The camera will contribute Gaussian noise to the signal with a standard deviation defined by `ReadOutNoise_ADC` in ADC\n","*   The `emitter_density` is defined as the number of emitters / um^2 on any given frame. Variability in the emitter density can be applied by adjusting `emitter_density_std`. The latter parameter represents the standard deviation of the normal distribution that the density is drawn from for each individual frame. `emitter_density` **is defined in number of emitters / um^2**.\n","*   The `n_photons` and `sigma` can additionally include some Gaussian variability by setting `n_photons_std` and `sigma_std`.\n","\n","Important note:\n","- All dimensions are in nanometer (e.g. `FOV_size` = 6400 represents a field of view of 6.4 um x 6.4 um).\n","\n"]},{"cell_type":"code","metadata":{"id":"sQyLXpEhitsg","colab_type":"code","cellView":"form","colab":{}},"source":["\n","# ---------------------------- User input ----------------------------\n","#@markdown Run the simulation\n","#@markdown --- \n","#@markdown Camera settings: \n","FOV_size =  6400#@param {type:\"number\"}\n","pixel_size =  100#@param {type:\"number\"}\n","ADC_per_photon_conversion = 1 #@param {type:\"number\"}\n","ReadOutNoise_ADC =  4.5#@param {type:\"number\"}\n","ADC_offset =  50#@param {type:\"number\"}\n","\n","#@markdown Acquisition settings: \n","emitter_density =  6#@param {type:\"number\"}\n","emitter_density_std =  0#@param {type:\"number\"}\n","\n","number_of_frames =  20#@param {type:\"integer\"}\n","\n","sigma = 110 #@param {type:\"number\"}\n","sigma_std = 5 #@param {type:\"number\"}\n","# NA =  1.1 #@param {type:\"number\"}\n","# wavelength =  800#@param {type:\"number\"}\n","# wavelength_std =  150#@param {type:\"number\"}\n","n_photons =  2250#@param {type:\"number\"}\n","n_photons_std =  250#@param {type:\"number\"}\n","\n","\n","# ---------------------------- Variable initialisation ----------------------------\n","# Start the clock to measure how long it takes\n","start = time.time()\n","\n","print('-----------------------------------------------------------')\n","n_molecules = emitter_density*FOV_size*FOV_size/10**6\n","n_molecules_std = emitter_density_std*FOV_size*FOV_size/10**6\n","print('Number of molecules / FOV: '+str(round(n_molecules,2))+' +/- '+str((round(n_molecules_std,2))))\n","\n","# sigma = 0.21*wavelength/NA\n","# sigma_std = 0.21*wavelength_std/NA\n","# print('Gaussian PSF sigma: '+str(round(sigma,2))+' +/- '+str(round(sigma_std,2))+' nm')\n","\n","M = N = round(FOV_size/pixel_size)\n","FOV_size = M*pixel_size\n","print('Final image size: '+str(M)+'x'+str(M)+' ('+str(round(FOV_size/1000, 3))+'um x'+str(round(FOV_size/1000,3))+' um)')\n","\n","np.random.seed(1)\n","display_upsampling = 8 # used to display the loc map here\n","NoiseFreeImages = np.zeros((number_of_frames, M, M))\n","locImage = np.zeros((number_of_frames, display_upsampling*M, display_upsampling*N))\n","\n","frames = []\n","all_xloc = []\n","all_yloc = []\n","all_photons = []\n","all_sigmas = []\n","\n","# ---------------------------- Main simulation loop ----------------------------\n","print('-----------------------------------------------------------')\n","for f in tqdm(range(number_of_frames)):\n","  \n","  # Define the coordinates of emitters by randomly distributing them across the FOV\n","  n_mol = int(max(round(np.random.normal(n_molecules, n_molecules_std, size=1)[0]), 0))\n","  x_c = np.random.uniform(low=0.0, high=FOV_size, size=n_mol)\n","  y_c = np.random.uniform(low=0.0, high=FOV_size, size=n_mol)\n","  photon_array = np.random.normal(n_photons, n_photons_std, size=n_mol)\n","  sigma_array = np.random.normal(sigma, sigma_std, size=n_mol)\n","  # x_c = np.linspace(0,3000,5)\n","  # y_c = np.linspace(0,3000,5)\n","\n","  all_xloc += x_c.tolist()\n","  all_yloc += y_c.tolist()\n","  frames += ((f+1)*np.ones(x_c.shape[0])).tolist()\n","  all_photons += photon_array.tolist()\n","  all_sigmas += sigma_array.tolist()\n","\n","  locImage[f] = FromLoc2Image_SimpleHistogram(x_c, y_c, image_size = (N*display_upsampling, M*display_upsampling), pixel_size = pixel_size/display_upsampling)\n","\n","  # # Get the approximated locations according to the grid pixel size\n","  # Chr_emitters = [int(max(min(round(display_upsampling*x_c[i]/pixel_size),N*display_upsampling-1),0)) for i in range(len(x_c))]\n","  # Rhr_emitters = [int(max(min(round(display_upsampling*y_c[i]/pixel_size),M*display_upsampling-1),0)) for i in range(len(y_c))]\n","\n","  # # Build Localization image\n","  # for (r,c) in zip(Rhr_emitters, Chr_emitters):\n","  #   locImage[f][r][c] += 1\n","\n","  NoiseFreeImages[f] = FromLoc2Image_Erf(x_c, y_c, photon_array, sigma_array, image_size = (M,M), pixel_size = pixel_size)\n","\n","\n","# ---------------------------- Create DataFrame fof localization file ----------------------------\n","# Table with localization info as dataframe output\n","LocData = pd.DataFrame()\n","LocData[\"frame\"] = frames\n","LocData[\"x [nm]\"] = all_xloc\n","LocData[\"y [nm]\"] = all_yloc\n","LocData[\"Photon #\"] = all_photons\n","LocData[\"Sigma [nm]\"] = all_sigmas\n","LocData.index += 1  # set indices to start at 1 and not 0 (same as ThunderSTORM)\n","\n","\n","# ---------------------------- Estimation of SNR ----------------------------\n","n_frames_for_SNR = 100\n","M_SNR = 10\n","x_c = np.random.uniform(low=0.0, high=pixel_size*M_SNR, size=n_frames_for_SNR)\n","y_c = np.random.uniform(low=0.0, high=pixel_size*M_SNR, size=n_frames_for_SNR)\n","photon_array = np.random.normal(n_photons, n_photons_std, size=n_frames_for_SNR)\n","sigma_array = np.random.normal(sigma, sigma_std, size=n_frames_for_SNR)\n","\n","SNR = np.zeros(n_frames_for_SNR)\n","for i in range(n_frames_for_SNR):\n","  SingleEmitterImage = FromLoc2Image_Erf(np.array([x_c[i]]), np.array([x_c[i]]), np.array([photon_array[i]]), np.array([sigma_array[i]]), (M_SNR, M_SNR), pixel_size)\n","  Signal_photon = np.max(SingleEmitterImage)\n","  Noise_photon = math.sqrt((ReadOutNoise_ADC/ADC_per_photon_conversion)**2 + Signal_photon)\n","  SNR[i] = Signal_photon/Noise_photon\n","\n","print('SNR: '+str(round(np.mean(SNR),2))+' +/- '+str(round(np.std(SNR),2)))\n","# ---------------------------- ----------------------------\n","\n","\n","# Table with info\n","simParameters = pd.DataFrame()\n","simParameters[\"FOV size (nm)\"] = [FOV_size]\n","simParameters[\"Pixel size (nm)\"] = [pixel_size]\n","simParameters[\"ADC/photon\"] = [ADC_per_photon_conversion]\n","simParameters[\"Read-out noise (ADC)\"] = [ReadOutNoise_ADC]\n","simParameters[\"Constant offset (ADC)\"] = [ADC_offset]\n","\n","simParameters[\"Emitter density (emitters/um^2)\"] = [emitter_density]\n","simParameters[\"STD of emitter density (emitters/um^2)\"] = [emitter_density_std]\n","simParameters[\"Number of frames\"] = [number_of_frames]\n","# simParameters[\"NA\"] = [NA]\n","# simParameters[\"Wavelength (nm)\"] = [wavelength]\n","# simParameters[\"STD of wavelength (nm)\"] = [wavelength_std]\n","simParameters[\"Sigma (nm))\"] = [sigma]\n","simParameters[\"STD of Sigma (nm))\"] = [sigma_std]\n","simParameters[\"Number of photons\"] = [n_photons]\n","simParameters[\"STD of number of photons\"] = [n_photons_std]\n","simParameters[\"SNR\"] = [np.mean(SNR)]\n","simParameters[\"STD of SNR\"] = [np.std(SNR)]\n","\n","\n","# ---------------------------- Finish simulation ----------------------------\n","# Calculating the noisy image\n","Images = ADC_per_photon_conversion * np.random.poisson(NoiseFreeImages) + ReadOutNoise_ADC * np.random.normal(size = (number_of_frames, M, N)) + ADC_offset\n","Images[Images <= 0] = 0\n","\n","# Convert to 16-bit or 32-bits integers\n","if Images.max() < (2**16-1):\n","  Images = Images.astype(np.uint16)\n","else:\n","  Images = Images.astype(np.uint32)\n","\n","\n","# ---------------------------- Display ----------------------------\n","# Displaying the time elapsed for simulation\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds,1),\"sec(s)\")\n","\n","\n","# Interactively display the results using Widgets\n","def scroll_in_time(frame):\n","  f = plt.figure(figsize=(18,6))\n","  plt.subplot(1,3,1)\n","  plt.imshow(locImage[frame-1], interpolation='bilinear', vmin = 0, vmax=0.1)\n","  plt.title('Localization image')\n","  plt.axis('off');\n","\n","  plt.subplot(1,3,2)\n","  plt.imshow(NoiseFreeImages[frame-1], interpolation='nearest', cmap='gray')\n","  plt.title('Noise-free simulation')\n","  plt.axis('off');\n","\n","  plt.subplot(1,3,3)\n","  plt.imshow(Images[frame-1], interpolation='nearest', cmap='gray')\n","  plt.title('Noisy simulation')\n","  plt.axis('off');\n","\n","interact(scroll_in_time, frame=widgets.IntSlider(min=1, max=Images.shape[0], step=1, value=0, continuous_update=False));\n","\n","# Display the head of the dataframe with localizations\n","LocData.tail()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pz7RfSuoeJeq","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ---\n","# @markdown #Play this cell to save the simulated stack\n","# @markdown ####Please select a path to the folder where to save the simulated data. It is not necesary to save the data to run the training, but keeping the simulated for your own record can be useful to check its validity.\n","Save_path = \"\" #@param {type:\"string\"}\n","\n","if not os.path.exists(Save_path):\n","  os.makedirs(Save_path)\n","  print('Folder created.')\n","else:\n","  print('Training data already exists in folder: Data overwritten.')\n","\n","saveAsTIF(Save_path, 'SimulatedDataset', Images, pixel_size)\n","# io.imsave(os.path.join(Save_path, 'SimulatedDataset.tif'),Images)\n","LocData.to_csv(os.path.join(Save_path, 'SimulatedDataset.csv'))\n","simParameters.to_csv(os.path.join(Save_path, 'SimulatedParameters.csv'))\n","print('Training dataset saved.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_8e3kE-JhVY","colab_type":"text"},"source":["## **3.2. Generate training patches**\n","---\n","\n","Training patches need to be created from the training data generated above. \n","*   The `patch_size` needs to give sufficient contextual information and for most cases a `patch_size` of 26 (corresponding to patches of 26x26 pixels) works fine. **DEFAULT: 26**\n","*   The `upsampling_factor` defines the effective magnification of the final super-resolved image compared to the input image (this is called magnification in ThunderSTORM). This is used to generate the super-resolved patches as target dataset. Using an `upsampling_factor` of 16 will require the use of more memory and it may be necessary to decreae the `patch_size` to 16 for example. **DEFAULT: 8**\n","*   The `num_patches_per_frame` defines the number of patches extracted from each frame generated in section 3.1. **DEFAULT: 500**\n","*   The `min_number_of_emitters_per_patch` defines the minimum number of emitters that need to be present in the patch to be a valid patch. An empty patch does not contain useful information for the network to learn from. **DEFAULT: 7**\n","*   The `max_num_patches` defines the maximum number of patches to generate. Fewer may be generated depending on how many pacthes are rejected and how many frames are available. **DEFAULT: 10000**\n","*   The `gaussian_sigma` defines the Gaussian standard deviation (in magnified pixels) applied to generate the super-resolved target image. **DEFAULT: 1**\n","*   The `L2_weighting_factor` is a normalization factor used in the loss function. It helps balancing the loss from the L2 norm. When using higher densities, this factor should be decreased and vice-versa. This factor can be autimatically calculated using an empiraical formula. **DEFAULT: 100**\n","\n"]},{"cell_type":"code","metadata":{"id":"AsNx5KzcFNvC","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ## **Provide patch parameters**\n","\n","\n","# -------------------- User input --------------------\n","patch_size = 26 #@param {type:\"integer\"}\n","upsampling_factor = 8 #@param [\"4\", \"8\", \"16\"] {type:\"raw\"}\n","num_patches_per_frame =  500#@param {type:\"integer\"}\n","min_number_of_emitters_per_patch = 7#@param {type:\"integer\"}\n","max_num_patches =  10000#@param {type:\"integer\"}\n","gaussian_sigma = 1#@param {type:\"integer\"}\n","\n","#@markdown Estimate the optimal normalization factor automatically?\n","Automatic_normalization = True #@param {type:\"boolean\"}\n","#@markdown Otherwise, it will use the following value:\n","L2_weighting_factor = 100 #@param {type:\"number\"}\n","\n","\n","# -------------------- Prepare variables --------------------\n","# Start the clock to measure how long it takes\n","start = time.time()\n","\n","# Initialize some parameters\n","pixel_size_hr = pixel_size/upsampling_factor # in nm\n","n_patches = min(number_of_frames*num_patches_per_frame, max_num_patches)\n","patch_size = patch_size*upsampling_factor\n","\n","# Dimensions of the high-res grid\n","Mhr = upsampling_factor*M # in pixels\n","Nhr = upsampling_factor*N # in pixels\n","\n","# Initialize the training patches and labels\n","patches = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n","spikes = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n","heatmaps = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n","\n","# Run over all frames and construct the training examples\n","k = 1 # current patch count\n","skip_counter = 0 # number of dataset skipped due to low density\n","id_start = 0 # id position in LocData for current frame\n","print('Generating '+str(n_patches)+' patches of '+str(patch_size)+'x'+str(patch_size))\n","\n","n_locs = len(LocData.index)\n","print('Total number of localizations: '+str(n_locs))\n","density = n_locs/(M*N*number_of_frames*(0.001*pixel_size)**2)\n","print('Density: '+str(round(density,2))+' locs/um^2')\n","n_locs_per_patch = patch_size**2*density\n","\n","if Automatic_normalization:\n","  # This empirical formulae attempts to balance the loss L2 function between the background and the bright spikes\n","  # A value of 100 was originally chosen to balance L2 for a patch size of 2.6x2.6^2 0.1um pixel size and density of 3 (hence the 20.28), at upsampling_factor = 8\n","  L2_weighting_factor = 100/math.sqrt(min(n_locs_per_patch, min_number_of_emitters_per_patch)*8**2/(upsampling_factor**2*20.28))\n","  print('Normalization factor: '+str(round(L2_weighting_factor,2)))\n","\n","# -------------------- Patch generation loop --------------------\n","\n","print('-----------------------------------------------------------')\n","for (f, thisFrame) in enumerate(tqdm(Images)):\n","\n","  # Upsample the frame\n","  upsampledFrame = np.kron(thisFrame, np.ones((upsampling_factor,upsampling_factor)))\n","  # Read all the provided high-resolution locations for current frame\n","  DataFrame = LocData[LocData['frame'] == f+1].copy()\n","\n","  # Get the approximated locations according to the high-res grid pixel size\n","  Chr_emitters = [int(max(min(round(DataFrame['x [nm]'][i]/pixel_size_hr),Nhr-1),0)) for i in range(id_start+1,id_start+1+len(DataFrame.index))]\n","  Rhr_emitters = [int(max(min(round(DataFrame['y [nm]'][i]/pixel_size_hr),Mhr-1),0)) for i in range(id_start+1,id_start+1+len(DataFrame.index))]\n","  id_start += len(DataFrame.index)\n","\n","  # Build Localization image\n","  LocImage = np.zeros((Mhr,Nhr))\n","  LocImage[(Rhr_emitters, Chr_emitters)] = 1\n","\n","  # Here, there's a choice between the original Gaussian (classification approach) and using the erf function\n","  HeatMapImage = L2_weighting_factor*gaussian_filter(LocImage, gaussian_sigma)  \n","  # HeatMapImage = L2_weighting_factor*FromLoc2Image_MultiThreaded(np.array(list(DataFrame['x [nm]'])), np.array(list(DataFrame['y [nm]'])), \n","                                                            #  np.ones(len(DataFrame.index)), pixel_size_hr*gaussian_sigma*np.ones(len(DataFrame.index)), \n","                                                            #  Mhr, pixel_size_hr)\n","  \n","\n","  # Generate random position for the top left corner of the patch\n","  xc = np.random.randint(0, Mhr-patch_size, size=num_patches_per_frame)\n","  yc = np.random.randint(0, Nhr-patch_size, size=num_patches_per_frame)\n","\n","  for c in range(len(xc)):\n","    if LocImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size].sum() < min_number_of_emitters_per_patch:\n","      skip_counter += 1\n","      continue\n","    \n","    else:\n","        # Limit maximal number of training examples to 15k\n","      if k > max_num_patches:\n","        break\n","      else:\n","        # Assign the patches to the right part of the images\n","        patches[k-1] = upsampledFrame[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n","        spikes[k-1] = LocImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n","        heatmaps[k-1] = HeatMapImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n","        k += 1 # increment current patch count\n","\n","# Remove the empty data\n","patches = patches[:k-1]\n","spikes = spikes[:k-1]\n","heatmaps = heatmaps[:k-1]\n","n_patches = k-1\n","\n","# -------------------- Failsafe --------------------\n","# Check if the size of the training set is smaller than 5k to notify user to simulate more images using ThunderSTORM\n","if ((k-1) < 5000):\n","  # W  = '\\033[0m'  # white (normal)\n","  # R  = '\\033[31m' # red\n","  print(bcolors.WARNING+'!! WARNING: Training set size is below 5K - Consider simulating more images in ThunderSTORM. !!'+bcolors.NORMAL)\n","\n","\n","\n","# -------------------- Displays --------------------\n","print('Number of patches skipped due to low density: '+str(skip_counter))\n","# dataSize = int((getsizeof(patches)+getsizeof(heatmaps)+getsizeof(spikes))/(1024*1024)) #rounded in MB\n","# print('Size of patches: '+str(dataSize)+' MB')\n","print(str(n_patches)+' patches were generated.')\n","\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n","\n","# Display patches interactively with a slider\n","def scroll_patches(patch):\n","  f = plt.figure(figsize=(16,6))\n","  plt.subplot(1,3,1)\n","  plt.imshow(patches[patch-1], interpolation='nearest', cmap='gray')\n","  plt.title('Raw data (frame #'+str(patch)+')')\n","  plt.axis('off');\n","\n","  plt.subplot(1,3,2)\n","  plt.imshow(heatmaps[patch-1], interpolation='nearest')\n","  plt.title('Heat map')\n","  plt.axis('off');\n","\n","  plt.subplot(1,3,3)\n","  plt.imshow(spikes[patch-1], interpolation='nearest')\n","  plt.title('Localization map')\n","  plt.axis('off');\n","\n","interact(scroll_patches, patch=widgets.IntSlider(min=1, max=patches.shape[0], step=1, value=0, continuous_update=False));\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSjXFMevK7Iz","colab_type":"text"},"source":["# **4. Train the network**\n","---"]},{"cell_type":"markdown","metadata":{"id":"hVeyKU0MdAPx","colab_type":"text"},"source":["## **4.1. Select your paths and parameters**\n","\n","---\n","\n","<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n","\n","<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n","\n","\n","<font size = 5>**Training parameters**\n","\n","<font size = 4>**`number_of_epochs`:**Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10-30) epochs, but a full training should run for ~100 epochs. Evaluate the performance after training (see 5). **Default value: 80**\n","\n","<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 16**\n","\n","<font size = 4>**`number_of_steps`:** Define the number of training steps by epoch. **If this value is set to 0**, by default this parameter is calculated so that each patch is seen at least once per epoch. **Default value: Number of patch / batch_size**\n","\n","<font size = 4>**`percentage_validation`:**  Input the percentage of your training dataset you want to use to validate the network during training. **Default value: 30** \n","\n","<font size = 4>**`initial_learning_rate`:** This parameter represents the initial value to be used as learning rate in the optimizer. **Default value: 0.001**"]},{"cell_type":"code","metadata":{"id":"oa5cDZ7f_PF6","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ###Path to training images and parameters\n","\n","model_path = \"\" #@param {type: \"string\"} \n","model_name = \"\" #@param {type: \"string\"} \n","number_of_epochs =  80#@param {type:\"integer\"}\n","batch_size =  16#@param {type:\"integer\"}\n","\n","number_of_steps =  0#@param {type:\"integer\"}\n","percentage_validation = 30 #@param {type:\"number\"}\n","initial_learning_rate = 0.001 #@param {type:\"number\"}\n","\n","\n","percentage_validation /= 100\n","if number_of_steps == 0: \n","  number_of_steps = int((1-percentage_validation)*n_patches/batch_size)\n","  print('Number of steps: '+str(number_of_steps))\n","\n","# Pretrained model path initialised here so next cell does not need to be run\n","h5_file_path = ''\n","Use_pretrained_model = False\n","\n","if not ('patches' in locals()):\n","  # W  = '\\033[0m'  # white (normal)\n","  # R  = '\\033[31m' # red\n","  print(WARNING+'!! WARNING: No patches were found in memory currently. !!')\n","\n","Save_path = os.path.join(model_path, model_name)\n","if os.path.exists(Save_path):\n","  print(bcolors.WARNING+'The model folder already exists and will be overwritten.'+bcolors.NORMAL)\n","\n","print('-----------------------------')\n","print('Training parameters set.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WIyEvQBWLp9n","colab_type":"text"},"source":["\n","## **4.2. Using weights from a pre-trained model as initial weights**\n","---\n","<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a Deep-STORM 2D model**. \n","\n","<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**.\n","\n","<font size = 4> In order to continue training from the point where the pre-trained model left off, it is adviseable to also **load the learning rate** that was used when the training ended. This is automatically saved for models trained with ZeroCostDL4Mic and will be loaded here. If no learning rate can be found in the model folder provided, the default learning rate will be used. "]},{"cell_type":"code","metadata":{"id":"oHL5g0w8LqR0","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ##Loading weights from a pre-trained network\n","\n","Use_pretrained_model = False #@param {type:\"boolean\"}\n","pretrained_model_choice = \"Model_from_file\" #@param [\"Model_from_file\"]\n","Weights_choice = \"best\" #@param [\"last\", \"best\"]\n","\n","#@markdown ###If you chose \"Model_from_file\", please provide the path to the model folder:\n","pretrained_model_path = \"\" #@param {type:\"string\"}\n","\n","# --------------------- Check if we load a previously trained model ------------------------\n","if Use_pretrained_model:\n","\n","# --------------------- Load the model from the choosen path ------------------------\n","  if pretrained_model_choice == \"Model_from_file\":\n","    h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".hdf5\")\n","\n","# --------------------- Download the a model provided in the XXX ------------------------\n","\n","  if pretrained_model_choice == \"Model_name\":\n","    pretrained_model_name = \"Model_name\"\n","    pretrained_model_path = \"/content/\"+pretrained_model_name\n","    print(\"Downloading the 2D_Demo_Model_from_Stardist_2D_paper\")\n","    if os.path.exists(pretrained_model_path):\n","      shutil.rmtree(pretrained_model_path)\n","    os.makedirs(pretrained_model_path)\n","    wget.download(\"\", pretrained_model_path)\n","    wget.download(\"\", pretrained_model_path)\n","    wget.download(\"\", pretrained_model_path)    \n","    wget.download(\"\", pretrained_model_path)\n","    h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".hdf5\")\n","\n","# --------------------- Add additional pre-trained models here ------------------------\n","\n","\n","\n","# --------------------- Check the model exist ------------------------\n","# If the model path chosen does not contain a pretrain model then use_pretrained_model is disabled, \n","  if not os.path.exists(h5_file_path):\n","    print(bcolors.WARNING+'WARNING: weights_'+Weights_choice+'.hdf5 pretrained model does not exist'+bcolors.NORMAL)\n","    Use_pretrained_model = False\n","\n","  \n","# If the model path contains a pretrain model, we load the training rate, \n","  if os.path.exists(h5_file_path):\n","#Here we check if the learning rate can be loaded from the quality control folder\n","    if os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n","      with open(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv'),'r') as csvfile:\n","        csvRead = pd.read_csv(csvfile, sep=',')\n","        #print(csvRead)\n","        if \"learning rate\" in csvRead.columns: #Here we check that the learning rate column exist (compatibility with model trained un ZeroCostDL4Mic bellow 1.4)\n","          print(\"pretrained network learning rate found\")\n","          #find the last learning rate\n","          lastLearningRate = csvRead[\"learning rate\"].iloc[-1]\n","          #Find the learning rate corresponding to the lowest validation loss\n","          min_val_loss = csvRead[csvRead['val_loss'] == min(csvRead['val_loss'])]\n","          #print(min_val_loss)\n","          bestLearningRate = min_val_loss['learning rate'].iloc[-1]\n","          if Weights_choice == \"last\":\n","            print('Last learning rate: '+str(lastLearningRate))\n","          if Weights_choice == \"best\":\n","            print('Learning rate of best validation loss: '+str(bestLearningRate))\n","        if not \"learning rate\" in csvRead.columns: #if the column does not exist, then initial learning rate is used instead\n","          bestLearningRate = initial_learning_rate\n","          lastLearningRate = initial_learning_rate\n","          print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead.'+bcolors.NORMAL)\n","\n","#Compatibility with models trained outside ZeroCostDL4Mic but default learning rate will be used\n","    if not os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n","      print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(initial_learning_rate)+' will be used instead'+bcolors.NORMAL)\n","      bestLearningRate = initial_learning_rate\n","      lastLearningRate = initial_learning_rate\n","\n","\n","# Display info about the pretrained model to be loaded (or not)\n","if Use_pretrained_model:\n","  print('Weights found in:')\n","  print(h5_file_path)\n","  print('will be loaded prior to training.')\n","\n","else:\n","  print('No pretrained network will be used.')\n","  h5_file_path = ''\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OADNcie-LHxA","colab_type":"text"},"source":["## **4.4. Start Trainning**\n","---\n","<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n","\n","<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches."]},{"cell_type":"code","metadata":{"id":"qDgMu_mAK8US","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Start training\n","\n","# Start the clock to measure how long it takes\n","start = time.time()\n","\n","# --------------------- Using pretrained model ------------------------\n","#Here we ensure that the learning rate set correctly when using pre-trained models\n","if Use_pretrained_model:\n","  if Weights_choice == \"last\":\n","    initial_learning_rate = lastLearningRate\n","\n","  if Weights_choice == \"best\":            \n","    initial_learning_rate = bestLearningRate\n","# --------------------- ---------------------- ------------------------\n","\n","\n","#here we check that no model with the same name already exist, if so delete\n","if os.path.exists(Save_path):\n","  shutil.rmtree(Save_path)\n","\n","# Create the model folder!\n","os.makedirs(Save_path)\n","\n","# Let's go !\n","train_model(patches, heatmaps, Save_path, \n","            steps_per_epoch=number_of_steps, epochs=number_of_epochs, batch_size=batch_size,\n","            upsampling_factor = upsampling_factor,\n","            validation_split = percentage_validation,\n","            initial_learning_rate = initial_learning_rate, \n","            pretrained_model_path = h5_file_path,\n","            L2_weighting_factor = L2_weighting_factor)\n","\n","# # Show info about the GPU memory useage\n","# !nvidia-smi\n","\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CHVTRjEOLRDH","colab_type":"text"},"source":["##**4.5. Download your model(s) from Google Drive**\n","\n","\n","---\n","<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder as all data can be erased at the next training if using the same folder."]},{"cell_type":"markdown","metadata":{"id":"4N7-ShZpLhwr","colab_type":"text"},"source":["# **5. Evaluate your model**\n","---\n","\n","<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model. \n","\n","<font size = 4>**We highly recommend to perform quality control on all newly trained models.**"]},{"cell_type":"code","metadata":{"id":"JDRsm7uKoBa-","colab_type":"code","cellView":"form","colab":{}},"source":["# model name and path\n","#@markdown ###Do you want to assess the model you just trained ?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, please provide the path to the model folder:\n","#@markdown #####During training, the model files are automatically saved inside a folder named after the parameter `model_name` (see section 4.1). Provide the name of this folder as `QC_model_path` . \n","\n","QC_model_path = \"\" #@param {type:\"string\"}\n","\n","if (Use_the_current_trained_model): \n","  QC_model_path = os.path.join(model_path, model_name)\n","\n","if os.path.exists(QC_model_path):\n","  print(\"The \"+os.path.basename(QC_model_path)+\" model will be evaluated\")\n","else:\n","  print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!'+bcolors.NORMAL)\n","  print('Please make sure you provide a valid model path before proceeding further.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gw7KaHZUoHC4","colab_type":"text"},"source":["## **5.1. Inspection of the loss function**\n","---\n","\n","<font size = 4>First, it is good practice to evaluate the training progress by comparing the training loss with the validation loss. The latter is a metric which shows how well the network performs on a subset of unseen data which is set aside from the training dataset. For more information on this, see for example [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols *et al.*\n","\n","<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target.\n","\n","<font size = 4>**Validation loss** describes the same error value between the model's prediction on a validation image and compared to it's target.\n","\n","<font size = 4>During training both values should decrease before reaching a minimal value which does not decrease further even after more training. Comparing the development of the validation loss with the training loss can give insights into the model's performance.\n","\n","<font size = 4>Decreasing **Training loss** and **Validation loss** indicates that training is still necessary and increasing the `number_of_epochs` is recommended. Note that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required. If the **Validation loss** suddenly increases again an the **Training loss** simultaneously goes towards zero, it means that the network is overfitting to the training data. In other words the network is remembering the exact patterns from the training data and no longer generalizes well to unseen data. In this case the training dataset has to be increased."]},{"cell_type":"code","metadata":{"id":"qUc-JMOcoGNZ","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Play the cell to show a plot of training errors vs. epoch number\n","import csv\n","from matplotlib import pyplot as plt\n","\n","lossDataFromCSV = []\n","vallossDataFromCSV = []\n","\n","with open(os.path.join(QC_model_path,'Quality Control/training_evaluation.csv'),'r') as csvfile:\n","    csvRead = csv.reader(csvfile, delimiter=',')\n","    next(csvRead)\n","    for row in csvRead:\n","        lossDataFromCSV.append(float(row[0]))\n","        vallossDataFromCSV.append(float(row[1]))\n","\n","epochNumber = range(len(lossDataFromCSV))\n","plt.figure(figsize=(15,10))\n","\n","plt.subplot(2,1,1)\n","plt.plot(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.plot(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training loss and validation loss vs. epoch number (linear scale)')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch number')\n","plt.legend()\n","\n","plt.subplot(2,1,2)\n","plt.semilogy(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.semilogy(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training loss and validation loss vs. epoch number (log scale)')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch number')\n","plt.legend()\n","plt.savefig(os.path.join(QC_model_path,'Quality Control/lossCurvePlots.png'))\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32eNQjFioQkY","colab_type":"text"},"source":["## **5.2. Error mapping and quality metrics estimation**\n","---\n","\n","<font size = 4>This section will display SSIM maps and RSE maps as well as calculating total SSIM, NRMSE and PSNR metrics for all the images provided in the \"QC_image_folder\" using teh corresponding localization data contained in \"QC_loc_folder\" !\n","\n","<font size = 4>**1. The SSIM (structural similarity) map** \n","\n","<font size = 4>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info). \n","\n","<font size=4>**mSSIM** is the SSIM value calculated across the entire window of both images.\n","\n","<font size=4>**The output below shows the SSIM maps with the mSSIM**\n","\n","<font size = 4>**2. The RSE (Root Squared Error) map** \n","\n","<font size = 4>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n","\n","\n","<font size =4>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n","\n","<font size = 4>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n","\n","<font size=4>**The output below shows the RSE maps with the NRMSE and PSNR values.**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"dhlTnxC5lUZy","colab_type":"code","cellView":"form","colab":{}},"source":["\n","# ------------------------ User input ------------------------\n","#@markdown ##Choose the folders that contain your Quality Control dataset\n","QC_image_folder = \"\" #@param{type:\"string\"}\n","QC_loc_folder = \"\" #@param{type:\"string\"}\n","#@markdown Get pixel size from file?\n","get_pixel_size_from_file = True #@param {type:\"boolean\"}\n","#@markdown Otherwise, use this value:\n","pixel_size = 100 #@param {type:\"number\"}\n","\n","if get_pixel_size_from_file:\n","  pixel_size_INPUT = None\n","else:\n","  pixel_size_INPUT = pixel_size\n","\n","\n","# ------------------------ QC analysis loop over provided dataset ------------------------\n","\n","savePath = os.path.join(QC_model_path, 'Quality Control')\n","\n","# Open and create the csv file that will contain all the QC metrics\n","with open(os.path.join(savePath, \"QC_metrics.csv\"), \"w\", newline='') as file:\n","  writer = csv.writer(file)\n","\n","  # Write the header in the csv file\n","  writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"WF v. GT mSSIM\", \"Prediction v. GT NRMSE\",\"WF v. GT NRMSE\", \"Prediction v. GT PSNR\", \"WF v. GT PSNR\"])\n","\n","  # These lists will be used to collect all the metrics values per slice\n","  file_name_list = []\n","  slice_number_list = []\n","  mSSIM_GvP_list = []\n","  mSSIM_GvWF_list = []\n","  NRMSE_GvP_list = []\n","  NRMSE_GvWF_list = []\n","  PSNR_GvP_list = []\n","  PSNR_GvWF_list = []\n","\n","  # Let's loop through the provided dataset in the QC folders\n","\n","  for (imageFilename, locFilename) in zip(list_files(QC_image_folder, 'tif'), list_files(QC_loc_folder, 'csv')):\n","    print('--------------')\n","    print(imageFilename)\n","    print(locFilename)\n","\n","    # Get the prediction\n","    batchFramePredictionLocalization(QC_image_folder, imageFilename, QC_model_path, savePath, pixel_size = pixel_size_INPUT)\n","\n","    # test_model(QC_image_folder, imageFilename, QC_model_path, savePath, display=False);\n","    thisPrediction = io.imread(os.path.join(savePath, 'Predicted_'+imageFilename))\n","    thisWidefield = io.imread(os.path.join(savePath, 'Widefield_'+imageFilename))\n","\n","    Mhr = thisPrediction.shape[0]\n","    Nhr = thisPrediction.shape[1]\n","\n","    if pixel_size_INPUT == None:\n","      pixel_size, N, M = getPixelSizeTIFFmetadata(os.path.join(QC_image_folder,imageFilename))\n","\n","    upsampling_factor = int(Mhr/M)\n","    print('Upsampling factor: '+str(upsampling_factor))\n","    pixel_size_hr = pixel_size/upsampling_factor # in nm\n","\n","    # Load the localization file and display the first\n","    LocData = pd.read_csv(os.path.join(QC_loc_folder,locFilename), index_col=0)\n","\n","    x = np.array(list(LocData['x [nm]']))\n","    y = np.array(list(LocData['y [nm]']))\n","    locImage = FromLoc2Image_SimpleHistogram(x, y, image_size = (Mhr,Nhr), pixel_size = pixel_size_hr)\n","\n","    # Remove extension from filename\n","    imageFilename_no_extension = os.path.splitext(imageFilename)[0]\n","\n","    # io.imsave(os.path.join(savePath, 'GT_image_'+imageFilename), locImage)\n","    saveAsTIF(savePath, 'GT_image_'+imageFilename_no_extension, locImage, pixel_size_hr)\n","\n","    # Normalize the images wrt each other by minimizing the MSE between GT and prediction\n","    test_GT_norm, test_prediction_norm = norm_minmse(locImage, thisPrediction, normalize_gt=True)\n","    # Normalize the images wrt each other by minimizing the MSE between GT and Source image\n","    test_GT_norm, test_wf_norm = norm_minmse(locImage, thisWidefield, normalize_gt=True)\n","\n","    # -------------------------------- Calculate the metric maps and save them --------------------------------\n","\n","    # Calculate the SSIM maps\n","    index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = structural_similarity(test_GT_norm, test_prediction_norm, data_range=1., full=True)\n","    index_SSIM_GTvsWF, img_SSIM_GTvsWF = structural_similarity(test_GT_norm, test_wf_norm, data_range=1., full=True)\n","\n","\n","    # Save ssim_maps\n","    img_SSIM_GTvsPrediction_32bit = np.float32(img_SSIM_GTvsPrediction)\n","    # io.imsave(os.path.join(savePath,'SSIM_GTvsPrediction_'+imageFilename),img_SSIM_GTvsPrediction_32bit)\n","    saveAsTIF(savePath,'SSIM_GTvsPrediction_'+imageFilename_no_extension, img_SSIM_GTvsPrediction_32bit, pixel_size_hr)\n","\n","\n","    img_SSIM_GTvsWF_32bit = np.float32(img_SSIM_GTvsWF)\n","    # io.imsave(os.path.join(savePath,'SSIM_GTvsWF_'+imageFilename),img_SSIM_GTvsWF_32bit)\n","    saveAsTIF(savePath,'SSIM_GTvsWF_'+imageFilename_no_extension, img_SSIM_GTvsWF_32bit, pixel_size_hr)\n","\n","  \n","    # Calculate the Root Squared Error (RSE) maps\n","    img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n","    img_RSE_GTvsWF = np.sqrt(np.square(test_GT_norm - test_wf_norm))\n","\n","    # Save SE maps\n","    img_RSE_GTvsPrediction_32bit = np.float32(img_RSE_GTvsPrediction)\n","    # io.imsave(os.path.join(savePath,'RSE_GTvsPrediction_'+imageFilename),img_RSE_GTvsPrediction_32bit)\n","    saveAsTIF(savePath,'RSE_GTvsPrediction_'+imageFilename_no_extension, img_RSE_GTvsPrediction_32bit, pixel_size_hr)\n","\n","    img_RSE_GTvsWF_32bit = np.float32(img_RSE_GTvsWF)\n","    # io.imsave(os.path.join(savePath,'RSE_GTvsWF_'+imageFilename),img_RSE_GTvsWF_32bit)\n","    saveAsTIF(savePath,'RSE_GTvsWF_'+imageFilename_no_extension, img_RSE_GTvsWF_32bit, pixel_size_hr)\n","\n","\n","    # -------------------------------- Calculate the RSE metrics and save them --------------------------------\n","\n","    # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n","    NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n","    NRMSE_GTvsWF = np.sqrt(np.mean(img_RSE_GTvsWF))\n","    \n","    # We can also measure the peak signal to noise ratio between the images\n","    PSNR_GTvsPrediction = psnr(test_GT_norm,test_prediction_norm,data_range=1.0)\n","    PSNR_GTvsWF = psnr(test_GT_norm,test_wf_norm,data_range=1.0)\n","\n","    writer.writerow([imageFilename,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsWF),str(NRMSE_GTvsPrediction),str(NRMSE_GTvsWF),str(PSNR_GTvsPrediction), str(PSNR_GTvsWF)])\n","\n","    # Collect values to display in dataframe output\n","    file_name_list.append(imageFilename)\n","    mSSIM_GvP_list.append(index_SSIM_GTvsPrediction)\n","    mSSIM_GvWF_list.append(index_SSIM_GTvsWF)\n","    NRMSE_GvP_list.append(NRMSE_GTvsPrediction)\n","    NRMSE_GvWF_list.append(NRMSE_GTvsWF)\n","    PSNR_GvP_list.append(PSNR_GTvsPrediction)\n","    PSNR_GvWF_list.append(PSNR_GTvsWF)\n","\n","\n","# Table with metrics as dataframe output\n","pdResults = pd.DataFrame(index = file_name_list)\n","pdResults[\"Prediction v. GT mSSIM\"] = mSSIM_GvP_list\n","pdResults[\"Wide-field v. GT mSSIM\"] = mSSIM_GvWF_list\n","pdResults[\"Prediction v. GT NRMSE\"] = NRMSE_GvP_list\n","pdResults[\"Wide-field v. GT NRMSE\"] = NRMSE_GvWF_list\n","pdResults[\"Prediction v. GT PSNR\"] = PSNR_GvP_list\n","pdResults[\"Wide-field v. GT PSNR\"] = PSNR_GvWF_list\n","\n","\n","# ------------------------ Display ------------------------\n","\n","print('--------------------------------------------')\n","@interact\n","def show_QC_results(file = list_files(QC_image_folder, 'tif')):\n","\n","  plt.figure(figsize=(15,15))\n","  # Target (Ground-truth)\n","  plt.subplot(3,3,1)\n","  plt.axis('off')\n","  img_GT = io.imread(os.path.join(savePath, 'GT_image_'+file))\n","  plt.imshow(img_GT, norm = simple_norm(img_GT, percent = 99.5))\n","  plt.title('Target',fontsize=15)\n","\n","  # Wide-field\n","  plt.subplot(3,3,2)\n","  plt.axis('off')\n","  img_Source = io.imread(os.path.join(savePath, 'Widefield_'+file))\n","  plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n","  plt.title('Widefield',fontsize=15)\n","\n","  #Prediction\n","  plt.subplot(3,3,3)\n","  plt.axis('off')\n","  img_Prediction = io.imread(os.path.join(savePath, 'Predicted_'+file))\n","  plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n","  plt.title('Prediction',fontsize=15)\n","\n","  #Setting up colours\n","  cmap = plt.cm.CMRmap\n","\n","  #SSIM between GT and Source\n","  plt.subplot(3,3,5)\n","  #plt.axis('off')\n","  plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","  img_SSIM_GTvsWF = io.imread(os.path.join(savePath, 'SSIM_GTvsWF_'+file))\n","  imSSIM_GTvsWF = plt.imshow(img_SSIM_GTvsWF, cmap = cmap, vmin=0, vmax=1)\n","  plt.colorbar(imSSIM_GTvsWF,fraction=0.046, pad=0.04)\n","  plt.title('Target vs. Widefield',fontsize=15)\n","  plt.xlabel('mSSIM: '+str(round(pdResults.loc[file][\"Wide-field v. GT mSSIM\"],3)),fontsize=14)\n","  plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n","\n","  #SSIM between GT and Prediction\n","  plt.subplot(3,3,6)\n","  #plt.axis('off')\n","  plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","  img_SSIM_GTvsPrediction = io.imread(os.path.join(savePath, 'SSIM_GTvsPrediction_'+file))\n","  imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n","  plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n","  plt.title('Target vs. Prediction',fontsize=15)\n","  plt.xlabel('mSSIM: '+str(round(pdResults.loc[file][\"Prediction v. GT mSSIM\"],3)),fontsize=14)\n","\n","  #Root Squared Error between GT and Source\n","  plt.subplot(3,3,8)\n","  #plt.axis('off')\n","  plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","  img_RSE_GTvsWF = io.imread(os.path.join(savePath, 'RSE_GTvsWF_'+file))\n","  imRSE_GTvsWF = plt.imshow(img_RSE_GTvsWF, cmap = cmap, vmin=0, vmax = 1)\n","  plt.colorbar(imRSE_GTvsWF,fraction=0.046,pad=0.04)\n","  plt.title('Target vs. Widefield',fontsize=15)\n","  plt.xlabel('NRMSE: '+str(round(pdResults.loc[file][\"Wide-field v. GT NRMSE\"],3))+', PSNR: '+str(round(pdResults.loc[file][\"Wide-field v. GT PSNR\"],3)),fontsize=14)\n","  plt.ylabel('RSE maps',fontsize=20, rotation=0, labelpad=75)\n","\n","  #Root Squared Error between GT and Prediction\n","  plt.subplot(3,3,9)\n","  #plt.axis('off')\n","  plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","  img_RSE_GTvsPrediction = io.imread(os.path.join(savePath, 'RSE_GTvsPrediction_'+file))\n","  imRSE_GTvsPrediction = plt.imshow(img_RSE_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n","  plt.colorbar(imRSE_GTvsPrediction,fraction=0.046,pad=0.04)\n","  plt.title('Target vs. Prediction',fontsize=15)\n","  plt.xlabel('NRMSE: '+str(round(pdResults.loc[file][\"Prediction v. GT NRMSE\"],3))+', PSNR: '+str(round(pdResults.loc[file][\"Prediction v. GT PSNR\"],3)),fontsize=14)\n","\n","print('--------------------------------------------')\n","pdResults.head()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTRou0izLjhd","colab_type":"text"},"source":["# **6. Using the trained model**\n","\n","---\n","\n","<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eAf8aBDmWTx7"},"source":["## **6.1 Generate image prediction and localizations from unseen dataset**\n","---\n","\n","<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as restored image stacks (ImageJ-compatible TIFF images).\n","\n","<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n","\n","<font size = 4>**`Result_folder`:** This folder will contain the found localizations csv.\n","\n","<font size = 4>**`batch_size`:** This paramter determines how many frames are processed by any single pass on the GPU. A higher `batch_size` will make the prediction faster but will use more GPU memory. If an OutOfMemory (OOM) error occurs, decrease the `batch_size`. **DEFAULT: 4**\n","\n","<font size = 4>**`threshold`:** This paramter determines threshold for local maxima finding. The value is expected to reside in the range **[0,1]**. A higher `threshold` will result in less localizations. **DEFAULT: 0.1**\n","\n","<font size = 4>**`neighborhood_size`:** This paramter determines size of the neighborhood within which the prediction needs to be a local maxima in recovery pixels (CCD pixel/upsampling_factor). A high `neighborhood_size` will make the prediction slower and potentially discard nearby localizations. **DEFAULT: 3**\n","\n","<font size = 4>**`use_local_average`:** This paramter determines whether to locally average the prediction in a 3x3 neighborhood to get the final localizations. If set to **True** it will make inference slightly slower depending on the size of the FOV. **DEFAULT: True**\n"]},{"cell_type":"code","metadata":{"id":"7qn06T_A0lxf","colab_type":"code","cellView":"form","colab":{}},"source":["\n","# ------------------------------- User input -------------------------------\n","#@markdown ### Data parameters\n","Data_folder = \"\" #@param {type:\"string\"}\n","Result_folder = \"\" #@param {type:\"string\"}\n","#@markdown Get pixel size from file?\n","get_pixel_size_from_file = True #@param {type:\"boolean\"}\n","#@markdown Otherwise, use this value (in nm):\n","pixel_size = 100 #@param {type:\"number\"}\n","\n","#@markdown ### Model parameters\n","#@markdown Do you want to use the model you just trained?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","#@markdown Otherwise, please provide path to the model folder below\n","prediction_model_path = \"\" #@param {type:\"string\"}\n","\n","#@markdown ### Prediction parameters\n","batch_size =  4#@param {type:\"integer\"}\n","\n","#@markdown ### Post processing parameters\n","threshold =  0.1#@param {type:\"number\"}\n","neighborhood_size =  3#@param {type:\"integer\"}\n","#@markdown Do you want to locally average the model output with CoG estimator ?\n","use_local_average = True #@param {type:\"boolean\"}\n","\n","\n","if get_pixel_size_from_file:\n","  pixel_size = None\n","\n","if (Use_the_current_trained_model): \n","  prediction_model_path = os.path.join(model_path, model_name)\n","\n","if os.path.exists(prediction_model_path):\n","  print(\"The \"+os.path.basename(prediction_model_path)+\" model will be used.\")\n","else:\n","  print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!'+bcolors.NORMAL)\n","  print('Please make sure you provide a valid model path before proceeding further.')\n","\n","# inform user whether local averaging is being used\n","if use_local_average == True: \n","  print('Using local averaging')\n","\n","if not os.path.exists(Result_folder):\n","  print('Result folder was created.')\n","  os.makedirs(Result_folder)\n","\n","\n","# ------------------------------- Run predictions -------------------------------\n","\n","start = time.time()\n","#%% This script tests the trained fully convolutional network based on the \n","# saved training weights, and normalization created using train_model.\n","\n","if os.path.isdir(Data_folder): \n","  for filename in list_files(Data_folder, 'tif'):\n","    # run the testing/reconstruction process\n","    print(\"------------------------------------\")\n","    print(\"Running prediction on: \"+ filename)\n","    batchFramePredictionLocalization(Data_folder, filename, prediction_model_path, Result_folder, \n","                                     batch_size, \n","                                     threshold, \n","                                     neighborhood_size, \n","                                     use_local_average,\n","                                     pixel_size = pixel_size)\n","\n","elif os.path.isfile(Data_folder):\n","  batchFramePredictionLocalization(os.path.dirname(Data_folder), os.path.basename(Data_folder), prediction_model_path, Result_folder, \n","                                   batch_size, \n","                                   threshold, \n","                                   neighborhood_size, \n","                                   use_local_average, \n","                                   pixel_size = pixel_size)\n","\n","\n","\n","print('--------------------------------------------------------------------')\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n","\n","\n","# ------------------------------- Interactive display -------------------------------\n","\n","print('--------------------------------------------------------------------')\n","print('---------------------------- Previews ------------------------------')\n","print('--------------------------------------------------------------------')\n","\n","if os.path.isdir(Data_folder): \n","  @interact\n","  def show_QC_results(file = list_files(Data_folder, 'tif')):\n","\n","    plt.figure(figsize=(15,7.5))\n","    # Wide-field\n","    plt.subplot(1,2,1)\n","    plt.axis('off')\n","    img_Source = io.imread(os.path.join(Result_folder, 'Widefield_'+file))\n","    plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n","    plt.title('Widefield', fontsize=15)\n","    # Prediction\n","    plt.subplot(1,2,2)\n","    plt.axis('off')\n","    img_Prediction = io.imread(os.path.join(Result_folder, 'Predicted_'+file))\n","    plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n","    plt.title('Predicted',fontsize=15)\n","\n","if os.path.isfile(Data_folder):\n","\n","  plt.figure(figsize=(15,7.5))\n","  # Wide-field\n","  plt.subplot(1,2,1)\n","  plt.axis('off')\n","  img_Source = io.imread(os.path.join(Result_folder, 'Widefield_'+os.path.basename(Data_folder)))\n","  plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n","  plt.title('Widefield', fontsize=15)\n","  # Prediction\n","  plt.subplot(1,2,2)\n","  plt.axis('off')\n","  img_Prediction = io.imread(os.path.join(Result_folder, 'Predicted_'+os.path.basename(Data_folder)))\n","  plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n","  plt.title('Predicted',fontsize=15)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZekzexaPmzFZ","colab_type":"text"},"source":["## **6.2 Drift correction**\n","---\n","\n","<font size = 4>The visualization above is the raw output of the network and displayed at the `upsampling_factor` chosen during model training. The display is a preview without any drift correction applied. This section performs drift correction using cross-correlation between time bins to estimate the drift.\n","\n","<font size = 4>**`Loc_file_path`:** is the path to the localization file to use for visualization.\n","\n","<font size = 4>**`original_image_path`:** is the path to the original image. This only serves to extract the original image size and pixel size to shape the visualization properly.\n","\n","<font size = 4>**`visualization_pixel_size`:** This parameter corresponds to the pixel size to use for the image reconstructions used for the Drift Correction estmication (in **nm**). A smaller pixel size will be more precise but will take longer to compute. **DEFAULT: 20**\n","\n","<font size = 4>**`number_of_bins`:** This parameter defines how many temporal bins are used across the full dataset. All localizations in each bins are used ot build an image. This image is used to find the drift with respect to the image obtained from the very first bin. A typical value would correspond to about 500 frames per bin. **DEFAULT: Total number of frames / 500**\n","\n","<font size = 4>**`polynomial_fit_degree`:** The drift obtained for each temporal bins needs to be interpolated to every single frames. This is performed by polynomial fit, the degree of which is defined here. **DEFAULT: 4**\n","\n","<font size = 4> The drift-corrected localization data is automaticaly saved in the `save_path` folder."]},{"cell_type":"code","metadata":{"id":"hYtP_vh6mzUP","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ##Data parameters\n","Loc_file_path = \"\" #@param {type:\"string\"}\n","# @markdown Provide information about original data. Get the info automatically from the raw data?\n","Get_info_from_file = True #@param {type:\"boolean\"}\n","# Loc_file_path = \"/content/gdrive/My Drive/Colab notebooks testing/DeepSTORM/Glia data from CL/Results from prediction/20200615-M6 with CoM localizations/Localizations_glia_actin_2D - 1-500fr_avg.csv\" #@param {type:\"string\"}\n","original_image_path = \"\" #@param {type:\"string\"}\n","# @markdown Otherwise, please provide image width, height (in pixels) and pixel size (in nm)\n","image_width =  256#@param {type:\"integer\"}\n","image_height =  256#@param {type:\"integer\"}\n","pixel_size = 100 #@param {type:\"number\"}\n","\n","# @markdown ##Drift correction parameters\n","visualization_pixel_size =  20#@param {type:\"number\"}\n","number_of_bins =  50#@param {type:\"integer\"}\n","polynomial_fit_degree =  4#@param {type:\"integer\"}\n","\n","# @markdown ##Saving parameters\n","save_path = '' #@param {type:\"string\"}\n","\n","\n","# Let's go !\n","start = time.time()\n","\n","# Get info from the raw file if selected\n","if Get_info_from_file:\n","  pixel_size, image_width, image_height = getPixelSizeTIFFmetadata(original_image_path, display=True)\n","\n","# Read the localizations in\n","LocData = pd.read_csv(Loc_file_path)\n","\n","# Calculate a few variables \n","Mhr = int(math.ceil(image_height*pixel_size/visualization_pixel_size))\n","Nhr = int(math.ceil(image_width*pixel_size/visualization_pixel_size))\n","nFrames = max(LocData['frame'])\n","x_max = max(LocData['x [nm]'])\n","y_max = max(LocData['y [nm]'])\n","image_size = (Mhr, Nhr)\n","n_locs = len(LocData.index)\n","\n","print('Image size: '+str(image_size))\n","print('Number of frames in data: '+str(nFrames))\n","print('Number of localizations in data: '+str(n_locs))\n","\n","blocksize = math.ceil(nFrames/number_of_bins)\n","print('Number of frames per block: '+str(blocksize))\n","\n","blockDataFrame = LocData[(LocData['frame'] < blocksize)].copy()\n","xc_array = blockDataFrame['x [nm]'].to_numpy(dtype=np.float32)\n","yc_array = blockDataFrame['y [nm]'].to_numpy(dtype=np.float32)\n","\n","# Preparing the Reference image\n","photon_array = np.ones(yc_array.shape[0])\n","sigma_array = np.ones(yc_array.shape[0])\n","ImageRef = FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = image_size, pixel_size = visualization_pixel_size)\n","ImagesRef = np.rot90(ImageRef, k=2)\n","\n","xDrift = np.zeros(number_of_bins)\n","yDrift = np.zeros(number_of_bins)\n","\n","filename_no_extension = os.path.splitext(os.path.basename(Loc_file_path))[0]\n","\n","with open(os.path.join(save_path, filename_no_extension+\"_DriftCorrectionData.csv\"), \"w\", newline='') as file:\n","  writer = csv.writer(file)\n","\n","  # Write the header in the csv file\n","  writer.writerow([\"Block #\", \"x-drift [nm]\",\"y-drift [nm]\"])\n","\n","  for b in tqdm(range(number_of_bins)):\n","\n","    blockDataFrame = LocData[(LocData['frame'] >= (b*blocksize)) & (LocData['frame'] < ((b+1)*blocksize))].copy()\n","    xc_array = blockDataFrame['x [nm]'].to_numpy(dtype=np.float32)\n","    yc_array = blockDataFrame['y [nm]'].to_numpy(dtype=np.float32)\n","\n","    photon_array = np.ones(yc_array.shape[0])\n","    sigma_array = np.ones(yc_array.shape[0])\n","    ImageBlock = FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = image_size, pixel_size = visualization_pixel_size)\n","\n","    XC = fftconvolve(ImagesRef, ImageBlock, mode = 'same')\n","    yDrift[b], xDrift[b] = subPixelMaxLocalization(XC, method = 'CoM')\n","\n","    # saveAsTIF(save_path, 'ImageBlock'+str(b), ImageBlock, visualization_pixel_size)\n","    # saveAsTIF(save_path, 'XCBlock'+str(b), XC, visualization_pixel_size)\n","    writer.writerow([str(b), str((xDrift[b]-xDrift[0])*visualization_pixel_size), str((yDrift[b]-yDrift[0])*visualization_pixel_size)])\n","\n","\n","print('--------------------------------------------------------------------')\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n","\n","print('Fitting drift data...')\n","bin_number = np.arange(number_of_bins)*blocksize + blocksize/2\n","xDrift = (xDrift-xDrift[0])*visualization_pixel_size\n","yDrift = (yDrift-yDrift[0])*visualization_pixel_size\n","\n","xDriftCoeff = np.polyfit(bin_number, xDrift, polynomial_fit_degree)\n","yDriftCoeff = np.polyfit(bin_number, yDrift, polynomial_fit_degree)\n","\n","xDriftFit = np.poly1d(xDriftCoeff)\n","yDriftFit = np.poly1d(yDriftCoeff)\n","bins = np.arange(nFrames)\n","xDriftInterpolated = xDriftFit(bins)\n","yDriftInterpolated = yDriftFit(bins)\n","\n","\n","# ------------------ Displaying the image results ------------------\n","\n","plt.figure(figsize=(15,10))\n","plt.plot(bin_number,xDrift, 'r+', label='x-drift')\n","plt.plot(bin_number,yDrift, 'b+', label='y-drift')\n","plt.plot(bins,xDriftInterpolated, 'r-', label='y-drift (fit)')\n","plt.plot(bins,yDriftInterpolated, 'b-', label='y-drift (fit)')\n","plt.title('Cross-correlation estimated drift')\n","plt.ylabel('Drift [nm]')\n","plt.xlabel('Bin number')\n","plt.legend();\n","\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\", hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n","\n","\n","# ------------------ Actual drift correction -------------------\n","\n","print('Correcting localization data...')\n","xc_array = LocData['x [nm]'].to_numpy(dtype=np.float32)\n","yc_array = LocData['y [nm]'].to_numpy(dtype=np.float32)\n","frames = LocData['frame'].to_numpy(dtype=np.int32)\n","\n","\n","xc_array_Corr, yc_array_Corr = correctDriftLocalization(xc_array, yc_array, frames, xDriftInterpolated, yDriftInterpolated)\n","ImageRaw = FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = image_size, pixel_size = visualization_pixel_size)\n","ImageCorr = FromLoc2Image_SimpleHistogram(xc_array_Corr, yc_array_Corr, image_size = image_size, pixel_size = visualization_pixel_size)\n","\n","\n","# ------------------ Displaying the imge results ------------------\n","plt.figure(figsize=(15,7.5))\n","# Raw\n","plt.subplot(1,2,1)\n","plt.axis('off')\n","plt.imshow(ImageRaw, norm = simple_norm(ImageRaw, percent = 99.5))\n","plt.title('Raw', fontsize=15);\n","# Corrected\n","plt.subplot(1,2,2)\n","plt.axis('off')\n","plt.imshow(ImageCorr, norm = simple_norm(ImageCorr, percent = 99.5))\n","plt.title('Corrected',fontsize=15);\n","\n","\n","# ------------------ Table with info -------------------\n","driftCorrectedLocData = pd.DataFrame()\n","driftCorrectedLocData['frame'] = frames\n","driftCorrectedLocData['x [nm]'] = xc_array_Corr\n","driftCorrectedLocData['y [nm]'] = yc_array_Corr\n","driftCorrectedLocData['confidence [a.u]'] = LocData['confidence [a.u]']\n","\n","driftCorrectedLocData.to_csv(os.path.join(save_path, filename_no_extension+'_DriftCorrected.csv'))\n","print('-------------------------------')\n","print('Corrected localizations saved.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzOuc-V7rB-r","colab_type":"text"},"source":["## **6.3 Visualization of the localizations**\n","---\n","\n","\n","<font size = 4>The visualization in section 6.1 is the raw output of the network and displayed at the `upsampling_factor` chosen during model training. This section performs visualization of the result by plotting the localizations as a simple histogram.\n","\n","<font size = 4>**`Loc_file_path`:** is the path to the localization file to use for visualization.\n","\n","<font size = 4>**`original_image_path`:** is the path to the original image. This only serves to extract the original image size and pixel size to shape the visualization properly.\n","\n","<font size = 4>**`visualization_pixel_size`:** This parameter corresponds to the pixel size to use for the final image reconstruction (in **nm**). **DEFAULT: 10**\n","\n","<font size = 4>**`visualization_mode`:** This parameter defines what visualization method is used to visualize the final image. NOTES: The Integrated Gaussian can be quite slow. **DEFAULT: Simple histogram.**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"876yIXnqq-nW","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ##Data parameters\n","Use_current_drift_corrected_localizations = True #@param {type:\"boolean\"}\n","# @markdown Otherwise provide a localization file path\n","Loc_file_path = \"\" #@param {type:\"string\"}\n","# @markdown Provide information about original data. Get the info automatically from the raw data?\n","Get_info_from_file = True #@param {type:\"boolean\"}\n","# Loc_file_path = \"/content/gdrive/My Drive/Colab notebooks testing/DeepSTORM/Glia data from CL/Results from prediction/20200615-M6 with CoM localizations/Localizations_glia_actin_2D - 1-500fr_avg.csv\" #@param {type:\"string\"}\n","original_image_path = \"\" #@param {type:\"string\"}\n","# @markdown Otherwise, please provide image width, height (in pixels) and pixel size (in nm)\n","image_width =  256#@param {type:\"integer\"}\n","image_height =  256#@param {type:\"integer\"}\n","pixel_size =  100#@param {type:\"number\"}\n","\n","# @markdown ##Visualization parameters\n","visualization_pixel_size =  10#@param {type:\"number\"}\n","visualization_mode = \"Simple histogram\" #@param [\"Simple histogram\", \"Integrated Gaussian (SLOW!)\"]\n","\n","if not Use_current_drift_corrected_localizations:\n","  filename_no_extension = os.path.splitext(os.path.basename(Loc_file_path))[0]\n","\n","\n","if Get_info_from_file:\n","  pixel_size, image_width, image_height = getPixelSizeTIFFmetadata(original_image_path, display=True)\n","\n","if Use_current_drift_corrected_localizations:\n","  LocData = driftCorrectedLocData\n","else:\n","  LocData = pd.read_csv(Loc_file_path)\n","\n","Mhr = int(math.ceil(image_height*pixel_size/visualization_pixel_size))\n","Nhr = int(math.ceil(image_width*pixel_size/visualization_pixel_size))\n","\n","\n","nFrames = max(LocData['frame'])\n","x_max = max(LocData['x [nm]'])\n","y_max = max(LocData['y [nm]'])\n","image_size = (Mhr, Nhr)\n","\n","print('Image size: '+str(image_size))\n","print('Number of frames in data: '+str(nFrames))\n","print('Number of localizations in data: '+str(len(LocData.index)))\n","\n","xc_array = LocData['x [nm]'].to_numpy()\n","yc_array = LocData['y [nm]'].to_numpy()\n","if (visualization_mode == 'Simple histogram'):\n","  locImage = FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = image_size, pixel_size = visualization_pixel_size)\n","elif (visualization_mode == 'Shifted histogram'):\n","  print(bcolors.WARNING+'Method not implemented yet!'+bcolors.NORMAL)\n","  locImage = np.zeros(image_size)\n","elif (visualization_mode == 'Integrated Gaussian (SLOW!)'):\n","  photon_array = np.ones(xc_array.shape)\n","  sigma_array = np.ones(xc_array.shape)\n","  locImage = FromLoc2Image_Erf(xc_array, yc_array, photon_array, sigma_array, image_size = image_size, pixel_size = visualization_pixel_size)\n","\n","print('--------------------------------------------------------------------')\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n","\n","# Display\n","plt.figure(figsize=(20,10))\n","plt.axis('off')\n","# plt.imshow(locImage, cmap='gray');\n","plt.imshow(locImage, norm = simple_norm(locImage, percent = 99.5));\n","\n","\n","LocData.head()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdOhWwMn1zIT","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ---\n","# @markdown #Play this cell to save the visualization\n","# @markdown ####Please select a path to the folder where to save the visualization.\n","save_path = \"\" #@param {type:\"string\"}\n","\n","if not os.path.exists(save_path):\n","  os.makedirs(save_path)\n","  print('Folder created.')\n","\n","saveAsTIF(save_path, filename_no_extension+'_Visualization', locImage, visualization_pixel_size)\n","print('Image saved.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1EszIF4Dkz_n","colab_type":"text"},"source":["## **6.4. Download your predictions**\n","---\n","\n","<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."]},{"cell_type":"markdown","metadata":{"id":"UgN-NooKk3nV","colab_type":"text"},"source":["\n","#**Thank you for using Deep-STORM 2D!**"]}]}