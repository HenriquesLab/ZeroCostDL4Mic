{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av1qDcfthk1a"
      },
      "source": [
        "# **Diffusion Model for SMLM**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>Diffusion model for single molecule localization microscopy (SMLM) is a project aiming to generate high quality, realistic synthetic 2D grayscale images of SMLM data. This work was first published in 2023 by [Saguy *et al.*](https://www.biorxiv.org/content/10.1101/2023.07.06.548004v2.full.pdf).\n",
        "\n",
        "<font size = 4> The network architecture used here is based on OpenAI's diffusion model, \"Improved Diffusion\", with minor adjustments and additions:\n",
        "- [Nichol, A.Q. and Dhariwal, P., 2021, July. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning (pp. 8162-8171). PMLR.](https://proceedings.mlr.press/v139/nichol21a.html)\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>*Disclaimer*:\n",
        "\n",
        "<font size = 4>This notebook is inspired from the *Zero-Cost Deep-Learning for Microscopy* project (ZeroCostDL4Mic) (https://github.com/HenriquesLab/DeepLearning_Collab/wiki)\n",
        "\n",
        "<font size = 4>This notebook is based on the following paper:\n",
        "\n",
        "- <font size = 4>**This microtubule does not exist: Super-resolution microscopy image generation by a diffusion model**, bioRxiv, 2023-07 by Saguy Alon, Tav Nahimov, Maia Lehrman, Onit Alalouf, and Yoav Shechtman [link to paper](https://www.biorxiv.org/content/10.1101/2023.07.06.548004v2.full.pdf)\n",
        "\n",
        "  - <font size = 4 >Source code found in: https://github.com/tavnah/improved_diffusion_for_SMLM\n",
        "\n",
        "  - <font size = 4 >**Dataset availability:** in our paper we used images from [Shareloc](https://shareloc.xyz/#/). More specifically, Alpha-tubulin immuno-labeled with Alexa 647 in U2OS cells acquired by Manish Singh from the Imaging and Modeling Unit at Institut Pasteur, France [Link to images](https://shareloc.xyz/#/r/7234160)\n",
        "\n",
        "\n",
        "\n",
        "<font size = 4>**Please also cite this original paper when using or developing this notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKktwSaWhq9e"
      },
      "source": [
        "# **How to use this notebook?**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>Video describing how to use ZeroCostDL4Mic notebooks are available on youtube:\n",
        "  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n",
        "  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n",
        "\n",
        "\n",
        "---\n",
        "###**Structure of a notebook**\n",
        "\n",
        "<font size = 4>The notebook contains two types of cell:  \n",
        "\n",
        "<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n",
        "\n",
        "<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n",
        "\n",
        "---\n",
        "###**Table of contents, Code snippets** and **Files**\n",
        "\n",
        "<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n",
        "\n",
        "<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n",
        "\n",
        "<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n",
        "\n",
        "<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here.\n",
        "\n",
        "<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n",
        "\n",
        "<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n",
        "\n",
        "---\n",
        "###**Making changes to the notebook**\n",
        "\n",
        "<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n",
        "\n",
        "<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n",
        "You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v_Jl2QZhvLh"
      },
      "source": [
        "#**0. Before getting started**\n",
        "---\n",
        "\n",
        "<font size = 4>**Important note**\n",
        "\n",
        "<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to generate images using the model that you trained.\n",
        "\n",
        "<font size = 4>- If you wish to **Evaluate your model** using a model previously trained and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n",
        "\n",
        "<font size = 4>- If you only wish to **generate images** using a model previously trained and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvJvtQQgiVDF"
      },
      "source": [
        "# **1. Install Diffusion Model for SMLM<font color=white> and dependencies**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ],
      "metadata": {
        "id": "JeLC2VRq1TYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Install Diffusion Model for SMLM and dependencies\n",
        "\n",
        "\n",
        "!pip install fpdf2==2.7.4\n",
        "!pip install mpi4py==3.1.4\n",
        "!pip install improved_diffusion_for_SMLM==0.3.0\n",
        "!pip install nd2reader==3.3.0"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MrsQz_yv1SmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Load key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ],
      "metadata": {
        "id": "t-4qxfou1pdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XMi71QrxiZbS"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Load key dependencies\n",
        "\n",
        "#Libraries contains information of certain topics.\n",
        "\n",
        "#Put the imported code and libraries here\n",
        "\n",
        "Notebook_version = '1.12' #Contact the ZeroCostDL4Mic team to find out about the version number\n",
        "Network = 'Diffusion_Model'\n",
        "\n",
        "#Create a variable to get and store relative base path\n",
        "import os\n",
        "base_path = os.getcwd()\n",
        "\n",
        "def get_requirements_path():\n",
        "    # Store requirements file in 'contents' directory\n",
        "    current_dir = os.getcwd()\n",
        "    dir_count = current_dir.count('/') - 1\n",
        "    path = '../' * (dir_count) + 'requirements.txt'\n",
        "    return path\n",
        "\n",
        "def filter_files(file_list, filter_list):\n",
        "    filtered_list = []\n",
        "    for fname in file_list:\n",
        "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
        "            filtered_list.append(fname)\n",
        "    return filtered_list\n",
        "\n",
        "def build_requirements_file(before, after):\n",
        "    path = get_requirements_path()\n",
        "\n",
        "    # Exporting requirements.txt for local run\n",
        "    !pip freeze > $path\n",
        "\n",
        "    # Get minimum requirements file\n",
        "    df = pd.read_csv(path)\n",
        "    mod_list = [m.split('.')[0] for m in after if not m in before]\n",
        "    req_list_temp = df.values.tolist()\n",
        "    req_list = [x[0] for x in req_list_temp]\n",
        "\n",
        "    # Replace with package name and handle cases where import name is different to module name\n",
        "    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
        "    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n",
        "    filtered_list = filter_files(req_list, mod_replace_list)\n",
        "\n",
        "    file=open(path,'w')\n",
        "    for item in filtered_list:\n",
        "        file.writelines(item + '\\n')\n",
        "\n",
        "    file.close()\n",
        "\n",
        "import sys\n",
        "before = [str(m) for m in sys.modules]\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import time\n",
        "import csv\n",
        "import shutil\n",
        "import random\n",
        "import subprocess\n",
        "import warnings\n",
        "import cv2\n",
        "import html\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from skimage import io\n",
        "from fpdf import FPDF, HTMLMixin\n",
        "from builtins import any as b_any\n",
        "from datetime import datetime\n",
        "from ipywidgets import interact, widgets\n",
        "from pip._internal.operations.freeze import freeze\n",
        "from tifffile import imread, imsave\n",
        "from pathlib import PosixPath\n",
        "\n",
        "import improved_diffusion_for_SMLM.scripts.image_train as image_train\n",
        "import improved_diffusion_for_SMLM.scripts.image_sample as image_sample\n",
        "import improved_diffusion_for_SMLM.scripts.patches as patches_utils\n",
        "\n",
        "class bcolors:\n",
        "  WARNING = '\\033[31m'\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "\n",
        "# Check if this is the latest version of the notebook\n",
        "All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n",
        "print('Notebook version: '+Notebook_version)\n",
        "Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n",
        "print('Latest notebook version: '+Latest_Notebook_version)\n",
        "if Notebook_version == Latest_Notebook_version:\n",
        "  print(\"This notebook is up-to-date.\")\n",
        "else:\n",
        "  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pdf_export(trained = False, augmentation = False, pretrained_model = False):\n",
        "    # save FPDF() class into a\n",
        "    # variable pdf\n",
        "    #from datetime import datetime\n",
        "\n",
        "    class MyFPDF(FPDF, HTMLMixin):\n",
        "        pass\n",
        "\n",
        "    pdf = MyFPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_right_margin(-1)\n",
        "    pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "    Network = \"Diffusion Model for SMLM\"\n",
        "    day = datetime.now()\n",
        "    datetime_str = str(day)[0:10]\n",
        "\n",
        "    Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
        "    pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "\n",
        "    # add another cell\n",
        "    if trained:\n",
        "      training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n",
        "      pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n",
        "    pdf.ln(1)\n",
        "\n",
        "    Header_2 = 'Information for your materials and methods:'\n",
        "    pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n",
        "\n",
        "    all_packages = ''\n",
        "    for requirement in freeze(local_only=True):\n",
        "      all_packages = all_packages+requirement+', '\n",
        "    #print(all_packages)\n",
        "\n",
        "    #Main Packages\n",
        "    main_packages = ''\n",
        "    version_numbers = []\n",
        "    for name in ['tensorflow','numpy','Keras','csbdeep']:\n",
        "      find_name=all_packages.find(name)\n",
        "      main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
        "      #Version numbers only here:\n",
        "      version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
        "\n",
        "    cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n",
        "    cuda_version = cuda_version.stdout.decode('utf-8')\n",
        "    cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
        "    gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n",
        "    gpu_name = gpu_name.stdout.decode('utf-8')\n",
        "    gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
        "    #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
        "    #print(gpu_name)\n",
        "\n",
        "    #shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n",
        "    dataset_size = len(os.listdir(Training_source))\n",
        "\n",
        "    text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' steps, patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+', using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), csbdeep (v '+version_numbers[3]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "    if pretrained_model:\n",
        "      text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' steps , patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+', using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version+') (von Chamier & Laine et al., 2020). The model was re-trained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), csbdeep (v '+version_numbers[3]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font_size(10.)\n",
        "    pdf.multi_cell(190, 5, txt = text, align='L')\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(28, 5, txt='Augmentation: ', ln=0)\n",
        "    pdf.set_font('')\n",
        "    if augmentation:\n",
        "      aug_text = 'The dataset was augmented by default'\n",
        "    else:\n",
        "      aug_text = 'No augmentation was used for training.'\n",
        "    pdf.multi_cell(190, 5, txt=aug_text, align='L')\n",
        "    pdf.set_font('Arial', size = 11, style = 'B')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font_size(10.)\n",
        "    if Use_Default_Advanced_Parameters:\n",
        "      pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
        "    pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
        "    pdf.ln(1)\n",
        "    html = \"\"\"\n",
        "    <table width=40% style=\"margin-left:0px;\">\n",
        "      <tr>\n",
        "        <th width = 50% align=\"left\">Parameter</th>\n",
        "        <th width = 50% align=\"left\">Value</th>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>number_of_epochs</td>\n",
        "        <td width = 50%>{0}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>patch_size</td>\n",
        "        <td width = 50%>{1}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>batch_size</td>\n",
        "        <td width = 50%>{2}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>number_of_diffusion_steps</td>\n",
        "        <td width = 50%>{3}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>number_of_channels</td>\n",
        "        <td width = 50%>{4}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>initial_learning_rate</td>\n",
        "        <td width = 50%>{5}</td>\n",
        "      </tr>\n",
        "    </table>\n",
        "    \"\"\".format(number_of_epochs,str(patch_size)+'x'+str(patch_size),batch_size,number_of_diffusion_steps,number_of_channels,initial_learning_rate)\n",
        "    pdf.write_html(html)\n",
        "\n",
        "    #pdf.multi_cell(190, 5, txt = text_2, align='L')\n",
        "    pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    pdf.cell(29, 5, txt= 'Training_source:', align = 'L', ln=0)\n",
        "    pdf.set_font('')\n",
        "    pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n",
        "    pdf.set_font('')\n",
        "    # pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    # pdf.cell(27, 5, txt= 'Training_target:', align = 'L', ln=0)\n",
        "    # pdf.set_font('')\n",
        "    #pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n",
        "    #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n",
        "    pdf.ln(1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    pdf.cell(22, 5, txt= 'Model Path:', align = 'L', ln=0)\n",
        "    pdf.set_font('')\n",
        "    pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(60, 5, txt = 'Example Training', ln=1)\n",
        "    pdf.ln(1)\n",
        "\n",
        "    for file in os.listdir(Saving_path):\n",
        "      if not os.path.isdir(file):\n",
        "        break\n",
        "    exp_size = io.imread(os.path.join(Saving_path, file)).shape\n",
        "    pdf.image(os.path.join(Saving_path, file), x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "    pdf.ln(1)\n",
        "    ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "    pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "    ref_2 = '- Diffusion Model for SMLM: Saguy et al. \"This microtubule does not exist: Super-resolution microscopy image generation by a diffusion model\" BioRxiv, 2023'\n",
        "    pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "    if augmentation:\n",
        "      ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
        "      pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
        "    pdf.ln(3)\n",
        "    reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
        "    pdf.set_font('Arial', size = 11, style='B')\n",
        "    pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "\n",
        "    pdf.output(model_path+'/'+model_name+'/'+model_name+\"_training_report.pdf\")\n",
        "\n",
        "\n",
        "#Make a pdf summary of the QC results\n",
        "\n",
        "def qc_pdf_export():\n",
        "  class MyFPDF(FPDF, HTMLMixin):\n",
        "    pass\n",
        "\n",
        "  pdf = MyFPDF()\n",
        "  pdf.add_page()\n",
        "  pdf.set_right_margin(-1)\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "  Network = \"Diffusion Model for SMLM\"\n",
        "  model_name = os.path.basename(full_QC_model_path)\n",
        "  day = datetime.now()\n",
        "  datetime_str = str(day)[0:10]\n",
        "\n",
        "  Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n",
        "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "\n",
        "  all_packages = ''\n",
        "  for requirement in freeze(local_only=True):\n",
        "    all_packages = all_packages+requirement+', '\n",
        "\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(2)\n",
        "  pdf.cell(190, 5, txt = 'Development of Training Losses', ln=1, align='L')\n",
        "  pdf.ln(1)\n",
        "  #exp_size = io.imread(full_QC_model_path+'Quality Control/QC_example_data.png').shape\n",
        "  if os.path.exists(model_folder.__fspath__()+'/lossCurvePlots.png'):\n",
        "    pdf.image(model_folder.__fspath__()+'/lossCurvePlots.png', x = 11, y = None)\n",
        "  else:\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size=10)\n",
        "    pdf.multi_cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.', align='L')\n",
        "  pdf.ln(2)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.ln(3)\n",
        "  #pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n",
        "  #pdf.ln(1)\n",
        "  #exp_size = io.imread(full_QC_model_path+'Quality Control/QC_example_data.png').shape\n",
        "  #pdf.image(full_QC_model_path+'Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/10), h = round(exp_size[0]/10))\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "  ref_2 = '- Diffusion Model for SMLM: Saguy et al. \"This microtubule does not exist: Super-resolution microscopy image generation by a diffusion model\" BioRxiv, 2023'\n",
        "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "\n",
        "  pdf.ln(3)\n",
        "  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
        "\n",
        "  pdf.set_font('Arial', size = 11, style='B')\n",
        "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "\n",
        "  pdf.output(full_QC_model_path +'/'+QC_model_name+'_QC_report.pdf')\n",
        "\n",
        "print(\"Depencies installed and imported.\")\n",
        "\n",
        "# Build requirements file for local run\n",
        "# -- the developers should leave this below all the other installations\n",
        "after = [str(m) for m in sys.modules]\n",
        "build_requirements_file(before, after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPOJkyFYiA15"
      },
      "source": [
        "# **2. Initialise the Colab session**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dvLrwF_iEXS"
      },
      "source": [
        "\n",
        "## **2.1. Check for GPU access**\n",
        "---\n",
        "\n",
        "By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n",
        "\n",
        "<font size = 4>Go to **Runtime -> Change the Runtime type**\n",
        "\n",
        "<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n",
        "\n",
        "<font size = 4>**Accelerator: GPU** *(Graphics processing unit)*\n",
        "\n",
        "<font size = 4>After running the next cell, make sure you see the following sentence at the top: **\"You have gpu access\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o_-wbDOiIHF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Run this cell to check if you have GPU access\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name()=='':\n",
        "  print('You do not have GPU access.')\n",
        "  print('Did you change your runtime ?')\n",
        "  print('If the runtime settings are correct then Google did not allocate GPU to your session')\n",
        "  print('Expect slow performance. To access GPU try reconnecting later')\n",
        "\n",
        "else:\n",
        "  print('You have GPU access')\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEyJvvxSiN6L"
      },
      "source": [
        "## **2.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow'. This will give Colab access to the data on the drive.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWVR1U5tiM9h",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Run this cell to connect your Google Drive to Colab\n",
        "\n",
        "#@markdown * Click on the URL.\n",
        "\n",
        "#@markdown * Sign in your Google Account.\n",
        "\n",
        "#@markdown * Copy the authorization code.\n",
        "\n",
        "#@markdown * Enter the authorization code.\n",
        "\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\".\n",
        "\n",
        "#mounts user's Google Drive to Google Colab.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKaeBnSuifZn"
      },
      "source": [
        "# **3. Select your paths and parameters**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>The code below allows the user to enter the paths to where the training data is and to define the training parameters.\n",
        "\n",
        "<font size = 4>**Note: Currently, this notebook supports the following formats: .jpg and .tif. Additionally, we do not support stacks of images at the time, so you may only use single image per input file.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StTGluw2iidc"
      },
      "source": [
        "## **3.1. Setting the main training parameters**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyRjBdClimfK"
      },
      "source": [
        "<font size = 5> **Paths for training and results**\n",
        "\n",
        "<font size = 4>**`Training_source`**: This is the path to the folder containing the training data. This model does not require paired images, so only the source images representing the image data type to generate is needed. To find the path of the folder containing your dataset, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
        "\n",
        "<font size = 4>**`model_name`**: When naming your model use \"_\" instead of \"-\", for example you can name your model My_model and not My-model. Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n",
        "\n",
        "<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
        "\n",
        "<font size = 5>**Training parameters**\n",
        "\n",
        "<font size = 4>**`number_of_epochs`**: Specify the number of training epochs (steps) for the network training. Typically, you should use at least 30000 steps. **Default value: 30000**\n",
        "\n",
        "<font size = 4>**`image_size`**: Specify the generated image size. This is the size in pixels of the image that is genetated by the model. The model is trained to generate images of the same resolution as the training data. This parameter can be memory-limited. **Default value: 64**\n",
        "\n",
        "<font size = 4>**`number_of_diffusion_steps`**: Specify the number of diffusion iterations the model will perform to generate an image. A higher number of diffusion steps usually leads to better-quality outputs but requires more computation. **Default value: 2000**\n",
        "\n",
        "<font size = 4>**`save_interval`**: Specify the saving frequency during training. It relates to the number of iterations. **Default value: 2000**\n",
        "\n",
        "<font size = 5>**Advanced training parameters - experienced users only**\n",
        "\n",
        "<font size =4>**`batch_size`**: Specify the number of patches used in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 10**\n",
        "\n",
        "<font size =4>**`num_channels`**: Specify the number of channels used in the first convolutional layer of the network. Increasing this value will lead to bigger model, and slower runtime. **Default value: 64**\n",
        "\n",
        "<font size = 4>**`noise_schedule`**: The noise schedule determines how the model gradually reduces noise or uncertainty in the data during each iteration. Valid values are: 'linear' or 'cosine'. **Default value: cosine**\n",
        "\n",
        "<font size =4>**`learning_rate`**: Specify the learning rate during training. **Default value: 0.00001**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1sKnXrDieiR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Path to training images:\n",
        "\n",
        "Training_source = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the model and path to model folder:\n",
        "model_name = \"\" #@param {type:\"string\"}\n",
        "model_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# ------------- Initialising folder, variables and failsafes ------------\n",
        "#  Create the folders where to save the model and the QC\n",
        "full_model_path = os.path.join(model_path, model_name)\n",
        "if os.path.exists(full_model_path):\n",
        "  print(bcolors.WARNING + '!! WARNING: Folder already exists and will be overwritten !!')\n",
        "else:\n",
        "  os.makedirs(full_model_path)\n",
        "\n",
        "\n",
        "# other parameters for training.\n",
        "#@markdown ###Training Parameters\n",
        "#@markdown Number of epochs (-1 for infinite):\n",
        "\n",
        "number_of_epochs =  30000#@param {type:\"number\"}\n",
        "#@markdown The wanted image size for generating images.\n",
        "\n",
        "image_size =\"128\" #@param [\"32\",\"64\",\"128\",\"256\"]\n",
        "image_size=int(image_size)\n",
        "patch_size = image_size\n",
        "\n",
        "#@markdown The saving intervel:\n",
        "save_interval = 2000#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "\n",
        "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please input:\n",
        "\n",
        "number_of_diffusion_steps = 2000#@param {type:\"number\"}\n",
        "batch_size =  10#@param {type:\"number\"}\n",
        "num_channels = 64#@param {type:\"number\"}\n",
        "noise_schedule='cosine' #@param {type:\"string\"}\n",
        "learning_rate=1e-5#@param {type:\"number\"}\n",
        "\n",
        "if (Use_Default_Advanced_Parameters):\n",
        "  print(\"Default advanced parameters enabled\")\n",
        "  batch_size = 10\n",
        "  number_of_diffusion_steps = 2000\n",
        "  num_channels = 64\n",
        "  noise_schedule='cosine'\n",
        "  learning_rate=1e-5\n",
        "\n",
        "# Here we check that patch_size is divisible by 8\n",
        "if not patch_size % 8 == 0:\n",
        "    patch_size = ((int(patch_size / 8)-1) * 8)\n",
        "    print (bcolors.WARNING + \" Your chosen patch_size is not divisible by 8; therefore the patch_size chosen is now:\",patch_size)\n",
        "\n",
        "Use_Data_augmentation = False\n",
        "Use_pretrained_model = False\n",
        "pretrained_model_path = \"\"\n",
        "initial_learning_rate = learning_rate\n",
        "\n",
        "# This will display a randomly chosen dataset input and output\n",
        "random_choice = random.choice(os.listdir(Training_source))\n",
        "x = io.imread(os.path.join(Training_source, random_choice))\n",
        "\n",
        "f=plt.figure(figsize=(8,8))\n",
        "plt.imshow(x, cmap='gray', interpolation='nearest')\n",
        "plt.title('Training source')\n",
        "plt.axis('off');\n",
        "plt.savefig(os.path.join(full_model_path, 'TrainingDataExample_pix2pix.png'),bbox_inches='tight',pad_inches=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqIe455yfmP2"
      },
      "source": [
        "## **3.2. Generate training patches**\n",
        "---\n",
        "<font size = 4>To train the network we split each training image to multiple training patches with size equal to the **`image size`** parameter specified above. The number of generated patches is determined by the following parameters.\n",
        "\n",
        "<font size = 4> **`overlap`**: The parameter should be a value between 0 and 1, representing the extent of overlap between patches. A value of 0 implies no overlap, while 0.5 corresponds to 50% overlap, and so on. **Default value: 0.25**\n",
        "\n",
        "\n",
        "<font size = 4> **`q1_percentile`**: During our analysis we normalize the patches between [0, 1]. Since fluorescence data may posses high dynamic range, an additional normalization is often required. This parameter specifies which percentile will be saturated to prevent disapprenece of realtively weak fluorescence signal.  **Default value: 0.01**\n",
        "\n",
        "**In case you have already generated the training patches, you can specify the patches folder as the `Training_source` parameter in the previous section and skip this cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij40oRJAvYUb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@markdown Chose parameters for patches generation:\n",
        "\n",
        "overlap = 0.25 #@param{type:\"number\"}\n",
        "q1_percentile = 0.0 #@param{type:\"number\"}\n",
        "q3_percentile = 1-q1_percentile\n",
        "\n",
        "\n",
        "crop_start = (0, 0)\n",
        "#@markdown **Would you like to save your patches?**\n",
        "\n",
        "Save_patches = False #@param {type:\"boolean\"}\n",
        "Saving_path_patches = \"\" #@param {type:\"string\"}\n",
        "os.makedirs(Saving_path_patches, exist_ok=True)\n",
        "\n",
        "if not Save_patches:\n",
        "  Saving_path_patches = os.path.join(base_path, \"patches\")\n",
        "\n",
        "patch_size = image_size\n",
        "patches, _= patches_utils.create_patches_for_type(Training_source, (patch_size,patch_size), overlap=overlap, crop_start=crop_start, n_rotations=0)\n",
        "patches_utils.save_patches(patches, Saving_path_patches, q1_percentile, q3_percentile)\n",
        "\n",
        "Training_source = Saving_path_patches\n",
        "\n",
        "## show 4 random patches\n",
        "if(patches.shape[0] >= 4):\n",
        "  patches_idx = random.sample(range(patches.shape[0]), 4)\n",
        "\n",
        "  fig, axes = plt.subplots(1,4, figsize=(12, 3))\n",
        "  for idx in range(4):\n",
        "    scaled_image = patches_utils.remove_outliers(np.array(patches[patches_idx[idx],:,:]), q1_percentile, q3_percentile)\n",
        "    axes[idx].imshow(scaled_image, cmap='gray')\n",
        "    axes[idx].axis('off')\n",
        "  plt.suptitle(\"patches for training\")\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLYZQA6GitQL"
      },
      "source": [
        "## **3.3. Data augmentation**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4GfK6-1iwbf"
      },
      "source": [
        "<font size = 4>Data augmentation can improve training progress by increasing the dataset size. Smaller datasets may lead to memorization of every example in the dataset (overfitting). Augmentation is not necessary for training and if your training dataset is large you may disable it.\n",
        "\n",
        "<font size = 4>Data augmentation is performed here by rotating the patches in XY-Plane and flip them along X-Axis. This only works if the images are square in XY.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkBGtraZi3Ob",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#from matplotlib.image import imrea\n",
        "\n",
        "Use_Data_augmentation = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Select this option if you want to use augmentation to increase the size of your dataset\n",
        "\n",
        "#@markdown **Rotate each image 3 times by 90 degrees.**\n",
        "Rotation = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown **Flip each image once around the x axis of the stack.**\n",
        "Flip = True #@param{type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown **Would you like to save your augmented images?**\n",
        "\n",
        "Save_augmented_images = False #@param {type:\"boolean\"}\n",
        "\n",
        "Saving_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "if not Save_augmented_images:\n",
        "  Saving_path= base_path\n",
        "\n",
        "\n",
        "def rotation_aug(Source_path, flip=False):\n",
        "  Source_images = os.listdir(Source_path)\n",
        "\n",
        "  for image in Source_images:\n",
        "    source_img = io.imread(os.path.join(Source_path,image))\n",
        "    # Source Rotation\n",
        "    if len(source_img.shape)==2:\n",
        "      axes=(0,1)\n",
        "    if len(source_img.shape)==3:\n",
        "      axes=(1,2)\n",
        "    source_img_90 = np.rot90(source_img,axes=axes)\n",
        "    source_img_180 = np.rot90(source_img_90,axes=axes)\n",
        "    source_img_270 = np.rot90(source_img_180,axes=axes)\n",
        "    # Add a flip to the rotation\n",
        "\n",
        "    if flip == True:\n",
        "      source_img_lr = np.fliplr(source_img)\n",
        "      source_img_90_lr = np.fliplr(source_img_90)\n",
        "      source_img_180_lr = np.fliplr(source_img_180)\n",
        "      source_img_270_lr = np.fliplr(source_img_270)\n",
        "\n",
        "\n",
        "    # Save the augmented files\n",
        "    # Source images\n",
        "    io.imsave(Saving_path+'/augmented_patches/'+image,source_img)\n",
        "    io.imsave(Saving_path+'/augmented_patches/'+os.path.splitext(image)[0]+'_90.tif',source_img_90)\n",
        "    io.imsave(Saving_path+'/augmented_patches/'+os.path.splitext(image)[0]+'_180.tif',source_img_180)\n",
        "    io.imsave(Saving_path+'/augmented_patches/'+os.path.splitext(image)[0]+'_270.tif',source_img_270)\n",
        "\n",
        "    if flip == True:\n",
        "      io.imsave(Saving_path+'/augmented_patches/'+os.path.splitext(image)[0]+'_lr.tif',source_img_lr)\n",
        "      io.imsave(Saving_path+'/augmented_patches/'+os.path.splitext(image)[0]+'_90_lr.tif',source_img_90_lr)\n",
        "      io.imsave(Saving_path+'/augmented_patches/'+os.path.splitext(image)[0]+'_180_lr.tif',source_img_180_lr)\n",
        "      io.imsave(Saving_path+'/augmented_patches/'+os.path.splitext(image)[0]+'_270_lr.tif',source_img_270_lr)\n",
        "\n",
        "def flip(Source_path):\n",
        "  Source_images = os.listdir(Source_path)\n",
        "\n",
        "  for image in Source_images:\n",
        "    source_img = io.imread(os.path.join(Source_path,image))\n",
        "\n",
        "    source_img_lr = np.fliplr(source_img)\n",
        "\n",
        "    io.imsave(Saving_path+'/augmented_patches/'+image,source_img)\n",
        "    io.imsave(Saving_path+'/augmented_patches/'+os.path.splitext(image)[0]+'_lr.tif',source_img_lr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if Use_Data_augmentation:\n",
        "\n",
        "  if os.path.exists(Saving_path+'/augmented_patches'):\n",
        "    shutil.rmtree(Saving_path+'/augmented_patches')\n",
        "  os.mkdir(Saving_path+'/augmented_patches')\n",
        "\n",
        "  print(\"Data augmentation enabled\")\n",
        "  print(\"Data augmentation in progress....\")\n",
        "\n",
        "  if Rotation == True:\n",
        "    rotation_aug(Training_source,flip=Flip)\n",
        "\n",
        "  elif Rotation == False and Flip == True:\n",
        "    flip(Training_source)\n",
        "  print(\"Done\")\n",
        "\n",
        "\n",
        "if not Use_Data_augmentation:\n",
        "  print(bcolors.WARNING+\"Data augmentation disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y-47ZmFiyG_"
      },
      "source": [
        "## **3.4. Using weights from a pre-trained model as initial weights**\n",
        "---\n",
        "<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a model created using this notebook**.\n",
        "\n",
        "<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained on different data. **You do not need to run this section if you want to train a network from scratch**.\n",
        "\n",
        "<font size = 4> In order to continue training from the point where the pre-trained model left off, it is adviseable to also **load the learning rate** that was used when the training ended. This is automatically saved for models trained with ZeroCostDL4Mic and will be loaded here. If no learning rate can be found in the model folder provided, the default learning rate will be used.\n",
        "\n",
        "<font size = 4> **CUDA out of memory error:** In case you encounter this error, try restarting the runtime and running the notebook again. Notice that your local files will be removed, so don't forget to save trained model if you have one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSb9luhrjHe-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown ##Loading weights from a pre-trained network\n",
        "\n",
        "Use_pretrained_model = False #@param {type:\"boolean\"}\n",
        "\n",
        "pretrained_model_choice = \"Model_from_file\" #@param [\"Model_from_file\"]\n",
        "\n",
        "#Weights_choice = \"last\" #@param [\"last\", \"best\"]\n",
        "\n",
        "\n",
        "#@markdown ###If you chose \"Model_from_file\", please provide the path to the model checkpoint (A file named modelXXXXXX.pt):\n",
        "pretrained_model_path = \"\" #@param {type:\"string\"}\n",
        "initial_learning_rate = learning_rate\n",
        "# --------------------- Check if we load a previously trained model ------------------------\n",
        "if Use_pretrained_model:\n",
        "\n",
        "\n",
        "# --------------------- Load the model from the choosen path ------------------------\n",
        " if pretrained_model_choice == \"Model_from_file\":\n",
        "   #h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".h5\")\n",
        "   h5_file_path = pretrained_model_path\n",
        "\n",
        "# --------------------- Download the a model provided in the XXX ------------------------\n",
        "\n",
        "  # if pretrained_model_choice == \"Model_name\":\n",
        "  #   pretrained_model_name = \"Model_name\"\n",
        "  #   pretrained_model_path = \"/content/\"+pretrained_model_name\n",
        "  #   print(\"Downloading the 2D_Demo_Model_from_Stardist_2D_paper\")\n",
        "  #   if os.path.exists(pretrained_model_path):\n",
        "  #     shutil.rmtree(pretrained_model_path)\n",
        "  #   os.makedirs(pretrained_model_path)\n",
        "  #   wget.download(\"\", pretrained_model_path)\n",
        "  #   wget.download(\"\", pretrained_model_path)\n",
        "  #   wget.download(\"\", pretrained_model_path)\n",
        "  #   wget.download(\"\", pretrained_model_path)\n",
        "  #   h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".h5\")\n",
        "\n",
        "# --------------------- Add additional pre-trained models here ------------------------\n",
        "\n",
        "# --------------------- Check the model exist ------------------------\n",
        "# If the model path chosen does not contain a pretrain model then use_pretrained_model is disabled,\n",
        " if not os.path.exists(h5_file_path):\n",
        "  print(bcolors.WARNING+'WARNING: pretrained model does not exist')\n",
        "  Use_pretrained_model = False\n",
        "\n",
        "\n",
        "# If the model path contains a pretrain model, we load the training rate,\n",
        " if os.path.exists(h5_file_path):\n",
        "#Here we check if the learning rate can be loaded from the quality control folder\n",
        "  if os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n",
        "\n",
        "      with open(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv'),'r') as csvfile:\n",
        "        csvRead = pd.read_csv(csvfile, sep=',')\n",
        "        #print(csvRead)\n",
        "\n",
        "        if \"learning rate\" in csvRead.columns: #Here we check that the learning rate column exist (compatibility with model trained un ZeroCostDL4Mic bellow 1.4)\n",
        "          print(\"pretrained network learning rate found\")\n",
        "          #find the last learning rate\n",
        "          lastLearningRate = csvRead[\"learning rate\"].iloc[-1]\n",
        "          #Find the learning rate corresponding to the lowest validation loss\n",
        "          min_val_loss = csvRead[csvRead['val_loss'] == min(csvRead['val_loss'])]\n",
        "          #print(min_val_loss)\n",
        "          bestLearningRate = min_val_loss['learning rate'].iloc[-1]\n",
        "\n",
        "          #if Weights_choice == \"last\":\n",
        "          #  print('Last learning rate: '+str(lastLearningRate))\n",
        "\n",
        "          #if Weights_choice == \"best\":\n",
        "          #  print('Learning rate of best validation loss: '+str(bestLearningRate))\n",
        "\n",
        "        if not \"learning rate\" in csvRead.columns: #if the column does not exist, then initial learning rate is used instead\n",
        "          bestLearningRate = initial_learning_rate\n",
        "          lastLearningRate = initial_learning_rate\n",
        "          print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead')\n",
        "\n",
        "#Compatibility with models trained outside ZeroCostDL4Mic but default learning rate will be used\n",
        "  if not os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n",
        "     print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(initial_learning_rate)+' will be used instead')\n",
        "     bestLearningRate = initial_learning_rate\n",
        "     lastLearningRate = initial_learning_rate\n",
        "\n",
        "\n",
        "# Display info about the pretrained model to be loaded (or not)\n",
        "if Use_pretrained_model:\n",
        "  print('Weights found in:')\n",
        "  print(h5_file_path)\n",
        "  print('will be loaded prior to training.')\n",
        "\n",
        "else:\n",
        "  print(bcolors.WARNING+'No pretrained nerwork will be used.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjTtP2OmjMqM"
      },
      "source": [
        "# **4. Train the network**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ9NgI6XjQIk"
      },
      "source": [
        "## **4.1. Train the network**\n",
        "---\n",
        "<font size = 4>When running the cell below you should see training updates every 500 steps (typically, using image size = 256, it takes ~8 minutes to perform 500 training steps). Network training should take several hours.\n",
        "\n",
        "<font size = 4>**CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of diffusion steps.\n",
        "\n",
        "<font size = 4>During the training process, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder as all data can be erased at the next training if specifying the same model folder.\n",
        "\n",
        "<font size = 4>During training the mean loss and the mean squared error are reported every 500 steps. The reported values with postfix q0, q1, q2, and q3 represents the errors during the first, second, and third quartile of the diffusion steps respectively. Namely, if you use 1000 diffusion steps, the mse_q1 reports the mean squared error during training for diffusion steps number 250-500.\n",
        "\n",
        "<font size = 4>**Stopping the training before the training ends:** you can stop the training before it reaches the specified number of epochs. The model parameters are saved every 2000 steps by default (you can change it in training parameters). In case you wish to continue the training from the last checkpoint, go back to **section 3.4** and load the last version of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVUd0Lr0jUjy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown ## Run this cell to train the network\n",
        "\n",
        "# Export the training parameters as pdf (before training, in case training fails)\n",
        "#pdf_export(augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "# Start Training\n",
        "if Use_Data_augmentation:\n",
        "  Data_folder = Saving_path+'/augmented_patches'\n",
        "else:\n",
        "  Data_folder=Training_source\n",
        "#Insert the code necessary to initiate training of your model\n",
        "image_train.main(model_name =model_name,num_steps=number_of_epochs,logdir=model_path, data_dir=Data_folder, image_size=image_size, num_channels=num_channels, noise_schedule=noise_schedule, diffusion_steps=number_of_diffusion_steps, batch_size=batch_size,lr=learning_rate, resume_checkpoint=pretrained_model_path, save_interval=save_interval)\n",
        "\n",
        "#trained from scratch or if the pretrained weights are used (3.3.)\n",
        "\n",
        "# Displaying the time elapsed for training\n",
        "dt = time.time() - start\n",
        "mins, sec = divmod(dt, 60)\n",
        "hour, mins = divmod(mins, 60)\n",
        "print(\"Time elapsed:\",hour, \"hour(s)\",mins,\"min(s)\",round(sec),\"sec(s)\")\n",
        "\n",
        "# Export the training parameters as pdf (after training)\n",
        "pdf_export(trained = True, augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tm3aimXjZ1B"
      },
      "source": [
        "# **5. Evaluate your model**\n",
        "---\n",
        "\n",
        "<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model.\n",
        "\n",
        "<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAXu1FR0jYZC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Do you want to assess the model you just trained ?\n",
        "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ###If not, please provide the name of the model and path to model folder:\n",
        "#@markdown #####During training, the model files are automatically saved inside a folder named after model_name in section 3. Provide the path to this folder below.\n",
        "\n",
        "QC_model_folder = \"\" #@param {type:\"string\"}\n",
        "QC_model_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# exp_size = io.imread(full_QC_model_path+'Quality Control/QC_example_data.png').shape\n",
        "\n",
        "\n",
        "if (Use_the_current_trained_model):\n",
        "  QC_model_folder = model_path\n",
        "  QC_model_name = model_name\n",
        "\n",
        "QC_model_path = os.path.join(QC_model_folder, QC_model_name)\n",
        "\n",
        "model_folder = PosixPath(QC_model_folder)\n",
        "for file in model_folder.iterdir():\n",
        "  if file.name.endswith('csv'):\n",
        "    QC_model_path = file.__fspath__()\n",
        "\n",
        "if os.path.exists(QC_model_path):\n",
        "  print(\"The \"+QC_model_name+\" network will be evaluated\")\n",
        "else:\n",
        "  W  = '\\033[0m'  # white (normal)\n",
        "  R  = '\\033[31m' # red\n",
        "  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
        "  print('Please make sure you provide a valid model path and model name before proceeding further.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULMuc37njkXM"
      },
      "source": [
        "## **5.1. Inspection of the loss function**\n",
        "---\n",
        "\n",
        "<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target. that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VCvEofKjjHN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to show a plot of training errors vs. epoch number\n",
        "\n",
        "# model_folder = PosixPath(model_path)\n",
        "# for file in model_folder.iterdir():\n",
        "#   if file.name.endswith('csv'):\n",
        "#     QC_model_path = file.__fspath__()\n",
        "\n",
        "lossDataFromCSV = []\n",
        "vallossDataFromCSV = []\n",
        "\n",
        "with open(QC_model_path,'r') as csvfile:\n",
        "    csvRead = csv.reader(csvfile, delimiter=',')\n",
        "    next(csvRead)\n",
        "    for row in csvRead:\n",
        "        lossDataFromCSV.append(float(row[0]))\n",
        "\n",
        "epochNumber = range(len(lossDataFromCSV))\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(epochNumber,lossDataFromCSV, label='Training loss')\n",
        "plt.title('Training loss vs. epoch number (linear scale)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch number')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.semilogy(epochNumber,lossDataFromCSV, label='Training loss')\n",
        "plt.title('Training loss vs. epoch number (log scale)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch number')\n",
        "plt.legend()\n",
        "plt.savefig(model_folder.__fspath__()+'/lossCurvePlots.png')\n",
        "plt.show()\n",
        "\n",
        "#qc_pdf_export()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB8QNLekkCyZ"
      },
      "source": [
        "# **6. Using the trained model**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>In this section, we generate new images using the trained model (in section 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2DrAOANkIWu"
      },
      "source": [
        "## **6.1. Generate images from the model**\n",
        "---\n",
        "\n",
        "<font size = 4>The current trained model (from section 4.2) can now be used to generate images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Generated output images are saved in your **Result_folder** folder as image stacks (ImageJ-compatible TIFF images).\n",
        "Note that the generated images are completely virtual and random.\n",
        "\n",
        "<font size = 4>**`Result_folder`:** This folder will contain the generated output images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mELG8z-ykCKV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Provide the path to the folder where the generated images are saved, then play the cell.\n",
        "\n",
        "Result_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###How many images do you want to generate?\n",
        "num_samples = 10#@param {type:\"number\"}\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Do you want to use the current trained model?\n",
        "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ###Which step of the trained model do you want to use?\n",
        "#@markdown #### If you want to use the last step of the trained model, you can set the value to -1.\n",
        "\n",
        "step_num_current_model= -1 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ###If not, provide the full path to model folder (it has a name similar to `modelXXXXXX.pt`):\n",
        "model_full_path = \"\" #@param {type:\"string\"}\n",
        "#@markdown ###Don't forget to make sure that the pretrined model  parameters match the parameters in section 3.1 (for example, image_size and difussion steps)\n",
        "\n",
        "#Here we find the loaded model name and parent path\n",
        "#generation_model_name = os.path.basename(Prediction_model_folder)\n",
        "#generation_model_path = os.path.dirname(Prediction_model_folder)\n",
        "\n",
        "if (Use_the_current_trained_model):\n",
        "  print(\"Using current trained network\")\n",
        "  #generation_model_name = model_name\n",
        "  #generation_model_path = model_path\n",
        "  files = os.listdir(os.path.join(model_path, model_name))\n",
        "  iteration_numbers = [int(file_name[-9:-3]) for file_name in files if file_name.startswith('ema')]\n",
        "\n",
        "\n",
        "  if step_num_current_model==-1:\n",
        "    model_step = sorted(iteration_numbers)[-1]\n",
        "  else:\n",
        "    if step_num_current_model in iteration_numbers:\n",
        "      model_step = step_num_current_model\n",
        "    else:\n",
        "      W  = '\\033[0m'  # white (normal)\n",
        "      R  = '\\033[31m' # red\n",
        "      print(R+'!! WARNING: The chosen model step does not exist !!'+W)\n",
        "      print('Please make sure you provide a valid model step before proceeding further.')\n",
        "  full_generation_model_path = os.path.join(model_path, model_name) + f'/ema_0.9999_{ str(model_step).zfill(6)}.pt'\n",
        "else:\n",
        "  full_generation_model_path = model_full_path\n",
        "\n",
        "  if os.path.exists(full_generation_model_path):\n",
        "    print(\"The \"+model_full_path+\" network will be used.\")\n",
        "  else:\n",
        "    W  = '\\033[0m'  # white (normal)\n",
        "    R  = '\\033[31m' # red\n",
        "    print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
        "    print('Please make sure you provide a valid model path and model name before proceeding further.')\n",
        "\n",
        "# Generate images\n",
        "image_sample.main(model_path=full_generation_model_path, output_path=Result_folder, image_size=image_size, num_samples=num_samples)\n",
        "\n",
        "# Save the generated images as tif files.\n",
        "main_folder = PosixPath(Result_folder)\n",
        "for file in main_folder.iterdir():\n",
        "    if file.name.endswith('npz'):\n",
        "        with np.load(file.__fspath__()) as data:\n",
        "            images = data[data.files[0]]\n",
        "            for i in range(images.shape[0]):\n",
        "              y = images[i]\n",
        "              io.imsave(os.path.join(Result_folder, f\"generated_image_{i:05d}.tif\"), y)\n",
        "\n",
        "print(\"Images saved into folder:\", Result_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnSk14AJkRtJ"
      },
      "source": [
        "## **6.2. Inspect the predicted output**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlkZUhj4kQ2Z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown ##Run this cell to display randomly chosen generated images with contrast and brightness control sliders.\n",
        "\n",
        "main_folder = PosixPath(Result_folder)\n",
        "for file in main_folder.iterdir():\n",
        "    if file.name.endswith('npz'):\n",
        "        with np.load(file.__fspath__()) as data:\n",
        "            images = data[data.files[0]]\n",
        "\n",
        "num_of_imgs_to_display = np.min([8, images.shape[0]])\n",
        "indices = np.random.choice(np.arange(images.shape[0]-1), num_of_imgs_to_display, replace=False)\n",
        "\n",
        "def update_images(contrast, brightness):\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.suptitle('Random Generated Samples');\n",
        "    for image in range(indices.shape[0]):\n",
        "        y = images[indices[image], :, :, :]\n",
        "        y = cv2.convertScaleAbs(y, alpha=contrast, beta=brightness)\n",
        "        plt.subplot(2, indices.shape[0], image + 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(y, interpolation='nearest', cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "interact(update_images, contrast=widgets.FloatSlider(min=0.1, max=2, step=0.1, value=1, description='Contrast:'),\n",
        "         brightness=widgets.IntSlider(min=-100, max=100, step=10, value=0, description='Brightness:'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7WDm6bkYkb"
      },
      "source": [
        "## **6.3. Download your generated images**\n",
        "---\n",
        "\n",
        "<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owyvVA3ndrwA"
      },
      "source": [
        "# **7. Version log**\n",
        "---\n",
        "<font size = 4>**v1.0**:  \n",
        "*   Compatible for .jpg and .tif formats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOn8U-VkerU"
      },
      "source": [
        "\n",
        "#**Thank you for using Diffusion Model for SMLM!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}