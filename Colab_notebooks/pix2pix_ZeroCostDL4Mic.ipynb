{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pix2pix_ZeroCostDL4Mic.ipynb","provenance":[{"file_id":"1mqcexfPBaIWuvMWWbJZUFtPoZoJJwrEA","timestamp":1589278334507},{"file_id":"159ARwlQE7-zi0EHxunOF_YPFLt-ZVU5x","timestamp":1587562499898},{"file_id":"1W-7NHehG5MRFILvZZzhPWWnOdJMkadb2","timestamp":1586332290412},{"file_id":"1pUetEQICxYWkYVaQIgdRH1EZBTl7oc2A","timestamp":1586292199692},{"file_id":"1MD36ZkM6XR9EuV12zimJmfCjzyeYZFWq","timestamp":1586269469061},{"file_id":"16A2mbaHzlEElntS8qkFBOsBvZG-mUeY6","timestamp":1586253795726},{"file_id":"1gJlcjOiSxr2buDOxmcFbT_d-GqwLjXtK","timestamp":1583343225796},{"file_id":"10yGI51WzHfgWgZAyE-EbkZFEvIOd6CP6","timestamp":1583171396283}],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"V9zNGvape2-I","colab_type":"text"},"source":["# **pix2pix**\n","\n","---\n","\n","<font size = 4>pix2pix is a deep-learning method allowing image-to-image translation from one image domain type to another image domain type. It was first published by [Isola *et al.* in 2016](https://arxiv.org/abs/1611.07004). The image transformation requires paired images for training (supervised learning) and is made possible here by using a conditional Generative Adversarial Network (GAN) architecture to use information from the input image and obtain the equivalent translated image.\n","\n","<font size = 4> **This particular notebook enables image-to-image translation learned from paired dataset. If you are interested in performing unpaired image-to-image translation, you should consider using the CycleGAN notebook instead.**\n","\n","---\n","\n","<font size = 4>*Disclaimer*:\n","\n","<font size = 4>This notebook is part of the *Zero-Cost Deep-Learning to Enhance Microscopy* project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories.\n","\n","<font size = 4>This notebook is based on the following paper: \n","\n","<font size = 4> **Image-to-Image Translation with Conditional Adversarial Networks** by Isola *et al.* on arXiv in 2016 (https://arxiv.org/abs/1611.07004)\n","\n","<font size = 4>The source code of the PyTorch implementation of pix2pix can be found here: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n","\n","<font size = 4>**Please also cite this original paper when using or developing this notebook.**"]},{"cell_type":"markdown","metadata":{"id":"N3azwKB9O0oW","colab_type":"text"},"source":["# **License**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"ByW6Vqdn9sYV","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Double click to see the license information\n","\n","#------------------------- LICENSE FOR ZeroCostDL4Mic------------------------------------\n","#This ZeroCostDL4Mic notebook is distributed under the MIT licence\n","\n","\n","\n","#------------------------- LICENSE FOR CycleGAN ------------------------------------\n","\n","#Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\n","#All rights reserved.\n","\n","#Redistribution and use in source and binary forms, with or without\n","#modification, are permitted provided that the following conditions are met:\n","\n","#* Redistributions of source code must retain the above copyright notice, this\n","#  list of conditions and the following disclaimer.\n","\n","#* Redistributions in binary form must reproduce the above copyright notice,\n","#  this list of conditions and the following disclaimer in the documentation\n","#  and/or other materials provided with the distribution.\n","\n","#THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n","#AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n","#IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n","#DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n","#FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n","#DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n","#SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n","#CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n","#OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n","#OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","\n","\n","#--------------------------- LICENSE FOR pix2pix --------------------------------\n","#BSD License\n","\n","#For pix2pix software\n","#Copyright (c) 2016, Phillip Isola and Jun-Yan Zhu\n","#All rights reserved.\n","\n","#Redistribution and use in source and binary forms, with or without\n","#modification, are permitted provided that the following conditions are met:\n","\n","#* Redistributions of source code must retain the above copyright notice, this\n","#  list of conditions and the following disclaimer.\n","\n","#* Redistributions in binary form must reproduce the above copyright notice,\n","#  this list of conditions and the following disclaimer in the documentation\n","#  and/or other materials provided with the distribution.\n","\n","#----------------------------- LICENSE FOR DCGAN --------------------------------\n","#BSD License\n","\n","#For dcgan.torch software\n","\n","#Copyright (c) 2015, Facebook, Inc. All rights reserved.\n","\n","#Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n","\n","#Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n","\n","#Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n","\n","#Neither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n","\n","#THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWAz2i7RdxUV","colab_type":"text"},"source":["# **How to use this notebook?**\n","\n","---\n","\n","<font size = 4>Video describing how to use our notebooks are available on youtube:\n","  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n","  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n","\n","\n","---\n","###**Structure of a notebook**\n","\n","<font size = 4>The notebook contains two types of cell:  \n","\n","<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n","\n","<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n","\n","---\n","###**Table of contents, Code snippets** and **Files**\n","\n","<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n","\n","<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n","\n","<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n","\n","<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here. \n","\n","<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n","\n","<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n","\n","---\n","###**Making changes to the notebook**\n","\n","<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n","\n","<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n","You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."]},{"cell_type":"markdown","metadata":{"id":"vNMDQHm0Ah-Z","colab_type":"text"},"source":["#**0. Before getting started**\n","---\n","<font size = 4> For pix2pix to train, **it needs to have access to a paired training dataset**. This means that the same image needs to be acquired in the two conditions and provided with indication of correspondence.\n","\n","<font size = 4> Therefore, the data structure is important. It is necessary that all the input data are in the same folder and that all the output data is in a separate folder. The provided training dataset is already split in two folders called Training_source and Training_target. Information on how to generate a training dataset is available in our Wiki page: https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\n","\n","<font size = 4>**We strongly recommend that you generate extra paired images. These images can be used to assess the quality of your trained model (Quality control dataset)**. The quality control assessment can be done directly in this notebook.\n","\n","<font size = 4> **Additionally, the corresponding input and output files need to have the same name**.\n","\n","<font size = 4> Please note that you currently can **only use .PNG files!**\n","\n","\n","<font size = 4>Here's a common data structure that can work:\n","*   Experiment A\n","    - **Training dataset**\n","      - Training_source\n","        - img_1.png, img_2.png, ...\n","      - Training_target\n","        - img_1.png, img_2.png, ...\n","    - **Quality control dataset**\n","     - Training_source\n","        - img_1.png, img_2.png\n","      - Training_target\n","        - img_1.png, img_2.png\n","    - **Data to be predicted**\n","    - **Results**\n","\n","---\n","<font size = 4>**Important note**\n","\n","<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to run predictions using the model that you trained.\n","\n","<font size = 4>- If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n","\n","<font size = 4>- If you only wish to **run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n","---"]},{"cell_type":"markdown","metadata":{"id":"DMNHVZfHmbKb","colab_type":"text"},"source":["# **1. Initialise the Colab session**\n","---\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BCPhV-pe-syw","colab_type":"text"},"source":["\n","## **1.1. Check for GPU access**\n","---\n","\n","By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n","\n","<font size = 4>Go to **Runtime -> Change the Runtime type**\n","\n","<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n","\n","<font size = 4>**Accelator: GPU** *(Graphics processing unit)*\n"]},{"cell_type":"code","metadata":{"id":"VNZetvLiS1qV","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run this cell to check if you have GPU access\n","\n","\n","import tensorflow as tf\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBrnApIUBgxv","colab_type":"text"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. \n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","metadata":{"id":"01Djr8v-5pPk","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run this cell to connect your Google Drive to Colab\n","\n","#@markdown * Click on the URL. \n","\n","#@markdown * Sign in your Google Account. \n","\n","#@markdown * Copy the authorization code. \n","\n","#@markdown * Enter the authorization code. \n","\n","#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n","\n","#mounts user's Google Drive to Google Colab.\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4yWFoJNnoin","colab_type":"text"},"source":["# **2. Install pix2pix and dependencies**\n","---\n"]},{"cell_type":"code","metadata":{"id":"3u2mXn3XsWzd","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Install pix2pix and dependencies\n","\n","\n","\n","#Here, we install libraries which are not already included in Colab.\n","\n","\n","!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n","\n","import os\n","os.chdir('pytorch-CycleGAN-and-pix2pix/')\n","!pip install -r requirements.txt\n","\n","import imageio\n","from skimage import data\n","from skimage import exposure\n","from skimage.exposure import match_histograms\n","import glob\n","import os.path\n","\n","\n","\n","# ------- Common variable to all ZeroCostDL4Mic notebooks -------\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import urllib\n","import os, random\n","import shutil \n","import zipfile\n","from tifffile import imread, imsave\n","import time\n","import sys\n","from pathlib import Path\n","import pandas as pd\n","import csv\n","from glob import glob\n","from scipy import signal\n","from scipy import ndimage\n","from skimage import io\n","from sklearn.linear_model import LinearRegression\n","from skimage.util import img_as_uint\n","import matplotlib as mpl\n","from skimage.metrics import structural_similarity\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from astropy.visualization import simple_norm\n","from skimage import img_as_float32\n","from skimage.util import img_as_ubyte\n","from tqdm import tqdm \n","\n","\n","# Colors for the warning messages\n","class bcolors:\n","  WARNING = '\\033[31m'\n","\n","#Disable some of the tensorflow warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print('----------------------------')\n","print(\"Libraries installed\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fw0kkTU6CsU4","colab_type":"text"},"source":["# **3. Select your parameters and paths**\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"BLmBseWbRvxL","colab_type":"text"},"source":["## **3.1. Setting main training parameters**\n","---\n","<font size = 4>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CB6acvUFtWqd","colab_type":"text"},"source":["<font size = 5> **Paths for training, predictions and results**\n","\n","<font size = 4>**`Training_source:`, `Training_target`:** These are the paths to your folders containing the Training_source and Training_target training data respecively. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n","\n","<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n","\n","<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n","\n","<font size = 5>**Training parameters**\n","\n","<font size = 4>**`number_of_epochs`:**Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10) epochs, but a full training should run for 200 epochs or more. Evaluate the performance after training (see 5). **Default value: 200**\n","\n","<font size = 5>**Advanced Parameters - experienced users only**\n","\n","<font size = 4>**`patch_size`:** pix2pix divides the image into patches for training. Input the size of the patches (length of a side). The value should be smaller than the dimensions of the image and divisible by 8. **Default value: 512**\n","\n","<font size = 4>**When choosing the patch_size, the value should be i) large enough that it will enclose many instances, ii) small enough that the resulting patches fit into the RAM.**<font size = 4> \n","\n","<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 1**\n","\n","<font size = 4>**`initial_learning_rate`:** Input the initial value to be used as learning rate. **Default value: 0.0002**"]},{"cell_type":"code","metadata":{"id":"pIrTwJjzwV-D","colab_type":"code","cellView":"form","colab":{}},"source":["\n","\n","#@markdown ###Path to training images:\n","\n","Training_source = \"\" #@param {type:\"string\"}\n","InputFile = Training_source+\"/*.png\"\n","\n","Training_target = \"\" #@param {type:\"string\"}\n","OutputFile = Training_target+\"/*.png\"\n","\n","\n","#Define where the patch file will be saved\n","base = \"/content\"\n","\n","\n","# model name and path\n","#@markdown ###Name of the model and path to model folder:\n","model_name = \"\" #@param {type:\"string\"}\n","model_path = \"\" #@param {type:\"string\"}\n","\n","# other parameters for training.\n","#@markdown ###Training Parameters\n","#@markdown Number of epochs:\n","number_of_epochs =  200#@param {type:\"number\"}\n","\n","#@markdown ###Advanced Parameters\n","\n","Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n","#@markdown ###If not, please input:\n","patch_size =  512#@param {type:\"number\"} # in pixels\n","batch_size =  1#@param {type:\"number\"}\n","initial_learning_rate = 0.0002 #@param {type:\"number\"}\n","\n","\n","if (Use_Default_Advanced_Parameters): \n","  print(\"Default advanced parameters enabled\")\n","  batch_size = 1\n","  patch_size =  512\n","  initial_learning_rate = 0.0002\n","\n","#here we check that no model with the same name already exist, if so delete\n","if os.path.exists(model_path+'/'+model_name):\n","  print(bcolors.WARNING +\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n","  print(bcolors.WARNING +\"To continue training \"+model_name+\", choose a new model_name here, and load \"+model_name+\" in section 3.3\")\n","  \n","#To use pix2pix we need to organise the data in a way the network can understand\n","\n","Saving_path= \"/content/\"+model_name\n","#Saving_path= model_path+\"/\"+model_name\n","\n","if os.path.exists(Saving_path):\n","  shutil.rmtree(Saving_path)\n","os.makedirs(Saving_path)\n","\n","imageA_folder = Saving_path+\"/A\"\n","os.makedirs(imageA_folder)\n","\n","imageB_folder = Saving_path+\"/B\"\n","os.makedirs(imageB_folder)\n","\n","imageAB_folder = Saving_path+\"/AB\"\n","os.makedirs(imageAB_folder)\n","\n","TrainA_Folder = Saving_path+\"/A/train\"\n","os.makedirs(TrainA_Folder)\n","  \n","TrainB_Folder = Saving_path+\"/B/train\"\n","os.makedirs(TrainB_Folder)\n","\n","# Here we disable pre-trained model by default (in case the  cell is not ran)\n","Use_pretrained_model = False\n","\n","# Here we disable data augmentation by default (in case the cell is not ran)\n","\n","Use_Data_augmentation = False\n","\n","\n","# This will display a randomly chosen dataset input and output\n","random_choice = random.choice(os.listdir(Training_source))\n","x = imageio.imread(Training_source+\"/\"+random_choice)\n","\n","#Find image XY dimension\n","Image_Y = x.shape[0]\n","Image_X = x.shape[1]\n","\n","Image_min_dim = min(Image_Y, Image_X)\n","\n","\n","#Hyperparameters failsafes\n","if patch_size > min(Image_Y, Image_X):\n","  patch_size = min(Image_Y, Image_X)\n","  print (bcolors.WARNING + \" Your chosen patch_size is bigger than the xy dimension of your image; therefore the patch_size chosen is now:\",patch_size)\n","\n","# Here we check that patch_size is divisible by 4\n","if not patch_size % 4 == 0:\n","    patch_size = ((int(patch_size / 4)-1) * 4)\n","    print (bcolors.WARNING + \" Your chosen patch_size is not divisible by 4; therefore the patch_size chosen is now:\",patch_size)\n","\n","# Here we check that patch_size is at least bigger than 256\n","if patch_size < 256:\n","  patch_size = 256\n","  print (bcolors.WARNING + \" Your chosen patch_size is too small; therefore the patch_size chosen is now:\",patch_size)\n","\n","\n","\n","y = imageio.imread(Training_target+\"/\"+random_choice)\n","\n","f=plt.figure(figsize=(16,8))\n","plt.subplot(1,2,1)\n","plt.imshow(x, interpolation='nearest')\n","plt.title('Training source')\n","plt.axis('off');\n","\n","plt.subplot(1,2,2)\n","plt.imshow(y, interpolation='nearest')\n","plt.title('Training target')\n","plt.axis('off');\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LEowmfAWqPs","colab_type":"text"},"source":["## **3.2. Data augmentation**\n","---\n","<font size = 4>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Flz3qoQrWv0v","colab_type":"text"},"source":["<font size = 4>Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation is not necessary for training and if your training dataset is large you should disable it.\n","\n","<font size = 4>Data augmentation is performed here by [Augmentor.](https://github.com/mdbloice/Augmentor)\n","\n","<font size = 4>[Augmentor](https://github.com/mdbloice/Augmentor) was described in the following article:\n","\n","<font size = 4>Marcus D Bloice, Peter M Roth, Andreas Holzinger, Biomedical image augmentation using Augmentor, Bioinformatics, https://doi.org/10.1093/bioinformatics/btz259\n","\n","<font size = 4>**Please also cite this original paper when publishing results obtained using this notebook with augmentation enabled.** "]},{"cell_type":"code","metadata":{"id":"OsIBK-sywkfy","colab_type":"code","cellView":"form","colab":{}},"source":["#Data augmentation\n","\n","Use_Data_augmentation = False #@param {type:\"boolean\"}\n","\n","if Use_Data_augmentation:\n","  !pip install Augmentor\n","  import Augmentor\n","\n","\n","#@markdown ####Choose a factor by which you want to multiply your original dataset\n","\n","Multiply_dataset_by = 4 #@param {type:\"slider\", min:1, max:30, step:1}\n","\n","Save_augmented_images = False #@param {type:\"boolean\"}\n","\n","Saving_path = \"\" #@param {type:\"string\"}\n","\n","\n","Use_Default_Augmentation_Parameters = True #@param {type:\"boolean\"}\n","#@markdown ###If not, please choose the probability of the following image manipulations to be used to augment your dataset (1 = always used; 0 = disabled ):\n","\n","#@markdown ####Mirror and rotate images\n","rotate_90_degrees = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","rotate_270_degrees = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","flip_left_right = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","flip_top_bottom = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","#@markdown ####Random image Zoom\n","\n","random_zoom = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","random_zoom_magnification = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","#@markdown ####Random image distortion\n","\n","random_distortion = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","\n","#@markdown ####Image shearing and skewing  \n","\n","image_shear = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","max_image_shear = 1 #@param {type:\"slider\", min:1, max:25, step:1}\n","\n","skew_image = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","skew_image_magnitude = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","\n","if Use_Default_Augmentation_Parameters:\n","  rotate_90_degrees = 0.5\n","  rotate_270_degrees = 0.5\n","  flip_left_right = 0.5\n","  flip_top_bottom = 0.5\n","\n","  if not Multiply_dataset_by >5:\n","    random_zoom = 0\n","    random_zoom_magnification = 0.9\n","    random_distortion = 0\n","    image_shear = 0\n","    max_image_shear = 10\n","    skew_image = 0\n","    skew_image_magnitude = 0\n","\n","  if Multiply_dataset_by >5:\n","    random_zoom = 0.1\n","    random_zoom_magnification = 0.9\n","    random_distortion = 0.5\n","    image_shear = 0.2\n","    max_image_shear = 5\n","    skew_image = 0.2\n","    skew_image_magnitude = 0.4\n","\n","  if Multiply_dataset_by >25:\n","    random_zoom = 0.5\n","    random_zoom_magnification = 0.8\n","    random_distortion = 0.5\n","    image_shear = 0.5\n","    max_image_shear = 20\n","    skew_image = 0.5\n","    skew_image_magnitude = 0.6\n","\n","\n","list_files = os.listdir(Training_source)\n","Nb_files = len(list_files)\n","\n","Nb_augmented_files = (Nb_files * Multiply_dataset_by)\n","\n","\n","if Use_Data_augmentation:\n","  print(\"Data augmentation enabled\")\n","# Here we set the path for the various folder were the augmented images will be loaded\n","\n","# All images are first saved into the augmented folder\n","  #Augmented_folder = \"/content/Augmented_Folder\"\n","  \n","  if not Save_augmented_images:\n","    Saving_path= \"/content\"\n","\n","  Augmented_folder =  Saving_path+\"/Augmented_Folder\"\n","  if os.path.exists(Augmented_folder):\n","    shutil.rmtree(Augmented_folder)\n","  os.makedirs(Augmented_folder)\n","\n","  #Training_source_augmented = \"/content/Training_source_augmented\"\n","  Training_source_augmented = Saving_path+\"/Training_source_augmented\"\n","\n","  if os.path.exists(Training_source_augmented):\n","    shutil.rmtree(Training_source_augmented)\n","  os.makedirs(Training_source_augmented)\n","\n","  #Training_target_augmented = \"/content/Training_target_augmented\"\n","  Training_target_augmented = Saving_path+\"/Training_target_augmented\"\n","\n","  if os.path.exists(Training_target_augmented):\n","    shutil.rmtree(Training_target_augmented)\n","  os.makedirs(Training_target_augmented)\n","\n","\n","# Here we generate the augmented images\n","#Load the images\n","  p = Augmentor.Pipeline(Training_source, Augmented_folder)\n","\n","#Define the matching images\n","  p.ground_truth(Training_target)\n","#Define the augmentation possibilities\n","  if not rotate_90_degrees == 0:\n","    p.rotate90(probability=rotate_90_degrees)\n","  \n","  if not rotate_270_degrees == 0:\n","    p.rotate270(probability=rotate_270_degrees)\n","\n","  if not flip_left_right == 0:\n","    p.flip_left_right(probability=flip_left_right)\n","\n","  if not flip_top_bottom == 0:\n","    p.flip_top_bottom(probability=flip_top_bottom)\n","\n","  if not random_zoom == 0:\n","    p.zoom_random(probability=random_zoom, percentage_area=random_zoom_magnification)\n"," \n","  if not random_distortion == 0:\n","    p.random_distortion(probability=random_distortion, grid_width=4, grid_height=4, magnitude=8)\n","\n","  if not image_shear == 0:\n","    p.shear(probability=image_shear,max_shear_left=20,max_shear_right=20)\n","  \n","  if not skew_image == 0:\n","    p.skew(probability=skew_image,magnitude=skew_image_magnitude)\n","\n","  p.sample(int(Nb_augmented_files))\n","\n","  print(int(Nb_augmented_files),\"matching images generated\")\n","\n","# Here we sort through the images and move them back to augmented trainning source and targets folders\n","\n","  augmented_files = os.listdir(Augmented_folder)\n","\n","  for f in augmented_files:\n","\n","    if (f.startswith(\"_groundtruth_(1)_\")):\n","      shortname_noprefix = f[17:]\n","      shutil.copyfile(Augmented_folder+\"/\"+f, Training_target_augmented+\"/\"+shortname_noprefix) \n","    if not (f.startswith(\"_groundtruth_(1)_\")):\n","      shutil.copyfile(Augmented_folder+\"/\"+f, Training_source_augmented+\"/\"+f)\n","      \n","\n","  for filename in os.listdir(Training_source_augmented):\n","    os.chdir(Training_source_augmented)\n","    os.rename(filename, filename.replace('_original', ''))\n","  \n","  #Here we clean up the extra files\n","  shutil.rmtree(Augmented_folder)\n","\n","if not Use_Data_augmentation:\n","  print(bcolors.WARNING+\"Data augmentation disabled\") \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-leE8pEWRkn","colab_type":"text"},"source":["\n","## **3.3. Using weights from a pre-trained model as initial weights**\n","---\n","<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a pix2pix model**. \n","\n","<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**.\n"]},{"cell_type":"code","metadata":{"id":"CbOcS3wiWV9w","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ##Loading weights from a pre-trained network\n","\n","\n","Use_pretrained_model = False #@param {type:\"boolean\"}\n","\n","\n","#@markdown ###If not, please provide the path to the model folder:\n","pretrained_model_path = \"\" #@param {type:\"string\"}\n","\n","# --------------------- Check if we load a previously trained model ------------------------\n","if Use_pretrained_model:\n","\n","  h5_file_path = os.path.join(pretrained_model_path, \"latest_net_G.pth\")\n","  \n","\n","# --------------------- Check the model exist ------------------------\n","\n","  if not os.path.exists(h5_file_path):\n","    print(bcolors.WARNING+'WARNING: Pretrained model does not exist')\n","    Use_pretrained_model = False\n","    print(bcolors.WARNING+'No pretrained network will be used.')\n","\n","  if os.path.exists(h5_file_path):\n","    print(\"Pretrained model \"+os.path.basename(pretrained_model_path)+\" was found and will be loaded prior to training.\")\n","    \n","else:\n","  print(bcolors.WARNING+'No pretrained network will be used.')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQndJj70FzfL","colab_type":"text"},"source":["# **4. Train the network**\n","---"]},{"cell_type":"markdown","metadata":{"id":"-A4ipz8gs3Ew","colab_type":"text"},"source":["## **4.1. Prepare the training data for training**\n","---\n","<font size = 4>Here, we use the information from Section 3 to prepare the training data into a suitable format for training. **Your data will be copied in the google Colab \"content\" folder which may take some time depending on the size of your dataset.**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_V2ujGB60gDv","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Prepare the data for training\n","\n","\n","# --------------------- Here we load the augmented data or the raw data ------------------------\n","\n","if Use_Data_augmentation:\n","  Training_source_dir = Training_source_augmented\n","  Training_target_dir = Training_target_augmented\n","\n","if not Use_Data_augmentation:\n","  Training_source_dir = Training_source\n","  Training_target_dir = Training_target\n","# --------------------- ------------------------------------------------\n","\n","print(\"Data preparation in progress\")\n","\n","if os.path.exists(model_path+'/'+model_name):\n","  shutil.rmtree(model_path+'/'+model_name)\n","os.makedirs(model_path+'/'+model_name)\n","\n","#--------------- Here we move the files to trainA and train B ---------\n","\n","print('Copying training source data...')\n","for f in tqdm(os.listdir(Training_source_dir)):\n","    shutil.copyfile(Training_source_dir+\"/\"+f, TrainA_Folder+\"/\"+f)\n","\n","print('Copying training target data...')\n","for f in tqdm(os.listdir(Training_target_dir)):\n","    shutil.copyfile(Training_target_dir+\"/\"+f, TrainB_Folder+\"/\"+f)\n","\n","#---------------------------------------------------------------------\n","\n","#--------------- Here we combined A and B images---------\n","os.chdir(\"/content\")\n","!python pytorch-CycleGAN-and-pix2pix/datasets/combine_A_and_B.py --fold_A \"$imageA_folder\" --fold_B \"$imageB_folder\" --fold_AB \"$imageAB_folder\"\n","\n","\n","\n","# pix2pix uses EPOCH without lr decay and EPOCH with lr decay, here we automatically choose half and half\n","\n","number_of_epochs_lr_stable = int(number_of_epochs/2)\n","number_of_epochs_lr_decay = int(number_of_epochs/2)\n","\n","if Use_pretrained_model :\n","  for f in os.listdir(pretrained_model_path):\n","    if (f.startswith(\"latest_net_\")):      \n","      shutil.copyfile(pretrained_model_path+\"/\"+f, model_path+'/'+model_name+\"/\"+f)\n","\n","print('------------------------')\n","print(\"Data ready for training\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQPz0F6JlvJR","colab_type":"text"},"source":["## **4.2. Train the network**\n","---\n","<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n","\n","<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches or continue the training in a second Colab session. **Pix2pix will save model checkpoints every 5 epochs.**"]},{"cell_type":"code","metadata":{"id":"eBD50tAgv5qf","colab_type":"code","cellView":"form","colab":{}},"source":["\n","#@markdown ##Start training\n","\n","start = time.time()\n","\n","os.chdir(\"/content\")\n","\n","#--------------------------------- Command line inputs to change pix2pix paramaters------------\n","\n","       # basic parameters\n","        #('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n","        #('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n","        #('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n","        #('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n","       \n","       # model parameters\n","        #('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n","        #('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n","        #('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n","        #('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n","        #('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n","        #('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n","        #('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n","        #('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n","        #('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n","        #('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n","        #('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n","        #('--no_dropout', action='store_true', help='no dropout for the generator')\n","        \n","       # dataset parameters\n","        #('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n","        #('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n","        #('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n","        #('--num_threads', default=4, type=int, help='# threads for loading data')\n","        #('--batch_size', type=int, default=1, help='input batch size')\n","        #('--load_size', type=int, default=286, help='scale images to this size')\n","        #('--crop_size', type=int, default=256, help='then crop to this size')\n","        #('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n","        #('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n","        #('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n","        #('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n","        \n","       # additional parameters\n","        #('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n","        #('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n","        #('--verbose', action='store_true', help='if specified, print more debugging information')\n","        #('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n","        \n","       # visdom and HTML visualization parameters\n","        #('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n","        #('--display_ncols', type=int, default=4, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n","        #('--display_id', type=int, default=1, help='window id of the web display')\n","        #('--display_server', type=str, default=\"http://localhost\", help='visdom server of the web display')\n","        #('--display_env', type=str, default='main', help='visdom display environment name (default is \"main\")')\n","        #('--display_port', type=int, default=8097, help='visdom port of the web display')\n","        #('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n","        #('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n","        #('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n","        \n","       # network saving and loading parameters\n","        #('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n","        #('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n","        #('--save_by_iter', action='store_true', help='whether saves model by iteration')\n","        #('--continue_train', action='store_true', help='continue training: load the latest model')\n","        #('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n","        #('--phase', type=str, default='train', help='train, val, test, etc')\n","        \n","       # training parameters\n","        #('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')\n","        #('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')\n","        #('--beta1', type=float, default=0.5, help='momentum term of adam')\n","        #('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n","        #('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n","        #('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n","        #('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')\n","        #('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations'\n","\n","#---------------------------------------------------------\n","\n","#----- Start the training ------------------------------------\n","if not Use_pretrained_model:\n","  !python pytorch-CycleGAN-and-pix2pix/train.py --dataroot \"$imageAB_folder\" --name $model_name --model pix2pix --batch_size $batch_size --preprocess scale_width_and_crop --load_size $Image_min_dim --crop_size $patch_size --checkpoints_dir \"$model_path\"  --no_html --n_epochs $number_of_epochs_lr_stable --n_epochs_decay $number_of_epochs_lr_decay --lr $initial_learning_rate --display_id 0 --save_epoch_freq 5\n","\n","if Use_pretrained_model:\n","  !python pytorch-CycleGAN-and-pix2pix/train.py --dataroot \"$imageAB_folder\" --name $model_name --model pix2pix --batch_size $batch_size --preprocess scale_width_and_crop --load_size $Image_min_dim --crop_size $patch_size --checkpoints_dir \"$model_path\"  --no_html --n_epochs $number_of_epochs_lr_stable --n_epochs_decay $number_of_epochs_lr_decay --lr $initial_learning_rate --display_id 0 --save_epoch_freq 5 --continue_train\n","\n","\n","#---------------------------------------------------------\n","\n","print(\"Training, done.\")\n","\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","mins, sec = divmod(dt, 60) \n","hour, mins = divmod(mins, 60) \n","print(\"Time elapsed:\",hour, \"hour(s)\",mins,\"min(s)\",round(sec),\"sec(s)\")\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQjQb_J_Qyku","colab_type":"text"},"source":["##**4.3. Download your model(s) from Google Drive**\n","\n","\n","---\n","<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder as all data can be erased at the next training if using the same folder."]},{"cell_type":"markdown","metadata":{"id":"2HbZd7rFqAad","colab_type":"text"},"source":["# **5. Evaluate your model**\n","---\n","\n","<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model. \n","\n","<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NEBRRG8QyEDG","colab_type":"text"},"source":["## **5.1. Choose the model you want to assess**"]},{"cell_type":"code","metadata":{"id":"EdcnkCr9Nbl8","colab_type":"code","cellView":"form","colab":{}},"source":["# model name and path\n","#@markdown ###Do you want to assess the model you just trained ?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, please provide the path to the model folder:\n","\n","QC_model_folder = \"\" #@param {type:\"string\"}\n","\n","#Here we define the loaded model name and path\n","QC_model_name = os.path.basename(QC_model_folder)\n","QC_model_path = os.path.dirname(QC_model_folder)\n","\n","if (Use_the_current_trained_model): \n","  QC_model_name = model_name\n","  QC_model_path = model_path\n","\n","full_QC_model_path = QC_model_path+'/'+QC_model_name+'/'\n","if os.path.exists(full_QC_model_path):\n","  print(\"The \"+QC_model_name+\" network will be evaluated\")\n","else:\n","  W  = '\\033[0m'  # white (normal)\n","  R  = '\\033[31m' # red\n","  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","  print('Please make sure you provide a valid model path and model name before proceeding further.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ry9qN2tlydXq","colab_type":"text"},"source":["## **5.2. Identify the best checkpoint to use to make predictions**"]},{"cell_type":"markdown","metadata":{"id":"1yauWCc78HKD","colab_type":"text"},"source":["<font size = 4> Pix2pix save model checkpoints every five epochs. Due to the stochastic nature of GAN networks, the last checkpoint is not always the best one to use. As a consequence, it can be challenging to choose the most suitable checkpoint to use to make predictions.\n","\n","<font size = 4>This section allows you to perform predictions using all the saved checkpoints and to estimate the quality of these predictions by comparing them to the provided ground truths images. Metric used include:\n","\n","<font size = 4>**1. The SSIM (structural similarity) map** \n","\n","<font size = 4>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info). \n","\n","<font size=4>**mSSIM** is the SSIM value calculated across the entire window of both images.\n","\n","<font size=4>**The output below shows the SSIM maps with the mSSIM**\n","\n","<font size = 4>**2. The RSE (Root Squared Error) map** \n","\n","<font size = 4>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n","\n","\n","<font size =4>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n","\n","<font size = 4>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n","\n","<font size=4>**The output below shows the RSE maps with the NRMSE and PSNR values.**\n","\n"]},{"cell_type":"code","metadata":{"id":"2nBPucJdK3KS","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Choose the folders that contain your Quality Control dataset\n","\n","import glob\n","import os.path\n","\n","\n","Source_QC_folder = \"\" #@param{type:\"string\"}\n","Target_QC_folder = \"\" #@param{type:\"string\"}\n","\n","Image_type = \"RGB\" #@param [\"Grayscale\", \"RGB\"]\n","\n","\n","# average function\n","def Average(lst): \n","    return sum(lst) / len(lst) \n","\n","\n","\n","# Create a quality control folder\n","\n","if os.path.exists(QC_model_path+\"/\"+QC_model_name+\"/Quality Control\"):\n","  shutil.rmtree(QC_model_path+\"/\"+QC_model_name+\"/Quality Control\")\n","\n","os.makedirs(QC_model_path+\"/\"+QC_model_name+\"/Quality Control\")\n","\n","\n","# Create a quality control/Prediction Folder\n","\n","QC_prediction_results = QC_model_path+\"/\"+QC_model_name+\"/Quality Control/Prediction\"\n","\n","if os.path.exists(QC_prediction_results):\n","  shutil.rmtree(QC_prediction_results)\n","\n","os.makedirs(QC_prediction_results)\n","\n","# Here we count how many images are in our folder to be predicted and we had a few\n","Nb_files_Data_folder = len(os.listdir(Source_QC_folder)) +10\n","\n","\n","\n","# List images in Source_QC_folder\n","# This will find the image dimension of a randomly choosen image in Source_QC_folder \n","random_choice = random.choice(os.listdir(Source_QC_folder))\n","x = imageio.imread(Source_QC_folder+\"/\"+random_choice)\n","\n","#Find image XY dimension\n","Image_Y = x.shape[0]\n","Image_X = x.shape[1]\n","\n","Image_min_dim = min(Image_Y, Image_X)\n","\n","\n","# Here we need to move the data to be analysed so that pix2pix can find them\n","\n","\n","Saving_path_QC= \"/content/\"+QC_model_name+\"_images\"\n","\n","if os.path.exists(Saving_path_QC):\n","  shutil.rmtree(Saving_path_QC)\n","os.makedirs(Saving_path_QC)\n","\n","Saving_path_QC_folder = Saving_path_QC+\"/QC\"\n","\n","if os.path.exists(Saving_path_QC_folder):\n","  shutil.rmtree(Saving_path_QC_folder)\n","os.makedirs(Saving_path_QC_folder)\n","\n","\n","imageA_folder = Saving_path_QC_folder+\"/A\"\n","os.makedirs(imageA_folder)\n","\n","imageB_folder = Saving_path_QC_folder+\"/B\"\n","os.makedirs(imageB_folder)\n","\n","imageAB_folder = Saving_path_QC_folder+\"/AB\"\n","os.makedirs(imageAB_folder)\n","\n","testAB_folder = Saving_path_QC_folder+\"/AB/test\"\n","os.makedirs(testAB_folder)\n","\n","testA_Folder = Saving_path_QC_folder+\"/A/test\"\n","os.makedirs(testA_Folder)\n","  \n","testB_Folder = Saving_path_QC_folder+\"/B/test\"\n","os.makedirs(testB_Folder)\n","\n","QC_checkpoint_folders = \"/content/\"+QC_model_name\n","\n","if os.path.exists(QC_checkpoint_folders):\n","  shutil.rmtree(QC_checkpoint_folders)\n","os.makedirs(QC_checkpoint_folders)\n","\n","\n","\n","for files in os.listdir(Source_QC_folder):\n","  shutil.copyfile(Source_QC_folder+\"/\"+files, testA_Folder+\"/\"+files)\n","\n","for files in os.listdir(Target_QC_folder):\n","  shutil.copyfile(Target_QC_folder+\"/\"+files, testB_Folder+\"/\"+files)\n","  \n","#Here we create a merged folder containing only imageA\n","os.chdir(\"/content\")\n","\n","!python pytorch-CycleGAN-and-pix2pix/datasets/combine_A_and_B.py --fold_A \"$imageA_folder\" --fold_B \"$imageB_folder\" --fold_AB \"$imageAB_folder\"\n","\n","\n","# This will find the image dimension of a randomly choosen image in Source_QC_folder \n","random_choice = random.choice(os.listdir(Source_QC_folder))\n","x = imageio.imread(Source_QC_folder+\"/\"+random_choice)\n","\n","#Find image XY dimension\n","Image_Y = x.shape[0]\n","Image_X = x.shape[1]\n","\n","Image_min_dim = int(min(Image_Y, Image_X))\n","\n","patch_size_QC = Image_min_dim\n","\n","if not patch_size_QC % 256 == 0:\n","  patch_size_QC = ((int(patch_size_QC / 256)) * 256)\n","  print (\" Your image dimensions are not divisible by 256; therefore your images have now been resized to:\",patch_size_QC)\n","\n","if patch_size_QC < 256:\n","  patch_size_QC = 256\n","\n","\n","Nb_Checkpoint = len(glob.glob(os.path.join(full_QC_model_path, '*G.pth')))\n","\n","\n","print(Nb_Checkpoint)\n","\n","\n","## Initiate list\n","\n","Checkpoint_list = []\n","Average_ssim_score_list = []\n","\n","\n","for j in range(1, len(glob.glob(os.path.join(full_QC_model_path, '*G.pth')))+1):\n","  checkpoints = j*5\n","\n","  if checkpoints == Nb_Checkpoint*5:\n","    checkpoints = \"latest\"\n","\n","\n","  print(\"The checkpoint currently analysed is =\"+str(checkpoints))\n","\n","  Checkpoint_list.append(checkpoints)\n","\n","\n","  # Create a quality control/Prediction Folder\n","\n","  QC_prediction_results = QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)\n","\n","  if os.path.exists(QC_prediction_results):\n","    shutil.rmtree(QC_prediction_results)\n","\n","  os.makedirs(QC_prediction_results)\n","\n","\n","  # Create a quality control/Prediction Folder\n","\n","  QC_prediction_results = QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)\n","\n","  if os.path.exists(QC_prediction_results):\n","    shutil.rmtree(QC_prediction_results)\n","\n","  os.makedirs(QC_prediction_results)\n","\n","\n","#---------------------------- Predictions are performed here ----------------------\n","  os.chdir(\"/content\")\n","  !python pytorch-CycleGAN-and-pix2pix/test.py --dataroot \"$imageAB_folder\" --name \"$QC_model_name\" --model pix2pix --epoch $checkpoints --no_dropout --preprocess scale_width --load_size $patch_size_QC --crop_size $patch_size_QC --results_dir \"$QC_prediction_results\" --checkpoints_dir \"$QC_model_path\" --direction AtoB --num_test $Nb_files_Data_folder\n","#-----------------------------------------------------------------------------------\n","\n","#Here we need to move the data again and remove all the unnecessary folders\n","\n","  Checkpoint_name = \"test_\"+str(checkpoints)\n","\n","  QC_results_images = QC_prediction_results+\"/\"+QC_model_name+\"/\"+Checkpoint_name+\"/images\"\n","\n","  QC_results_images_files = os.listdir(QC_results_images)\n","\n","  for f in QC_results_images_files:  \n","    shutil.copyfile(QC_results_images+\"/\"+f, QC_prediction_results+\"/\"+f)\n","\n","  os.chdir(\"/content\")  \n","\n","  #Here we clean up the extra files\n","  shutil.rmtree(QC_prediction_results+\"/\"+QC_model_name)\n","\n","\n","  #-------------------------------- QC for RGB ------------------------------------\n","  if Image_type == \"RGB\":\n","# List images in Source_QC_folder\n","# This will find the image dimension of a randomly choosen image in Source_QC_folder \n","    random_choice = random.choice(os.listdir(Source_QC_folder))\n","    x = imageio.imread(Source_QC_folder+\"/\"+random_choice)\n","\n","    def ssim(img1, img2):\n","      return structural_similarity(img1,img2,data_range=1.,full=True, multichannel=True)\n","\n","# Open and create the csv file that will contain all the QC metrics\n","    with open(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/\"+\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\", \"w\", newline='') as file:\n","        writer = csv.writer(file)\n","\n","    # Write the header in the csv file\n","        writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\"])\n","        \n","        \n","    # Initiate list\n","        ssim_score_list = []  \n","\n","\n","    # Let's loop through the provided dataset in the QC folders\n","\n","\n","        for i in os.listdir(Source_QC_folder):\n","          if not os.path.isdir(os.path.join(Source_QC_folder,i)):\n","            print('Running QC on: '+i)\n","\n","            shortname_no_PNG = i[:-4]\n","        \n","      # -------------------------------- Target test data (Ground truth) --------------------------------\n","            \n","            test_GT = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), shortname_no_PNG+\"_real_B.png\"))\n","\n","      # -------------------------------- Source test data --------------------------------\n","            test_source = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints),shortname_no_PNG+\"_real_A.png\"))\n","        \n","     \n","      # -------------------------------- Prediction --------------------------------\n","      \n","            test_prediction = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints),shortname_no_PNG+\"_fake_B.png\"))\n","          \n","          #--------------------------- Here we normalise using histograms matching--------------------------------\n","            test_prediction_matched = match_histograms(test_prediction, test_GT, multichannel=True)\n","            test_source_matched = match_histograms(test_source, test_GT, multichannel=True)\n","            \n","      # -------------------------------- Calculate the metric maps and save them --------------------------------\n","\n","      # Calculate the SSIM maps\n","            index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT, test_prediction_matched)\n","            index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT, test_source_matched)\n","\n","            ssim_score_list.append(index_SSIM_GTvsPrediction)\n","\n","      #Save ssim_maps\n","            img_SSIM_GTvsPrediction_8bit = (img_SSIM_GTvsPrediction* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/SSIM_GTvsPrediction_\"+shortname_no_PNG+'.tif',img_SSIM_GTvsPrediction_8bit)\n","            img_SSIM_GTvsSource_8bit = (img_SSIM_GTvsSource* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/SSIM_GTvsSource_\"+shortname_no_PNG+'.tif',img_SSIM_GTvsSource_8bit)\n","      \n","      \n","            writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource)])\n","\n","      #Here we calculate the ssim average for each image in each checkpoints\n","\n","        Average_SSIM_checkpoint = Average(ssim_score_list)\n","        Average_ssim_score_list.append(Average_SSIM_checkpoint)\n","\n","#------------------------------------------- QC for Grayscale ----------------------------------------------\n","\n","  if Image_type == \"Grayscale\":\n","    def ssim(img1, img2):\n","      return structural_similarity(img1,img2,data_range=1.,full=True, gaussian_weights=True, use_sample_covariance=False, sigma=1.5)\n","\n","\n","    def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n","\n","\n","      mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n","      ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n","      return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n","\n","\n","    def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n","  \n","      if dtype is not None:\n","        x   = x.astype(dtype,copy=False)\n","        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n","        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n","        eps = dtype(eps)\n","\n","        try:\n","            import numexpr\n","            x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n","        except ImportError:\n","            x =                   (x - mi) / ( ma - mi + eps )\n","\n","        if clip:\n","            x = np.clip(x,0,1)\n","\n","        return x\n","\n","    def norm_minmse(gt, x, normalize_gt=True):\n","    \n","      if normalize_gt:\n","        gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n","        x = x.astype(np.float32, copy=False) - np.mean(x)\n","        #x = x - np.mean(x)\n","        gt = gt.astype(np.float32, copy=False) - np.mean(gt)\n","        #gt = gt - np.mean(gt)\n","        scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n","        return gt, scale * x\n","\n","# Open and create the csv file that will contain all the QC metrics\n","    with open(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/\"+\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\", \"w\", newline='') as file:\n","        writer = csv.writer(file)\n","\n","    # Write the header in the csv file\n","        writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\", \"Prediction v. GT NRMSE\", \"Input v. GT NRMSE\", \"Prediction v. GT PSNR\", \"Input v. GT PSNR\"])  \n","\n","      \n","    \n","    # Let's loop through the provided dataset in the QC folders\n","\n","\n","        for i in os.listdir(Source_QC_folder):\n","          if not os.path.isdir(os.path.join(Source_QC_folder,i)):\n","            print('Running QC on: '+i)\n","\n","            ssim_score_list = []\n","            shortname_no_PNG = i[:-4]\n","      # -------------------------------- Target test data (Ground truth) --------------------------------\n","            test_GT_raw = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), shortname_no_PNG+\"_real_B.png\"))\n","          \n","            test_GT = test_GT_raw[:,:,2]\n","\n","      # -------------------------------- Source test data --------------------------------\n","            test_source_raw = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints),shortname_no_PNG+\"_real_A.png\"))\n","          \n","            test_source = test_source_raw[:,:,2]\n","\n","      # Normalize the images wrt each other by minimizing the MSE between GT and Source image\n","            test_GT_norm,test_source_norm = norm_minmse(test_GT, test_source, normalize_gt=True)\n","\n","      # -------------------------------- Prediction --------------------------------\n","            test_prediction_raw = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints),shortname_no_PNG+\"_fake_B.png\"))\n","          \n","            test_prediction = test_prediction_raw[:,:,2]\n","\n","      # Normalize the images wrt each other by minimizing the MSE between GT and prediction\n","            test_GT_norm,test_prediction_norm = norm_minmse(test_GT, test_prediction, normalize_gt=True)        \n","\n","\n","      # -------------------------------- Calculate the metric maps and save them --------------------------------\n","\n","      # Calculate the SSIM maps\n","            index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT_norm, test_prediction_norm)\n","            index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT_norm, test_source_norm)\n","\n","            ssim_score_list.append(index_SSIM_GTvsPrediction)\n","\n","      #Save ssim_maps\n","          \n","            img_SSIM_GTvsPrediction_8bit = (img_SSIM_GTvsPrediction* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/SSIM_GTvsPrediction_\"+shortname_no_PNG+'.tif',img_SSIM_GTvsPrediction_8bit)\n","            img_SSIM_GTvsSource_8bit = (img_SSIM_GTvsSource* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/SSIM_GTvsSource_\"+shortname_no_PNG+'.tif',img_SSIM_GTvsSource_8bit)\n","      \n","      # Calculate the Root Squared Error (RSE) maps\n","            img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n","            img_RSE_GTvsSource = np.sqrt(np.square(test_GT_norm - test_source_norm))\n","\n","      # Save SE maps\n","            img_RSE_GTvsPrediction_8bit = (img_RSE_GTvsPrediction* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/RSE_GTvsPrediction_\"+shortname_no_PNG+'.tif',img_RSE_GTvsPrediction_8bit)\n","            img_RSE_GTvsSource_8bit = (img_RSE_GTvsSource* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/RSE_GTvsSource_\"+shortname_no_PNG+'.tif',img_RSE_GTvsSource_8bit)\n","\n","\n","      # -------------------------------- Calculate the RSE metrics and save them --------------------------------\n","\n","      # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n","            NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n","            NRMSE_GTvsSource = np.sqrt(np.mean(img_RSE_GTvsSource))\n","        \n","      # We can also measure the peak signal to noise ratio between the images\n","            PSNR_GTvsPrediction = psnr(test_GT_norm,test_prediction_norm,data_range=1.0)\n","            PSNR_GTvsSource = psnr(test_GT_norm,test_source_norm,data_range=1.0)\n","\n","            writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource),str(NRMSE_GTvsPrediction),str(NRMSE_GTvsSource),str(PSNR_GTvsPrediction),str(PSNR_GTvsSource)])\n","\n","          #Here we calculate the ssim average for each image in each checkpoints\n","\n","        Average_SSIM_checkpoint = Average(ssim_score_list)\n","        Average_ssim_score_list.append(Average_SSIM_checkpoint)\n","\n","\n","# All data is now processed saved\n","  \n","\n","# -------------------------------- Display --------------------------------\n","\n","# Display the IoV vs Threshold plot\n","plt.figure(figsize=(20,5))\n","plt.plot(Checkpoint_list, Average_ssim_score_list, label=\"SSIM\")\n","plt.title('Checkpoints vs. SSIM')\n","plt.ylabel('SSIM')\n","plt.xlabel('Checkpoints')\n","plt.legend()\n","plt.show()\n","\n","\n","\n","# -------------------------------- Display RGB --------------------------------\n","\n","from ipywidgets import interact\n","import ipywidgets as widgets\n","\n","\n","if Image_type == \"RGB\":\n","  random_choice_shortname_no_PNG = shortname_no_PNG\n","\n","  @interact\n","  def show_results(file=os.listdir(Source_QC_folder), checkpoints=Checkpoint_list):\n","\n","    random_choice_shortname_no_PNG = file[:-4]\n","\n","    df1 = pd.read_csv(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/\"+\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\", header=0)\n","    df2 = df1.set_index(\"image #\", drop = False)\n","    index_SSIM_GTvsPrediction = df2.loc[file, \"Prediction v. GT mSSIM\"]\n","    index_SSIM_GTvsSource = df2.loc[file, \"Input v. GT mSSIM\"]\n","\n","#Setting up colours\n","    cmap = None\n","\n","\n","    plt.figure(figsize=(15,15))\n","\n","# Target (Ground-truth)\n","    plt.subplot(3,3,1)\n","    plt.axis('off')\n","    img_GT = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_real_B.png\"), as_gray=False, pilmode=\"RGB\")\n","    \n","    plt.imshow(img_GT, cmap = cmap)\n","    plt.title('Target',fontsize=15)\n","\n","# Source\n","    plt.subplot(3,3,2)\n","    plt.axis('off')\n","    img_Source = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_real_A.png\"), as_gray=False, pilmode=\"RGB\")\n","    plt.imshow(img_Source, cmap = cmap)\n","    plt.title('Source',fontsize=15)\n","\n","#Prediction\n","    plt.subplot(3,3,3)\n","    plt.axis('off')\n","\n","    img_Prediction = io.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_fake_B.png\"))\n","\n","    plt.imshow(img_Prediction, cmap = cmap)\n","    plt.title('Prediction',fontsize=15)\n","\n","\n","#SSIM between GT and Source\n","    plt.subplot(3,3,5)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","\n","    img_SSIM_GTvsSource = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"SSIM_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n","\n","    imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n","#plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n","    plt.title('Target vs. Source',fontsize=15)\n","    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsSource,3)),fontsize=14)\n","    plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n","\n","#SSIM between GT and Prediction\n","    plt.subplot(3,3,6)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False) \n","\n","    img_SSIM_GTvsPrediction = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"SSIM_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n","\n","    imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n","#plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n","    plt.title('Target vs. Prediction',fontsize=15)\n","    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsPrediction,3)),fontsize=14)\n","\n","\n","# -------------------------------- Display Grayscale --------------------------------\n","\n","if Image_type == \"Grayscale\":\n","  random_choice_shortname_no_PNG = shortname_no_PNG\n","\n","  @interact\n","  def show_results(file=os.listdir(Source_QC_folder), checkpoints=Checkpoint_list):\n","\n","    random_choice_shortname_no_PNG = file[:-4]\n","\n","    df1 = pd.read_csv(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/\"+\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\", header=0)\n","    df2 = df1.set_index(\"image #\", drop = False)\n","    index_SSIM_GTvsPrediction = df2.loc[file, \"Prediction v. GT mSSIM\"]\n","    index_SSIM_GTvsSource = df2.loc[file, \"Input v. GT mSSIM\"]\n","\n","    NRMSE_GTvsPrediction = df2.loc[file, \"Prediction v. GT NRMSE\"]\n","    NRMSE_GTvsSource = df2.loc[file, \"Input v. GT NRMSE\"]\n","    PSNR_GTvsSource = df2.loc[file, \"Input v. GT PSNR\"]\n","    PSNR_GTvsPrediction = df2.loc[file, \"Prediction v. GT PSNR\"]\n"," \n","\n","    plt.figure(figsize=(20,20))\n","  # Currently only displays the last computed set, from memory\n","  # Target (Ground-truth)\n","    plt.subplot(3,3,1)\n","    plt.axis('off')\n","    img_GT = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_real_B.png\"))\n","\n","    plt.imshow(img_GT, norm=simple_norm(img_GT, percent = 99))\n","    plt.title('Target',fontsize=15)\n","\n","# Source\n","    plt.subplot(3,3,2)\n","    plt.axis('off')\n","    img_Source = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_real_A.png\"))\n","    plt.imshow(img_Source, norm=simple_norm(img_Source, percent = 99))\n","    plt.title('Source',fontsize=15)\n","\n","#Prediction\n","    plt.subplot(3,3,3)\n","    plt.axis('off')\n","    img_Prediction = io.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_fake_B.png\"))\n","    plt.imshow(img_Prediction, norm=simple_norm(img_Prediction, percent = 99))\n","    plt.title('Prediction',fontsize=15)\n","\n","#Setting up colours\n","    cmap = plt.cm.CMRmap\n","\n","#SSIM between GT and Source\n","    plt.subplot(3,3,5)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","\n","    img_SSIM_GTvsSource = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"SSIM_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n","    img_SSIM_GTvsSource = img_SSIM_GTvsSource / 255\n","    imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n","\n","  \n","    plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n","    plt.title('Target vs. Source',fontsize=15)\n","    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsSource,3)),fontsize=14)\n","    plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n","\n","#SSIM between GT and Prediction\n","    plt.subplot(3,3,6)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)  \n","  \n","  \n","    img_SSIM_GTvsPrediction = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"SSIM_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n","    img_SSIM_GTvsPrediction = img_SSIM_GTvsPrediction / 255\n","    imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n","\n","  \n","    plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n","    plt.title('Target vs. Prediction',fontsize=15)\n","    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsPrediction,3)),fontsize=14)\n","\n","#Root Squared Error between GT and Source\n","    plt.subplot(3,3,8)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","\n","    img_RSE_GTvsSource = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"RSE_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n","    img_RSE_GTvsSource = img_RSE_GTvsSource / 255\n","  \n","\n","    imRSE_GTvsSource = plt.imshow(img_RSE_GTvsSource, cmap = cmap, vmin=0, vmax = 1)\n","    plt.colorbar(imRSE_GTvsSource,fraction=0.046,pad=0.04)\n","    plt.title('Target vs. Source',fontsize=15)\n","    plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsSource,3))+', PSNR: '+str(round(PSNR_GTvsSource,3)),fontsize=14)\n","#plt.title('Target vs. Source PSNR: '+str(round(PSNR_GTvsSource,3)))\n","    plt.ylabel('RSE maps',fontsize=20, rotation=0, labelpad=75)\n","\n","#Root Squared Error between GT and Prediction\n","    plt.subplot(3,3,9)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","\n","    img_RSE_GTvsPrediction = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"RSE_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n","\n","    img_RSE_GTvsPrediction = img_RSE_GTvsPrediction / 255\n","\n","    imRSE_GTvsPrediction = plt.imshow(img_RSE_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n","    plt.colorbar(imRSE_GTvsPrediction,fraction=0.046,pad=0.04)\n","    plt.title('Target vs. Prediction',fontsize=15)\n","    plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsPrediction,3))+', PSNR: '+str(round(PSNR_GTvsPrediction,3)),fontsize=14)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Esqnbew8uznk"},"source":["# **6. Using the trained model**\n","\n","---\n","\n","<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"d8wuQGjoq6eN","colab_type":"text"},"source":["## **6.1. Generate prediction(s) from unseen dataset**\n","---\n","\n","<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as PNG images.\n","\n","<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n","\n","<font size = 4>**`Result_folder`:** This folder will contain the predicted output images.\n","\n","<font size = 4>**`checkpoint`:** Choose the checkpoint number you would like to use to perform predictions. To use the \"latest\" checkpoint, input \"latest\".\n"]},{"cell_type":"code","metadata":{"id":"yb3suNkfpNA9","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ### Provide the path to your dataset and to the folder where the predictions are saved, then play the cell to predict outputs from your unseen images.\n","import glob\n","import os.path\n","\n","latest = \"latest\"\n","\n","Data_folder = \"\" #@param {type:\"string\"}\n","Result_folder = \"\" #@param {type:\"string\"}\n","\n","\n","# model name and path\n","#@markdown ###Do you want to use the current trained model?\n","Use_the_current_trained_model = False #@param {type:\"boolean\"}\n","\n","\n","#@markdown ###If not, please provide the path to the model folder:\n","\n","Prediction_model_folder = \"\" #@param {type:\"string\"}\n","\n","#@markdown ###What model checkpoint would you like to use?\n","\n","checkpoint = latest#@param {type:\"raw\"}\n","\n","\n","#Here we find the loaded model name and parent path\n","Prediction_model_name = os.path.basename(Prediction_model_folder)\n","Prediction_model_path = os.path.dirname(Prediction_model_folder)\n","\n","#here we check if we use the newly trained network or not\n","if (Use_the_current_trained_model): \n","  print(\"Using current trained network\")\n","  Prediction_model_name = model_name\n","  Prediction_model_path = model_path\n","\n","\n","#here we check if the model exists\n","full_Prediction_model_path = Prediction_model_path+'/'+Prediction_model_name+'/'\n","\n","if os.path.exists(full_Prediction_model_path):\n","  print(\"The \"+Prediction_model_name+\" network will be used.\")\n","else:\n","  W  = '\\033[0m'  # white (normal)\n","  R  = '\\033[31m' # red\n","  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","  print('Please make sure you provide a valid model path and model name before proceeding further.')\n","\n","Nb_Checkpoint = len(glob.glob(os.path.join(full_Prediction_model_path, '*G.pth')))+1\n","\n","\n","if not checkpoint == \"latest\":\n","\n","  if  checkpoint < 10:\n","    checkpoint = 5\n","\n","  if not checkpoint % 5 == 0:\n","    checkpoint = ((int(checkpoint / 5)-1) * 5)\n","    print (bcolors.WARNING + \" Your chosen checkpoints is not divisible by 5; therefore the checkpoints chosen is now:\",checkpoints)\n","\n","\n","  \n","  if checkpoint == Nb_Checkpoint*5:\n","    checkpoint = \"latest\"\n","\n","  if checkpoint > Nb_Checkpoint*5:\n","    checkpoint = \"latest\"\n","\n","\n","# Here we need to move the data to be analysed so that pix2pix can find them\n","\n","Saving_path_prediction= \"/content/\"+Prediction_model_name\n","\n","if os.path.exists(Saving_path_prediction):\n","  shutil.rmtree(Saving_path_prediction)\n","os.makedirs(Saving_path_prediction)\n","\n","\n","imageA_folder = Saving_path_prediction+\"/A\"\n","os.makedirs(imageA_folder)\n","\n","imageB_folder = Saving_path_prediction+\"/B\"\n","os.makedirs(imageB_folder)\n","\n","imageAB_folder = Saving_path_prediction+\"/AB\"\n","os.makedirs(imageAB_folder)\n","\n","testAB_Folder = Saving_path_prediction+\"/AB/test\"\n","os.makedirs(testAB_Folder)\n","\n","testA_Folder = Saving_path_prediction+\"/A/test\"\n","os.makedirs(testA_Folder)\n","  \n","testB_Folder = Saving_path_prediction+\"/B/test\"\n","os.makedirs(testB_Folder)\n","\n","for files in os.listdir(Data_folder):\n","  shutil.copyfile(Data_folder+\"/\"+files, testA_Folder+\"/\"+files)\n","  shutil.copyfile(Data_folder+\"/\"+files, testB_Folder+\"/\"+files)\n","  \n","# Here we create a merged A / A image for the prediction\n","os.chdir(\"/content\")\n","!python pytorch-CycleGAN-and-pix2pix/datasets/combine_A_and_B.py --fold_A \"$imageA_folder\" --fold_B \"$imageB_folder\" --fold_AB \"$imageAB_folder\"\n","\n","\n","# Here we count how many images are in our folder to be predicted and we had a few\n","Nb_files_Data_folder = len(os.listdir(Data_folder)) +10\n","\n","\n","# This will find the image dimension of a randomly choosen image in Data_folder \n","random_choice = random.choice(os.listdir(Data_folder))\n","x = imageio.imread(Data_folder+\"/\"+random_choice)\n","\n","#Find image XY dimension\n","Image_Y = x.shape[0]\n","Image_X = x.shape[1]\n","\n","Image_min_dim = min(Image_Y, Image_X)\n","\n","\n","\n","#-------------------------------- Perform predictions -----------------------------\n","\n","#-------------------------------- Options that can be used to perform predictions -----------------------------\n","\n","# basic parameters\n","        #('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n","        #('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n","        #('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n","        #('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n","\n","# model parameters\n","        #('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n","        #('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n","        #('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n","        #('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n","        #('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n","        #('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n","        #('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n","        #('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n","        #('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n","        #('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n","        #('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n","        #('--no_dropout', action='store_true', help='no dropout for the generator')\n","        \n","# dataset parameters\n","        #('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n","        #('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n","        #('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n","        #('--num_threads', default=4, type=int, help='# threads for loading data')\n","        #('--batch_size', type=int, default=1, help='input batch size')\n","        #('--load_size', type=int, default=286, help='scale images to this size')\n","        #('--crop_size', type=int, default=256, help='then crop to this size')\n","        #('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n","        #('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n","        #('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n","        #('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n","        \n","# additional parameters\n","        #('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n","        #('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n","        #('--verbose', action='store_true', help='if specified, print more debugging information')\n","        #('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n","        \n","\n","        #('--ntest', type=int, default=float(\"inf\"), help='# of test examples.')\n","        #('--results_dir', type=str, default='./results/', help='saves results here.')\n","        #('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n","        #('--phase', type=str, default='test', help='train, val, test, etc')\n","\n","# Dropout and Batchnorm has different behavioir during training and test.\n","        #('--eval', action='store_true', help='use eval mode during test time.')\n","        #('--num_test', type=int, default=50, help='how many test images to run')\n","        # rewrite devalue values\n","        \n","# To avoid cropping, the load_size should be the same as crop_size\n","        #parser.set_defaults(load_size=parser.get_default('crop_size'))\n","\n","#------------------------------------------------------------------------\n","\n","\n","#---------------------------- Predictions are performed here ----------------------\n","\n","os.chdir(\"/content\")\n","\n","!python pytorch-CycleGAN-and-pix2pix/test.py --dataroot \"$imageAB_folder\" --name \"$Prediction_model_name\" --model pix2pix --no_dropout --preprocess scale_width --load_size $Image_min_dim --crop_size $Image_min_dim --results_dir \"$Result_folder\" --checkpoints_dir \"$Prediction_model_path\" --num_test $Nb_files_Data_folder --epoch $checkpoint\n","\n","#-----------------------------------------------------------------------------------\n","\n","\n","Checkpoint_name = \"test_\"+str(checkpoint)\n","\n","\n","Prediction_results_folder = Result_folder+\"/\"+Prediction_model_name+\"/\"+Checkpoint_name+\"/images\"\n","\n","Prediction_results_images = os.listdir(Prediction_results_folder)\n","\n","for f in Prediction_results_images:  \n","  if (f.endswith(\"_real_B.png\")):    \n","    os.remove(Prediction_results_folder+\"/\"+f)\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIe3CRD7XUxa","colab_type":"text"},"source":["## **6.2. Inspect the predicted output**\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"LmDP8xiwXTTL","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ##Run this cell to display a randomly chosen input and its corresponding predicted output.\n","import os\n","# This will display a randomly chosen dataset input and predicted output\n","random_choice = random.choice(os.listdir(Data_folder))\n","\n","\n","random_choice_no_extension = os.path.splitext(random_choice)\n","\n","\n","x = imageio.imread(Result_folder+\"/\"+Prediction_model_name+\"/test_\"+str(checkpoint)+\"/images/\"+random_choice_no_extension[0]+\"_real_A.png\")\n","\n","\n","y = imageio.imread(Result_folder+\"/\"+Prediction_model_name+\"/test_\"+str(checkpoint)+\"/images/\"+random_choice_no_extension[0]+\"_fake_B.png\")\n","\n","f=plt.figure(figsize=(16,8))\n","plt.subplot(1,2,1)\n","plt.imshow(x, interpolation='nearest')\n","plt.title('Input')\n","plt.axis('off');\n","\n","plt.subplot(1,2,2)\n","plt.imshow(y, interpolation='nearest')\n","plt.title('Prediction')\n","plt.axis('off');\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvkd66PldsXB","colab_type":"text"},"source":["## **6.3. Download your predictions**\n","---\n","\n","<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."]},{"cell_type":"markdown","metadata":{"id":"Rn9zpWpo0xNw","colab_type":"text"},"source":["\n","#**Thank you for using pix2pix!**"]}]}