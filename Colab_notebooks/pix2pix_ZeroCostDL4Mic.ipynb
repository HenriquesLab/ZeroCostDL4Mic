{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkSguVy8Xv83"
      },
      "source": [
        "# **pix2pix**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>pix2pix is a deep-learning method allowing image-to-image translation from one image domain type to another image domain type. It was first published by [Isola *et al.* in 2016](https://arxiv.org/abs/1611.07004). The image transformation requires paired images for training (supervised learning) and is made possible here by using a conditional Generative Adversarial Network (GAN) architecture to use information from the input image and obtain the equivalent translated image.\n",
        "\n",
        "<font size = 4> **This particular notebook enables image-to-image translation learned from paired dataset. If you are interested in performing unpaired image-to-image translation, you should consider using the CycleGAN notebook instead.**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>*Disclaimer*:\n",
        "\n",
        "<font size = 4>This notebook is part of the *Zero-Cost Deep-Learning to Enhance Microscopy* project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories.\n",
        "\n",
        "<font size = 4>This notebook is based on the following paper:\n",
        "\n",
        "<font size = 4> **Image-to-Image Translation with Conditional Adversarial Networks** by Isola *et al.* on arXiv in 2016 (https://arxiv.org/abs/1611.07004)\n",
        "\n",
        "<font size = 4>The source code of the PyTorch implementation of pix2pix can be found here: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "<font size = 4>**Please also cite this original paper when using or developing this notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7HfryEazzJE"
      },
      "source": [
        "# **Licenses**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### License for ZeroCostDL4Mic:\n",
        "<details>\n",
        "<summary> (click for more details) </summary>\n",
        "\n",
        "This ZeroCostDL4Mic notebook is distributed under the MIT licence\n",
        "\n",
        "</details>\n",
        "\n",
        "### License for CycleGAN:\n",
        "\n",
        "<details>\n",
        "<summary> (click for more details) </summary>\n",
        "\n",
        "Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice, this\n",
        "  list of conditions and the following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above copyright notice,\n",
        "  this list of conditions and the following disclaimer in the documentation\n",
        "  and/or other materials provided with the distribution.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
        "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\n",
        "</details>\n",
        "\n",
        "### License for pix2pix:\n",
        "\n",
        "<details>\n",
        "<summary> (click for more details) </summary>\n",
        "\n",
        "BSD License\n",
        "\n",
        "For pix2pix software\n",
        "Copyright (c) 2016, Phillip Isola and Jun-Yan Zhu\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice, this\n",
        "  list of conditions and the following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above copyright notice,\n",
        "  this list of conditions and the following disclaimer in the documentation\n",
        "  and/or other materials provided with the distribution.\n",
        "\n",
        "</details>\n",
        "\n",
        "### License for DCGAN:\n",
        "\n",
        "<details>\n",
        "<summary> (click for more details) </summary>\n",
        "\n",
        "BSD License\n",
        "\n",
        "For dcgan.torch software\n",
        "\n",
        "Copyright (c) 2015, Facebook, Inc. All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
        "\n",
        "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
        "\n",
        "Neither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWAz2i7RdxUV"
      },
      "source": [
        "# **How to use this notebook?**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>Video describing how to use our notebooks are available on youtube:\n",
        "  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n",
        "  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n",
        "\n",
        "\n",
        "---\n",
        "### **Structure of a notebook**\n",
        "\n",
        "<font size = 4>The notebook contains two types of cell:  \n",
        "\n",
        "<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n",
        "\n",
        "<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n",
        "\n",
        "---\n",
        "### **Table of contents, Code snippets** and **Files**\n",
        "\n",
        "<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n",
        "\n",
        "<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n",
        "\n",
        "<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n",
        "\n",
        "<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here.\n",
        "\n",
        "<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n",
        "\n",
        "<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n",
        "\n",
        "---\n",
        "### **Making changes to the notebook**\n",
        "\n",
        "<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n",
        "\n",
        "<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n",
        "You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKDLkLWUd-YX"
      },
      "source": [
        "# **0. Before getting started**\n",
        "---\n",
        "<font size = 4> For pix2pix to train, **it needs to have access to a paired training dataset**. This means that the same image needs to be acquired in the two conditions and provided with indication of correspondence.\n",
        "\n",
        "<font size = 4> Therefore, the data structure is important. It is necessary that all the input data are in the same folder and that all the output data is in a separate folder. The provided training dataset is already split in two folders called Training_source and Training_target. Information on how to generate a training dataset is available in our Wiki page: https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\n",
        "\n",
        "<font size = 4>**We strongly recommend that you generate extra paired images. These images can be used to assess the quality of your trained model (Quality control dataset)**. The quality control assessment can be done directly in this notebook.\n",
        "\n",
        "<font size = 4> **Additionally, the corresponding input and output files need to have the same name**.\n",
        "\n",
        "<font size = 4> Please note that you currently can **only use .PNG files!**\n",
        "\n",
        "\n",
        "<font size = 4>Here's a common data structure that can work:\n",
        "*   Experiment A\n",
        "    - **Training dataset**\n",
        "      - Training_source\n",
        "        - img_1.png, img_2.png, ...\n",
        "      - Training_target\n",
        "        - img_1.png, img_2.png, ...\n",
        "    - **Quality control dataset**\n",
        "     - Training_source\n",
        "        - img_1.png, img_2.png\n",
        "      - Training_target\n",
        "        - img_1.png, img_2.png\n",
        "    - **Data to be predicted**\n",
        "    - **Results**\n",
        "\n",
        "---\n",
        "<font size = 4>**Important note**\n",
        "\n",
        "<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to run predictions using the model that you trained.\n",
        "\n",
        "<font size = 4>- If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n",
        "\n",
        "<font size = 4>- If you only wish to **run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdN8B91xZO0x"
      },
      "source": [
        "# **1. Install pix2pix and dependencies**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fq21zJVFNASx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "Notebook_version = '1.17.6'\n",
        "Network = 'pix2pix'\n",
        "\n",
        "from builtins import any as b_any\n",
        "\n",
        "def get_requirements_path():\n",
        "    # Store requirements file in 'contents' directory\n",
        "    current_dir = os.getcwd()\n",
        "    dir_count = current_dir.count('/') - 1\n",
        "    path = '../' * (dir_count) + 'requirements.txt'\n",
        "    return path\n",
        "\n",
        "def filter_files(file_list, filter_list):\n",
        "    filtered_list = []\n",
        "    for fname in file_list:\n",
        "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
        "            filtered_list.append(fname)\n",
        "    return filtered_list\n",
        "\n",
        "def build_requirements_file(before, after):\n",
        "    path = get_requirements_path()\n",
        "\n",
        "    # Exporting requirements.txt for local run\n",
        "    !pip freeze > $path\n",
        "\n",
        "    # Get minimum requirements file\n",
        "    df = pd.read_csv(path)\n",
        "    mod_list = [m.split('.')[0] for m in after if not m in before]\n",
        "    req_list_temp = df.values.tolist()\n",
        "    req_list = [x[0] for x in req_list_temp]\n",
        "\n",
        "    # Replace with package name and handle cases where import name is different to module name\n",
        "    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
        "    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n",
        "    filtered_list = filter_files(req_list, mod_replace_list)\n",
        "\n",
        "    file=open(path,'w')\n",
        "    for item in filtered_list:\n",
        "        file.writelines(item)\n",
        "\n",
        "    file.close()\n",
        "\n",
        "import sys\n",
        "before = [str(m) for m in sys.modules]\n",
        "\n",
        "#@markdown ##Install pix2pix and dependencies\n",
        "\n",
        "#Create a variable to get and store relative base path\n",
        "import os\n",
        "base_path = os.getcwd()\n",
        "\n",
        "#Here, we install libraries which are not already included in Colab.\n",
        "# We are clonning from a specific commit to ensure the code used keeps working in the future\n",
        "# https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/bd893b38d158d0b663321052c24dc1f4acccf552\n",
        "!git clone --depth 1 https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git > /dev/null 2>&1\n",
        "%cd pytorch-CycleGAN-and-pix2pix\n",
        "!git fetch origin bd893b38d158d0b663321052c24dc1f4acccf552 --depth 1 > /dev/null 2>&1\n",
        "!git checkout bd893b38d158d0b663321052c24dc1f4acccf552 > /dev/null 2>&1\n",
        "%cd $base_path\n",
        "\n",
        "\n",
        "pix2pix_code_dir = os.getcwd()\n",
        "os.chdir(os.path.join(pix2pix_code_dir, \"pytorch-CycleGAN-and-pix2pix\"))\n",
        "!pip install -r requirements.txt\n",
        "!pip install fpdf2\n",
        "!pip install lpips\n",
        "## Install the BioImage Model Zoo library\n",
        "!pip install bioimageio.core==\"0.5.11\"\n",
        "\n",
        "import lpips\n",
        "from PIL import Image\n",
        "import imageio\n",
        "from skimage import data\n",
        "from skimage import exposure\n",
        "from skimage.exposure import match_histograms\n",
        "import os.path\n",
        "\n",
        "# ------- Common variable to all ZeroCostDL4Mic notebooks -------\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import urllib\n",
        "import os, random\n",
        "import shutil\n",
        "import zipfile\n",
        "#from tifffile import imread, imsave\n",
        "import time\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import csv\n",
        "from glob import glob\n",
        "from scipy import signal, ndimage, stats\n",
        "from skimage import io\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from skimage.util import img_as_uint\n",
        "import matplotlib as mpl\n",
        "from skimage.metrics import structural_similarity\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from astropy.visualization import simple_norm\n",
        "from skimage import img_as_float32\n",
        "from skimage.util import img_as_ubyte\n",
        "from tqdm import tqdm\n",
        "from fpdf import FPDF, HTMLMixin\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "from pip._internal.operations.freeze import freeze\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "from bioimageio.core.build_spec import build_model\n",
        "from bioimageio.core.resource_tests import test_model\n",
        "from bioimageio.core import predict_image\n",
        "\n",
        "# Colors for the warning messages\n",
        "class bcolors:\n",
        "  WARNING = '\\033[31m'\n",
        "\n",
        "#Disable some of the tensorflow warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print('----------------------------')\n",
        "print(\"Libraries installed\")\n",
        "\n",
        "\n",
        "# Check if this is the latest version of the notebook\n",
        "All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n",
        "print('Notebook version: '+Notebook_version)\n",
        "Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n",
        "print('Latest notebook version: '+Latest_Notebook_version)\n",
        "if Notebook_version == Latest_Notebook_version:\n",
        "  print(\"This notebook is up-to-date.\")\n",
        "else:\n",
        "  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
        "\n",
        "# average function\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "def ssim(img1, img2, multichannel=False):\n",
        "    if multichannel:\n",
        "        return structural_similarity(img1,img2,data_range=1.,full=True, multichannel=True)\n",
        "    else:\n",
        "        return structural_similarity(img1,img2,data_range=1.,full=True, gaussian_weights=True, use_sample_covariance=False, sigma=1.5)\n",
        "\n",
        "\n",
        "def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
        "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
        "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
        "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
        "\n",
        "\n",
        "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):\n",
        "    x   = x.astype(dtype,copy=False)\n",
        "    mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
        "    ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
        "    eps = dtype(eps)\n",
        "\n",
        "    try:\n",
        "        import numexpr\n",
        "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
        "    except ImportError:\n",
        "        x =                   (x - mi) / ( ma - mi + eps )\n",
        "\n",
        "    if clip:\n",
        "        x = np.clip(x,0,1)\n",
        "\n",
        "    return x\n",
        "\n",
        "def norm_minmse(gt, x):\n",
        "    gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n",
        "    x = x.astype(np.float32, copy=False) - np.mean(x)\n",
        "    #x = x - np.mean(x)\n",
        "    gt = gt.astype(np.float32, copy=False) - np.mean(gt)\n",
        "    #gt = gt - np.mean(gt)\n",
        "    scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n",
        "    return gt, scale * x\n",
        "\n",
        "def prepare_qc_dir(QC_model_path, QC_model_name, Im_path):\n",
        "    # Create a quality control/Prediction Folder\n",
        "    QC_prediction_results = QC_model_path+\"/\"+QC_model_name+\"/Quality Control/Prediction\"\n",
        "\n",
        "    if os.path.exists(QC_prediction_results):\n",
        "        print(\"The QC folder has been removed to save the new results\")\n",
        "        shutil.rmtree(QC_prediction_results)\n",
        "\n",
        "    os.makedirs(QC_prediction_results, exist_ok=True)\n",
        "\n",
        "    # Here we need to move the data to be analysed so that pix2pix can find them\n",
        "    Saving_path_QC = os.path.join(Im_path, QC_model_name+\"_images\")\n",
        "\n",
        "    #if os.path.exists(Saving_path_QC):\n",
        "    #    shutil.rmtree(Saving_path_QC)\n",
        "    os.makedirs(Saving_path_QC, exist_ok=True)\n",
        "\n",
        "    # Folde to save the images in the correct format\n",
        "    Saving_path_QC_folder = os.path.join(Saving_path_QC, \"QC\")\n",
        "\n",
        "    os.makedirs(Saving_path_QC_folder, exist_ok=True)\n",
        "\n",
        "    imageA_folder = os.path.join(Saving_path_QC_folder, \"A\", \"test\")\n",
        "    os.makedirs(imageA_folder, exist_ok=True)\n",
        "\n",
        "    imageB_folder = os.path.join(Saving_path_QC_folder, \"B\", \"test\")\n",
        "    os.makedirs(imageB_folder, exist_ok=True)\n",
        "\n",
        "    imageAB_folder = os.path.join(Saving_path_QC_folder, \"AB\", \"test\")\n",
        "    os.makedirs(imageAB_folder, exist_ok=True)\n",
        "    return QC_prediction_results, Saving_path_QC_folder\n",
        "\n",
        "#Here we copy and normalise the data\n",
        "def normalise_data(Source_QC_folder, Target_QC_folder, Normalisation_QC_source, Normalisation_QC_target, Im_path):\n",
        "    if Normalisation_QC_source == \"Contrast stretching\":\n",
        "\n",
        "        for filename in os.listdir(Source_QC_folder):\n",
        "\n",
        "            img = io.imread(os.path.join(Source_QC_folder,filename)).astype(np.float32)\n",
        "            short_name = os.path.splitext(filename)\n",
        "\n",
        "            p2, p99 = np.percentile(img, (1., 99.9))\n",
        "            img = exposure.rescale_intensity(img, in_range=(p2, p99))\n",
        "\n",
        "            img = 255 * img # Now scale by 255\n",
        "            img = img.astype(np.uint8)\n",
        "            cv2.imwrite(os.path.join(Im_path, \"A\", \"test\", f\"{short_name[0]}.png\"), img)\n",
        "\n",
        "    if Normalisation_QC_target == \"Contrast stretching\":\n",
        "        for filename in os.listdir(Target_QC_folder):\n",
        "\n",
        "            img = io.imread(os.path.join(Target_QC_folder,filename)).astype(np.float32)\n",
        "            short_name = os.path.splitext(filename)\n",
        "\n",
        "            p2, p99 = np.percentile(img, (1., 99.9))\n",
        "            img = exposure.rescale_intensity(img, in_range=(p2, p99))\n",
        "\n",
        "            img = 255 * img # Now scale by 255\n",
        "            img = img.astype(np.uint8)\n",
        "            cv2.imwrite(os.path.join(Im_path, \"B\", \"test\", f\"{short_name[0]}.png\"), img)\n",
        "\n",
        "    if Normalisation_QC_source == \"Adaptive Equalization\":\n",
        "        for filename in os.listdir(Source_QC_folder):\n",
        "\n",
        "            img = io.imread(os.path.join(Source_QC_folder,filename))\n",
        "            short_name = os.path.splitext(filename)\n",
        "\n",
        "            img = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
        "\n",
        "            img = 255 * img # Now scale by 255\n",
        "            img = img.astype(np.uint8)\n",
        "            cv2.imwrite(os.path.join(Im_path, \"A\", \"test\", f\"{short_name[0]}.png\"), img)\n",
        "\n",
        "\n",
        "    if Normalisation_QC_target == \"Adaptive Equalization\":\n",
        "        for filename in os.listdir(Target_QC_folder):\n",
        "\n",
        "            img = io.imread(os.path.join(Target_QC_folder,filename))\n",
        "            short_name = os.path.splitext(filename)\n",
        "\n",
        "            img = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
        "\n",
        "            img = 255 * img # Now scale by 255\n",
        "            img = img.astype(np.uint8)\n",
        "            cv2.imwrite(os.path.join(Im_path, \"B\", \"test\", f\"{short_name[0]}.png\"), img)\n",
        "\n",
        "    if Normalisation_QC_source == \"None\":\n",
        "        for filename in os.listdir(Source_QC_folder):\n",
        "            img = io.imread(os.path.join(Source_QC_folder,filename))\n",
        "            short_name = os.path.splitext(filename)\n",
        "            cv2.imwrite(os.path.join(Im_path, \"A\", \"test\", f\"{short_name[0]}.png\"), img)\n",
        "\n",
        "    if Normalisation_QC_target == \"None\":\n",
        "        for filename in os.listdir(Target_QC_folder):\n",
        "            img = io.imread(os.path.join(Target_QC_folder,filename))\n",
        "            short_name = os.path.splitext(filename)\n",
        "            cv2.imwrite(os.path.join(Im_path, \"B\", \"test\", f\"{short_name[0]}.png\"), img)\n",
        "\n",
        "def QC_RGB(Source_QC_folder, QC_folder):\n",
        "\n",
        "    # List images in Source_QC_folder\n",
        "    csv_file = os.path.join(QC_folder, f\"QC_metrics_{QC_model_name}{str(checkpoints)}.csv\")\n",
        "    # Open and create the csv file that will contain all the QC metrics\n",
        "    with open(csv_file, \"w\", newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "    # Write the header in the csv file\n",
        "        writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\" , \"Prediction v. GT lpips\", \"Input v. GT lpips\"])\n",
        "    # Initiate list\n",
        "        ssim_score_list = []\n",
        "        lpips_score_list = []\n",
        "\n",
        "    # Let's loop through the provided dataset in the QC folders\n",
        "        for i in os.listdir(Source_QC_folder):\n",
        "            if not os.path.isdir(os.path.join(Source_QC_folder,i)):\n",
        "                print('Running QC on: '+i)\n",
        "\n",
        "                shortname_no_PNG = i[:-4]\n",
        "\n",
        "                # -------------------------------- Target test data (Ground truth) --------------------------------\n",
        "                test_GT = imageio.imread(os.path.join(QC_folder, f\"{shortname_no_PNG}_real_B.png\"))\n",
        "                # -------------------------------- Source test data --------------------------------\n",
        "                test_source = imageio.imread(os.path.join(QC_folder, f\"{shortname_no_PNG}_real_A.png\"))\n",
        "                # -------------------------------- Prediction --------------------------------\n",
        "                test_prediction = imageio.imread(os.path.join(QC_folder,f\"{shortname_no_PNG}_fake_B.png\"))\n",
        "                #--------------------------- Here we normalise using histograms matching--------------------------------\n",
        "                test_prediction_matched = match_histograms(test_prediction, test_GT, multichannel=True)\n",
        "                test_source_matched = match_histograms(test_source, test_GT, multichannel=True)\n",
        "                # -------------------------------- Calculate the metric maps and save them --------------------------------\n",
        "                # Calculate the SSIM maps\n",
        "                index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT, test_prediction_matched, multichannel=True)\n",
        "                index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT, test_source_matched, multichannel=True)\n",
        "                ssim_score_list.append(index_SSIM_GTvsPrediction)\n",
        "\n",
        "                #Save ssim_maps\n",
        "                img_SSIM_GTvsPrediction_8bit = (img_SSIM_GTvsPrediction* 255).astype(\"uint8\")\n",
        "                io.imsave(os.path.join(QC_folder, f\"SSIM_GTvsPrediction_{shortname_no_PNG}.tif\"), img_SSIM_GTvsPrediction_8bit)\n",
        "\n",
        "                img_SSIM_GTvsSource_8bit = (img_SSIM_GTvsSource* 255).astype(\"uint8\")\n",
        "                io.imsave(os.path.join(QC_folder, f\"SSIM_GTvsSource_{shortname_no_PNG}.tif\"),img_SSIM_GTvsSource_8bit)\n",
        "\n",
        "                # -------------------------------- Pearson correlation coefficient --------------------------------\n",
        "\n",
        "          # -------------------------------- Calculate the perceptual difference metrics map and save them --------------------------------\n",
        "            if Do_lpips_analysis:\n",
        "\n",
        "                lpips_GTvsPrediction = perceptual_diff(test_GT, test_prediction, 'alex', True)\n",
        "                lpips_GTvsPrediction_image = lpips_GTvsPrediction[0,0,...].data.cpu().numpy()\n",
        "                lpips_GTvsPrediction_score= lpips_GTvsPrediction.mean().data.numpy()\n",
        "                lpips_score_list.append(lpips_GTvsPrediction_score)\n",
        "\n",
        "\n",
        "                lpips_GTvsSource = perceptual_diff(test_GT, test_source, 'alex', True)\n",
        "                lpips_GTvsSource_image = lpips_GTvsSource[0,0,...].data.cpu().numpy()\n",
        "                lpips_GTvsSource_score= lpips_GTvsSource.mean().data.numpy()\n",
        "\n",
        "\n",
        "                #lpips_GTvsPrediction_image_8bit = (lpips_GTvsPrediction_image* 255).astype(\"uint8\")\n",
        "                io.imsave(os.path.join(QC_folder, f\"lpips_GTvsPrediction_{shortname_no_PNG}.tif\"),lpips_GTvsPrediction_image)\n",
        "\n",
        "                #lpips_GTvsSource_image_8bit = (lpips_GTvsSource_image* 255).astype(\"uint8\")\n",
        "                io.imsave(os.path.join(QC_folder, f\"lpips_GTvsInput_{shortname_no_PNG}.tif\"),lpips_GTvsSource_image)\n",
        "            else:\n",
        "                lpips_GTvsPrediction_score = 0\n",
        "                lpips_score_list.append(lpips_GTvsPrediction_score)\n",
        "\n",
        "                lpips_GTvsSource_score = 0\n",
        "\n",
        "            writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource), str(lpips_GTvsPrediction_score),str(lpips_GTvsSource_score)])\n",
        "\n",
        "    #Here we calculate the ssim average for each image in each checkpoints\n",
        "    Average_SSIM_checkpoint = Average(ssim_score_list)\n",
        "    Average_lpips_checkpoint = Average(lpips_score_list)\n",
        "\n",
        "    return Average_SSIM_checkpoint, Average_lpips_checkpoint\n",
        "\n",
        "def QC_singlechannel(Source_QC_folder, QC_folder):\n",
        "    csv_file = os.path.join(QC_folder, f\"QC_metrics_{QC_model_name}{str(checkpoints)}.csv\")\n",
        "    # Open and create the csv file that will contain all the QC metrics\n",
        "    with open(csv_file, \"w\", newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the header in the csv file\n",
        "        writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\", \"Prediction v. GT NRMSE\", \"Input v. GT NRMSE\", \"Prediction v. GT PSNR\", \"Input v. GT PSNR\", \"Prediction v. GT lpips\", \"Input v. GT lpips\"])\n",
        "\n",
        "        # Initialize the lists\n",
        "        ssim_score_list = []\n",
        "        Pearson_correlation_coefficient_list = []\n",
        "        lpips_score_list = []\n",
        "\n",
        "        # Let's loop through the provided dataset in the QC folders\n",
        "        for i in os.listdir(Source_QC_folder):\n",
        "\n",
        "            if not os.path.isdir(os.path.join(Source_QC_folder,i)):\n",
        "                print('Running QC on: '+i)\n",
        "\n",
        "                shortname_no_PNG = i[:-4]\n",
        "\n",
        "                # -------------------------------- Target test data (Ground truth) --------------------------------\n",
        "                test_GT_raw = imageio.imread(os.path.join(QC_folder, f\"{shortname_no_PNG}_real_B.png\"))\n",
        "                test_GT = test_GT_raw[:,:,2]\n",
        "                # -------------------------------- Source test data --------------------------------\n",
        "                test_source_raw = imageio.imread(os.path.join(QC_folder, f\"{shortname_no_PNG}_real_A.png\"))\n",
        "                test_source = test_source_raw[:,:,2]\n",
        "                # Normalize the images wrt each other by minimizing the MSE between GT and Source image\n",
        "                test_GT_norm,test_source_norm = norm_minmse(test_GT, test_source)\n",
        "                # -------------------------------- Prediction --------------------------------\n",
        "                test_prediction_raw = imageio.imread(os.path.join(QC_folder,f\"{shortname_no_PNG}_fake_B.png\"))\n",
        "                test_prediction = test_prediction_raw[:,:,2]\n",
        "                # Normalize the images wrt each other by minimizing the MSE between GT and prediction\n",
        "                test_GT_norm,test_prediction_norm = norm_minmse(test_GT, test_prediction)\n",
        "\n",
        "\n",
        "                # -------------------------------- Calculate the metric maps and save them --------------------------------\n",
        "                # Calculate the SSIM maps\n",
        "                index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT_norm, test_prediction_norm, multichannel=False)\n",
        "                index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT_norm, test_source_norm, multichannel=False)\n",
        "                ssim_score_list.append(index_SSIM_GTvsPrediction)\n",
        "\n",
        "                #Save ssim_maps\n",
        "\n",
        "                img_SSIM_GTvsPrediction_8bit = (img_SSIM_GTvsPrediction* 255).astype(\"uint8\")\n",
        "                io.imsave(os.path.join(QC_folder, f\"SSIM_GTvsPrediction_{shortname_no_PNG}.tif\"),img_SSIM_GTvsPrediction_8bit)\n",
        "                img_SSIM_GTvsSource_8bit = (img_SSIM_GTvsSource* 255).astype(\"uint8\")\n",
        "                io.imsave(os.path.join(QC_folder, f\"SSIM_GTvsSource_{shortname_no_PNG}.tif\"),img_SSIM_GTvsSource_8bit)\n",
        "\n",
        "                # Calculate the Root Squared Error (RSE) maps\n",
        "                img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n",
        "                img_RSE_GTvsSource = np.sqrt(np.square(test_GT_norm - test_source_norm))\n",
        "\n",
        "                # Save SE maps\n",
        "                img_RSE_GTvsPrediction_8bit = (img_RSE_GTvsPrediction* 255).astype(\"uint8\")\n",
        "                io.imsave(os.path.join(QC_folder, f\"RSE_GTvsPrediction_{shortname_no_PNG}.tif\"),img_RSE_GTvsPrediction_8bit)\n",
        "                img_RSE_GTvsSource_8bit = (img_RSE_GTvsSource* 255).astype(\"uint8\")\n",
        "                io.imsave(os.path.join(QC_folder, f\"RSE_GTvsSource_{shortname_no_PNG}.tif\"),img_RSE_GTvsSource_8bit)\n",
        "\n",
        "                # -------------------------------- Calculate the RSE metrics and save them --------------------------------\n",
        "                # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n",
        "                NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n",
        "                NRMSE_GTvsSource = np.sqrt(np.mean(img_RSE_GTvsSource))\n",
        "\n",
        "                # We can also measure the peak signal to noise ratio between the images\n",
        "                PSNR_GTvsPrediction = psnr(test_GT_norm,test_prediction_norm,data_range=1.0)\n",
        "                PSNR_GTvsSource = psnr(test_GT_norm,test_source_norm,data_range=1.0)\n",
        "\n",
        "              # ---------------- Calculate the perceptual difference metrics map and save them -------------\n",
        "                if Do_lpips_analysis:\n",
        "                    lpips_GTvsPrediction = perceptual_diff(test_GT_raw, test_prediction_raw, 'alex', True)\n",
        "                    lpips_GTvsPrediction_image = lpips_GTvsPrediction[0,0,...].data.cpu().numpy()\n",
        "                    lpips_GTvsPrediction_score= lpips_GTvsPrediction.mean().data.numpy()\n",
        "                    lpips_score_list.append(lpips_GTvsPrediction_score)\n",
        "\n",
        "                    lpips_GTvsSource = perceptual_diff(test_GT_raw, test_source_raw, 'alex', True)\n",
        "                    lpips_GTvsSource_image = lpips_GTvsSource[0,0,...].data.cpu().numpy()\n",
        "                    lpips_GTvsSource_score= lpips_GTvsSource.mean().data.numpy()\n",
        "\n",
        "\n",
        "                    lpips_GTvsPrediction_image_8bit = (lpips_GTvsPrediction_image* 255).astype(\"uint8\")\n",
        "                    io.imsave(os.path.join(QC_folder, f\"lpips_GTvsPrediction_{shortname_no_PNG}.tif\"),lpips_GTvsPrediction_image_8bit)\n",
        "\n",
        "                    lpips_GTvsSource_image_8bit = (lpips_GTvsSource_image* 255).astype(\"uint8\")\n",
        "                    io.imsave(os.path.join(QC_folder, f\"lpips_GTvsInput_{shortname_no_PNG}.tif\"),lpips_GTvsSource_image_8bit)\n",
        "                else:\n",
        "                    lpips_GTvsPrediction_score = 0\n",
        "                    lpips_score_list.append(lpips_GTvsPrediction_score)\n",
        "\n",
        "                    lpips_GTvsSource_score = 0\n",
        "\n",
        "            writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource),str(NRMSE_GTvsPrediction),str(NRMSE_GTvsSource),str(PSNR_GTvsPrediction),str(PSNR_GTvsSource),str(lpips_GTvsPrediction_score),str(lpips_GTvsSource_score)])\n",
        "\n",
        "        #Here we calculate the ssim average for each image in each checkpoints\n",
        "    Average_SSIM_checkpoint = Average(ssim_score_list)\n",
        "    Average_lpips_checkpoint = Average(lpips_score_list)\n",
        "\n",
        "    return Average_SSIM_checkpoint, Average_lpips_checkpoint\n",
        "\n",
        "def perceptual_diff(im0, im1, network, spatial):\n",
        "\n",
        "  tensor0 = lpips.im2tensor(im0)\n",
        "  tensor1 = lpips.im2tensor(im1)\n",
        "  # Set up the loss function we will use\n",
        "  loss_fn = lpips.LPIPS(net=network, spatial=spatial, verbose=False)\n",
        "\n",
        "  diff = loss_fn.forward(tensor0, tensor1)\n",
        "\n",
        "  return diff\n",
        "\n",
        "def export_cyclegan_torchscript_model(path_model_checkpoint):\n",
        "    ## A new text file needs to be created to read the model using the cycleGAN code and exporting it.\n",
        "\n",
        "    # Base script:\n",
        "    export_code = \"\"\"import os\n",
        "from options.test_options import TestOptions\n",
        "from models import create_model\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "except ImportError:\n",
        "    print('Warning: wandb package cannot be found. The option \"--use_wandb\" will result in error.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    opt = TestOptions().parse()  # get test options\n",
        "    # hard-code some parameters for test\n",
        "    opt.num_threads = 0   # test code only supports num_threads = 0\n",
        "    opt.batch_size = 1    # test code only supports batch_size = 1\n",
        "    opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
        "    opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n",
        "    opt.display_id = -1   # no visdom display; the test code saves the results to a HTML file.\n",
        "    model = create_model(opt)      # create a model given opt.model and other options\n",
        "    model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
        "\n",
        "    # test with eval mode. This only affects layers like batchnorm and dropout.\n",
        "    # For [pix2pix]: we use batchnorm and dropout in the original pix2pix. You can experiment it with and without eval() mode.\n",
        "    # For [CycleGAN]: It should not affect CycleGAN as CycleGAN uses instancenorm without dropout.\n",
        "    model.eval()\n",
        "    import torch\n",
        "    net = getattr(model, 'netG')\n",
        "    model = torch.jit.script(net.module.cpu())\n",
        "    # save the model weights\\n\"\"\"\n",
        "    saving_line = \"\"\"    model.save(\"\"\" + f\"\\'{path_model_checkpoint}\\')\"\n",
        "    # Write the new python exporting script\n",
        "\n",
        "\n",
        "    with open(os.path.join(pix2pix_code_dir, \"pytorch-CycleGAN-and-pix2pix\", 'cyclegan_model_export.py'), 'w') as f:\n",
        "        f.write(export_code)\n",
        "        f.write(saving_line)\n",
        "\n",
        "\n",
        "def pdf_export(trained = False, augmentation = False, pretrained_model = False, Saving_path=\"/content/\"):\n",
        "  class MyFPDF(FPDF, HTMLMixin):\n",
        "    pass\n",
        "\n",
        "  pdf = MyFPDF()\n",
        "  pdf.add_page()\n",
        "  pdf.set_right_margin(-1)\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "  Network = 'pix2pix'\n",
        "  day = datetime.now()\n",
        "  datetime_str = str(day)[0:10]\n",
        "\n",
        "  Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
        "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "  pdf.ln(1)\n",
        "\n",
        "  # add another cell\n",
        "  if trained:\n",
        "    training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n",
        "    pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n",
        "  pdf.ln(1)\n",
        "\n",
        "  Header_2 = 'Information for your materials and method:'\n",
        "  pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n",
        "\n",
        "  all_packages = ''\n",
        "  for requirement in freeze(local_only=True):\n",
        "    all_packages = all_packages+requirement+', '\n",
        "  #print(all_packages)\n",
        "\n",
        "  #Main Packages\n",
        "  main_packages = ''\n",
        "  version_numbers = []\n",
        "  for name in ['tensorflow','numpy','torch']:\n",
        "    find_name=all_packages.find(name)\n",
        "    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
        "    #Version numbers only here:\n",
        "    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
        "\n",
        "  try:\n",
        "    cuda_version = subprocess.run([\"nvcc\",\"--version\"],stdout=subprocess.PIPE)\n",
        "    cuda_version = cuda_version.stdout.decode('utf-8')\n",
        "    cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
        "  except:\n",
        "    cuda_version = ' - No cuda found - '\n",
        "  try:\n",
        "    gpu_name = subprocess.run([\"nvidia-smi\"],stdout=subprocess.PIPE)\n",
        "    gpu_name = gpu_name.stdout.decode('utf-8')\n",
        "    gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
        "  except:\n",
        "    gpu_name = ' - No GPU found - '\n",
        "  #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
        "  #print(gpu_name)\n",
        "\n",
        "  shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n",
        "  dataset_size = len(os.listdir(Training_source))\n",
        "\n",
        "  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(dataset_size)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+' and a vanilla GAN loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2021). Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), torch (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "  if Use_pretrained_model:\n",
        "    text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(dataset_size)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+' and a vanilla GAN loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2021). The model was retrained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), torch (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  pdf.multi_cell(190, 5, txt = text, align='L')\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(28, 5, txt='Augmentation: ', ln=0)\n",
        "  pdf.set_font('')\n",
        "  if augmentation:\n",
        "    aug_text = 'The dataset was augmented by a factor of '+str(Multiply_dataset_by)+' by'\n",
        "    if rotate_270_degrees != 0 or rotate_90_degrees != 0:\n",
        "      aug_text = aug_text+'\\n- rotation'\n",
        "    if flip_left_right != 0 or flip_top_bottom != 0:\n",
        "      aug_text = aug_text+'\\n- flipping'\n",
        "    if random_zoom_magnification != 0:\n",
        "      aug_text = aug_text+'\\n- random zoom magnification'\n",
        "    if random_distortion != 0:\n",
        "      aug_text = aug_text+'\\n- random distortion'\n",
        "    if image_shear != 0:\n",
        "      aug_text = aug_text+'\\n- image shearing'\n",
        "    if skew_image != 0:\n",
        "      aug_text = aug_text+'\\n- image skewing'\n",
        "  else:\n",
        "    aug_text = 'No augmentation was used for training.'\n",
        "  pdf.multi_cell(190, 5, txt=aug_text, align='L')\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  if Use_Default_Advanced_Parameters:\n",
        "    pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
        "  pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
        "  pdf.ln(1)\n",
        "  html = \"\"\"\n",
        "  <table width=40% style=\"margin-left:0px;\">\n",
        "    <tr>\n",
        "      <th width = 50% align=\"left\">Parameter</th>\n",
        "      <th width = 50% align=\"left\">Value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>number_of_epochs</td>\n",
        "      <td width = 50%>{0}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>patch_size</td>\n",
        "      <td width = 50%>{1}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>batch_size</td>\n",
        "      <td width = 50%>{2}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>initial_learning_rate</td>\n",
        "      <td width = 50%>{3}</td>\n",
        "    </tr>\n",
        "  </table>\n",
        "  \"\"\".format(number_of_epochs,str(patch_size)+'x'+str(patch_size),batch_size,initial_learning_rate)\n",
        "  pdf.write_html(html)\n",
        "\n",
        "  #pdf.multi_cell(190, 5, txt = text_2, align='L')\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(30, 5, txt= 'Training_source:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(29, 5, txt= 'Training_target:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n",
        "  pdf.ln(1)\n",
        "  #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(22, 5, txt= 'Model Path:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
        "  pdf.ln(1)\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread(os.path.join(Saving_path, 'TrainingDataExample_pix2pix.png')).shape\n",
        "  pdf.image(os.path.join(Saving_path, 'TrainingDataExample_pix2pix.png'), x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "  pdf.ln(1)\n",
        "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "  pdf.ln(1)\n",
        "  ref_2 = '- pix2pix: Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.'\n",
        "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "  pdf.ln(1)\n",
        "  if augmentation:\n",
        "    ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
        "    pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
        "  pdf.ln(3)\n",
        "  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
        "  pdf.set_font('Arial', size = 11, style='B')\n",
        "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "  pdf.ln(1)\n",
        "\n",
        "  pdf.output(model_path+'/'+model_name+'/'+model_name+\"_training_report.pdf\")\n",
        "\n",
        "def qc_pdf_export():\n",
        "  class MyFPDF(FPDF, HTMLMixin):\n",
        "    pass\n",
        "\n",
        "  pdf = MyFPDF()\n",
        "  pdf.add_page()\n",
        "  pdf.set_right_margin(-1)\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "  Network = 'pix2pix'\n",
        "\n",
        "\n",
        "  day = datetime.now()\n",
        "  datetime_str = str(day)[0:10]\n",
        "\n",
        "  Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n",
        "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "  pdf.ln(1)\n",
        "\n",
        "  all_packages = ''\n",
        "  for requirement in freeze(local_only=True):\n",
        "    all_packages = all_packages+requirement+', '\n",
        "\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(2)\n",
        "  pdf.cell(190, 5, txt = 'Development of Training Losses', ln=1, align='L')\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread(full_QC_model_path+'/Quality Control/SSIMvsCheckpoint_data.png').shape\n",
        "  pdf.image(full_QC_model_path+'/Quality Control/SSIMvsCheckpoint_data.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "  pdf.ln(2)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.ln(3)\n",
        "  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread(full_QC_model_path+'/Quality Control/QC_example_data.png').shape\n",
        "  if number_channels == '3':\n",
        "    pdf.image(full_QC_model_path+'/Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/5), h = round(exp_size[0]/5))\n",
        "  if number_channels == '1':\n",
        "    pdf.image(full_QC_model_path+'/Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "\n",
        "  pdf.ln(1)\n",
        "  for checkpoint in os.listdir(full_QC_model_path+'/Quality Control'):\n",
        "    if os.path.isdir(os.path.join(full_QC_model_path,'Quality Control',checkpoint)) and checkpoint != 'Prediction':\n",
        "      pdf.set_font('')\n",
        "      pdf.set_font('Arial', size = 10, style = 'B')\n",
        "      pdf.cell(70, 5, txt = 'Metrics for checkpoint: '+ str(checkpoint), align='L', ln=1)\n",
        "      html = \"\"\"\n",
        "      <body>\n",
        "      <font size=\"8\" face=\"Courier\" >\n",
        "      <table width=95% style=\"margin-left:0px;\">\"\"\"\n",
        "      with open(full_QC_model_path+'/Quality Control/'+str(checkpoint)+'/QC_metrics_'+QC_model_name+str(checkpoint)+'.csv', 'r') as csvfile:\n",
        "        metrics = csv.reader(csvfile)\n",
        "        header = next(metrics)\n",
        "        image = header[0]\n",
        "        mSSIM_PvsGT = header[1]\n",
        "        mSSIM_SvsGT = header[2]\n",
        "        header = \"\"\"\n",
        "        <tr>\n",
        "        <th width = 60% align=\"left\">{0}</th>\n",
        "        <th width = 20% align=\"center\">{1}</th>\n",
        "        <th width = 20% align=\"center\">{2}</th>\n",
        "        </tr>\"\"\".format(image,mSSIM_PvsGT,mSSIM_SvsGT)\n",
        "        html = html+header\n",
        "        for row in metrics:\n",
        "          image = row[0]\n",
        "          mSSIM_PvsGT = row[1]\n",
        "          mSSIM_SvsGT = row[2]\n",
        "          cells = \"\"\"\n",
        "            <tr>\n",
        "              <td width = 60% align=\"left\">{0}</td>\n",
        "              <td width = 20% align=\"center\">{1}</td>\n",
        "              <td width = 20% align=\"center\">{2}</td>\n",
        "            </tr>\"\"\".format(image,str(round(float(mSSIM_PvsGT),3)),str(round(float(mSSIM_SvsGT),3)))\n",
        "          html = html+cells\n",
        "        html = html+\"\"\"</body></table>\"\"\"\n",
        "      pdf.write_html(html)\n",
        "      pdf.ln(2)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "  pdf.ln(1)\n",
        "  ref_2 = '- pix2pix: Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.'\n",
        "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "  pdf.ln(1)\n",
        "\n",
        "  pdf.ln(3)\n",
        "  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
        "\n",
        "  pdf.set_font('Arial', size = 11, style='B')\n",
        "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "  pdf.ln(1)\n",
        "\n",
        "  pdf.output(full_QC_model_path+'/Quality Control/'+QC_model_name+'_QC_report.pdf')\n",
        "\n",
        "# Build requirements file for local run\n",
        "after = [str(m) for m in sys.modules]\n",
        "build_requirements_file(before, after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yWFoJNnoin"
      },
      "source": [
        "# **2. Initialise the Colab session**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMNHVZfHmbKb"
      },
      "source": [
        "\n",
        "## **2.1. Check for GPU access**\n",
        "---\n",
        "\n",
        "By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n",
        "\n",
        "<font size = 4>Go to **Runtime -> Change the Runtime type**\n",
        "\n",
        "<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n",
        "\n",
        "<font size = 4>**Accelerator: GPU** *(Graphics processing unit)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zCvebubeSaGY"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Run this cell to check if you have GPU access\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name()=='':\n",
        "  print('You do not have GPU access.')\n",
        "  print('Did you change your runtime ?')\n",
        "  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n",
        "  print('Expect slow performance. To access GPU try reconnecting later')\n",
        "\n",
        "else:\n",
        "  print('You have GPU access')\n",
        "  !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNIVx8_CLolt"
      },
      "source": [
        "## **2.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R3JNPbRA6GAS"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "\n",
        "# mount user's Google Drive to Google Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLYcZR9gMv42"
      },
      "source": [
        "# **3. Select your parameters and paths**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ_QxtSWQ7CL"
      },
      "source": [
        "## **3.1. Setting main training parameters**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuESFimvMv43"
      },
      "source": [
        "<font size = 5> **Paths for training, predictions and results**\n",
        "\n",
        "<font size = 4>**`Training_source:`, `Training_target`:** These are the paths to your folders containing the Training_source and Training_target training data respecively. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
        "\n",
        "<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n",
        "\n",
        "<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
        "\n",
        "<font size = 5> **Normalisation**\n",
        "\n",
        "<font size = 4> Both source and target data can be normalised. Normalisation options are:\n",
        "\n",
        "<font size = 4>**`contrast streching`:** The intensity values are streched so they fill the entire dynamic range of a 8 bit depth image (uint8).\n",
        "\n",
        "<font size = 4>**`adaptive equalization`:** Local contrast enhancement by using histograms computed over different tile regions of the image. Local details can therefore be enhanced even in regions that are darker or lighter than most of the image. The normalised image is saved as uint8.\n",
        "\n",
        "<font size = 5>**Training parameters**\n",
        "\n",
        "<font size = 4>**`number_of_epochs`:**Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10) epochs, but a full training should run for 200 epochs or more. Evaluate the performance after training (see 5). **Default value: 200**\n",
        "\n",
        "<font size = 4>**`number_channels`**: Defines how the channels of the images are treated. 1 means that a grayscale image is predicted and 3 means that the channels of an RGB image are regarded and predictions are made for all three channels. If you have an image in RGB format but are interested in a consistent value across channels (=grayscale image), set it to 1 and the image is converted to grayscale automatically.\n",
        "\n",
        "<font size = 5>**Advanced Parameters - experienced users only**\n",
        "\n",
        "<font size = 4>**`patch_size`:** pix2pix divides the image into patches for training. Input the size of the patches (length of a side). The value should be smaller than the dimensions of the image and divisible by 8. **Default value: 512**\n",
        "\n",
        "<font size = 4>**When choosing the patch_size, the value should be i) large enough that it will enclose many instances, ii) small enough that the resulting patches fit into the RAM.**<font size = 4>\n",
        "\n",
        "<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 1**\n",
        "\n",
        "<font size = 4>**`initial_learning_rate`:** Input the initial value to be used as learning rate. **Default value: 0.0002**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EkGFqBHW1w3F"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "#@markdown ###Path to training images:\n",
        "\n",
        "Training_source = \"\" #@param {type:\"string\"}\n",
        "#InputFile = Training_source+\"/*.png\"\n",
        "\n",
        "Training_target = \"\" #@param {type:\"string\"}\n",
        "#OutputFile = Training_target+\"/*.png\"\n",
        "\n",
        "#@markdown ###Type of images:\n",
        "\n",
        "number_channels = \"1\" #@param [\"1\", \"3\"]\n",
        "\n",
        "#@markdown ###Image normalisation:\n",
        "\n",
        "Normalisation_training_source = \"Contrast stretching\" #@param [\"None\", \"Contrast stretching\", \"Adaptive Equalization\"]\n",
        "Normalisation_training_target = \"Contrast stretching\" #@param [\"None\", \"Contrast stretching\", \"Adaptive Equalization\"]\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the model and path to model folder:\n",
        "model_name = \"\" #@param {type:\"string\"}\n",
        "model_path = \"\" #@param {type:\"string\"}\n",
        "pix2pix_working_directory = os.getcwd()\n",
        "# other parameters for training.\n",
        "#@markdown ###Training Parameters\n",
        "#@markdown Number of epochs:\n",
        "number_of_epochs =  10#@param {type:\"number\"}\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "\n",
        "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please input:\n",
        "patch_size =  512#@param {type:\"number\"} # in pixels\n",
        "batch_size =  1#@param {type:\"number\"}\n",
        "initial_learning_rate = 0.0002 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "if (Use_Default_Advanced_Parameters):\n",
        "    print(\"Default advanced parameters enabled\")\n",
        "    batch_size = 1\n",
        "    patch_size =  512\n",
        "    initial_learning_rate = 0.0002\n",
        "\n",
        "#To use pix2pix we need to organise the data in a way the network can understand\n",
        "\n",
        "Saving_path= os.path.join(pix2pix_working_directory, model_name)\n",
        "\n",
        "if os.path.exists(Saving_path):\n",
        "    shutil.rmtree(Saving_path)\n",
        "os.makedirs(Saving_path)\n",
        "\n",
        "imageA_folder = Saving_path+\"/A\"\n",
        "os.makedirs(imageA_folder)\n",
        "\n",
        "imageB_folder = Saving_path+\"/B\"\n",
        "os.makedirs(imageB_folder)\n",
        "\n",
        "imageAB_folder = Saving_path+\"/AB\"\n",
        "os.makedirs(imageAB_folder)\n",
        "\n",
        "TrainA_Folder = Saving_path+\"/A/train\"\n",
        "os.makedirs(TrainA_Folder)\n",
        "\n",
        "TrainB_Folder = Saving_path+\"/B/train\"\n",
        "os.makedirs(TrainB_Folder)\n",
        "\n",
        "# Here we disable pre-trained model by default (in case the  cell is not ran)\n",
        "Use_pretrained_model = False\n",
        "\n",
        "# Here we disable data augmentation by default (in case the cell is not ran)\n",
        "\n",
        "Use_Data_augmentation = False\n",
        "\n",
        "# Here we normalise the image is enabled\n",
        "\n",
        "if Normalisation_training_source == \"Contrast stretching\":\n",
        "    Training_source_norm = Saving_path+\"/Training_source_norm\"\n",
        "    os.makedirs(Training_source_norm)\n",
        "    Training_source_list = [f for f in os.listdir(Training_source) if not f.__contains__(\".ipynb_checkpoints\")]\n",
        "    for filename in Training_source_list:\n",
        "        img = io.imread(os.path.join(Training_source,filename)).astype(np.float32)\n",
        "        short_name = os.path.splitext(filename)\n",
        "        p2, p99 = np.percentile(img, (1, 99.9))\n",
        "        img = exposure.rescale_intensity(img, in_range=(p2, p99))\n",
        "        img = 255 * img # Now scale by 255\n",
        "        img = img.astype(np.uint8)\n",
        "        cv2.imwrite(Training_source_norm+\"/\"+short_name[0]+\".png\", img)\n",
        "    Training_source = Training_source_norm\n",
        "\n",
        "\n",
        "if Normalisation_training_target == \"Contrast stretching\":\n",
        "    Training_target_norm = Saving_path+\"/Training_target_norm\"\n",
        "    os.makedirs(Training_target_norm)\n",
        "    Training_target_list = [f for f in os.listdir(Training_target) if not f.__contains__(\".ipynb_checkpoints\")]\n",
        "    for filename in Training_target_list:\n",
        "        img = io.imread(os.path.join(Training_target,filename)).astype(np.float32)\n",
        "        short_name = os.path.splitext(filename)\n",
        "        p2, p99 = np.percentile(img, (1, 99.9))\n",
        "        img = exposure.rescale_intensity(img, in_range=(p2, p99))\n",
        "        img = 255 * img # Now scale by 255\n",
        "        img = img.astype(np.uint8)\n",
        "        cv2.imwrite(Training_target_norm+\"/\"+short_name[0]+\".png\", img)\n",
        "    Training_target = Training_target_norm\n",
        "\n",
        "\n",
        "if Normalisation_training_source == \"Adaptive Equalization\":\n",
        "    Training_source_norm = Saving_path+\"/Training_source_norm\"\n",
        "    os.makedirs(Training_source_norm)\n",
        "    Training_source_list = [f for f in os.listdir(Training_source) if not f.__contains__(\".ipynb_checkpoints\")]\n",
        "    for filename in Training_source_list:\n",
        "        img = io.imread(os.path.join(Training_source,filename))\n",
        "        short_name = os.path.splitext(filename)\n",
        "        img = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
        "        img = 255 * img # Now scale by 255\n",
        "        img = img.astype(np.uint8)\n",
        "        cv2.imwrite(Training_source_norm+\"/\"+short_name[0]+\".png\", img)\n",
        "    Training_source = Training_source_norm\n",
        "\n",
        "\n",
        "if Normalisation_training_target == \"Adaptive Equalization\":\n",
        "    Training_target_norm = Saving_path+\"/Training_target_norm\"\n",
        "    os.makedirs(Training_target_norm)\n",
        "    Training_target_list = [f for f in os.listdir(Training_target) if not f.__contains__(\".ipynb_checkpoints\")]\n",
        "    for filename in Training_target_list:\n",
        "        img = io.imread(os.path.join(Training_target,filename))\n",
        "        short_name = os.path.splitext(filename)\n",
        "        img = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
        "        img = 255 * img # Now scale by 255\n",
        "        img = img.astype(np.uint8)\n",
        "        cv2.imwrite(Training_target_norm+\"/\"+short_name[0]+\".png\", img)\n",
        "    Training_target = Training_target_norm\n",
        "\n",
        "\n",
        "if Normalisation_training_source == \"None\":\n",
        "    Training_source_norm = Saving_path+\"/Training_source_norm\"\n",
        "    os.makedirs(Training_source_norm)\n",
        "    Training_source_list = [f for f in os.listdir(Training_source) if not f.__contains__(\".ipynb_checkpoints\")]\n",
        "    for filename in Training_source_list:\n",
        "        img = io.imread(os.path.join(Training_source,filename))\n",
        "        short_name = os.path.splitext(filename)\n",
        "        cv2.imwrite(Training_source_norm+\"/\"+short_name[0]+\".png\", img)\n",
        "    Training_source = Training_source_norm\n",
        "\n",
        "\n",
        "if Normalisation_training_target == \"None\":\n",
        "    Training_target_norm = Saving_path+\"/Training_target_norm\"\n",
        "    os.makedirs(Training_target_norm)\n",
        "    Training_target_list = [f for f in os.listdir(Training_target) if not f.__contains__(\".ipynb_checkpoints\")]\n",
        "    for filename in Training_target_list:\n",
        "        img = io.imread(os.path.join(Training_target,filename))\n",
        "        short_name = os.path.splitext(filename)\n",
        "        cv2.imwrite(Training_target_norm+\"/\"+short_name[0]+\".png\", img)\n",
        "    Training_target = Training_target_norm\n",
        "\n",
        "\n",
        "# This will display a randomly chosen dataset input and output\n",
        "random_choice = random.choice(os.listdir(Training_source))\n",
        "x = io.imread(Training_source+\"/\"+random_choice)\n",
        "\n",
        "#Find image XY dimension\n",
        "Image_Y = x.shape[0]\n",
        "Image_X = x.shape[1]\n",
        "\n",
        "Image_min_dim = min(Image_Y, Image_X)\n",
        "\n",
        "#Hyperparameters failsafes\n",
        "if patch_size > min(Image_Y, Image_X):\n",
        "    patch_size = min(Image_Y, Image_X)\n",
        "    print (bcolors.WARNING + \" Your chosen patch_size is bigger than the xy dimension of your image; therefore the patch_size chosen is now:\",patch_size)\n",
        "\n",
        "# Here we check that patch_size is divisible by 4\n",
        "if not patch_size % 4 == 0:\n",
        "    patch_size = ((int(patch_size / 4)-1) * 4)\n",
        "    print (bcolors.WARNING + \" Your chosen patch_size is not divisible by 4; therefore the patch_size chosen is now:\",patch_size)\n",
        "\n",
        "# Here we check that patch_size is at least bigger than 256\n",
        "if patch_size < 256:\n",
        "    patch_size = 256\n",
        "    print (bcolors.WARNING + \" Your chosen patch_size is too small; therefore the patch_size chosen is now:\",patch_size)\n",
        "\n",
        "y = io.imread(Training_target+\"/\"+random_choice)\n",
        "\n",
        "n_channel_x = 1 if x.ndim == 2 else x.shape[-1]\n",
        "n_channel_y = 1 if y.ndim == 2 else y.shape[-1]\n",
        "\n",
        "if n_channel_x == 1:\n",
        "    cmap_x = 'gray'\n",
        "else:\n",
        "    cmap_x = None\n",
        "\n",
        "if n_channel_y == 1:\n",
        "    cmap_y = 'gray'\n",
        "else:\n",
        "    cmap_y = None\n",
        "\n",
        "f=plt.figure(figsize=(16,8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(x, cmap=cmap_x, interpolation='nearest')\n",
        "plt.title('Training source')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y, cmap=cmap_y, interpolation='nearest')\n",
        "plt.title('Training target')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.savefig(os.path.join(Saving_path, 'TrainingDataExample_pix2pix.png'),bbox_inches='tight',pad_inches=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqP3HLsM1w3G"
      },
      "source": [
        "## **3.2. Data augmentation**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqR8q_AK1w3G"
      },
      "source": [
        "<font size = 4>Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation is not necessary for training and if your training dataset is large you should disable it.\n",
        "\n",
        "<font size = 4>Data augmentation is performed here by [Augmentor.](https://github.com/mdbloice/Augmentor)\n",
        "\n",
        "<font size = 4>[Augmentor](https://github.com/mdbloice/Augmentor) was described in the following article:\n",
        "\n",
        "<font size = 4>Marcus D Bloice, Peter M Roth, Andreas Holzinger, Biomedical image augmentation using Augmentor, Bioinformatics, https://doi.org/10.1093/bioinformatics/btz259\n",
        "\n",
        "<font size = 4>**Please also cite this original paper when publishing results obtained using this notebook with augmentation enabled.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "u3Yh7_4l1w3G"
      },
      "outputs": [],
      "source": [
        "#Data augmentation\n",
        "\n",
        "Use_Data_augmentation = True #@param {type:\"boolean\"}\n",
        "\n",
        "if Use_Data_augmentation:\n",
        "    !pip install Augmentor\n",
        "    import Augmentor\n",
        "\n",
        "\n",
        "#@markdown ####Choose a factor by which you want to multiply your original dataset\n",
        "\n",
        "Multiply_dataset_by = 3 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "Save_augmented_images = False #@param {type:\"boolean\"}\n",
        "\n",
        "Saving_path = \"\" #@param {type:\"string\"}\n",
        "Saving_path = os.path.join(Saving_path, model_name)\n",
        "\n",
        "Use_Default_Augmentation_Parameters = True #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please choose the probability of the following image manipulations to be used to augment your dataset (1 = always used; 0 = disabled ):\n",
        "\n",
        "#@markdown ####Mirror and rotate images\n",
        "rotate_90_degrees = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "rotate_270_degrees = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "flip_left_right = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "flip_top_bottom = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "#@markdown ####Random image Zoom\n",
        "\n",
        "random_zoom = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "random_zoom_magnification = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "#@markdown ####Random image distortion\n",
        "\n",
        "random_distortion = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "\n",
        "#@markdown ####Image shearing and skewing\n",
        "\n",
        "image_shear = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "max_image_shear = 10 #@param {type:\"slider\", min:1, max:25, step:1}\n",
        "\n",
        "skew_image = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "skew_image_magnitude = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "\n",
        "if Use_Default_Augmentation_Parameters:\n",
        "    rotate_90_degrees = 0.5\n",
        "    rotate_270_degrees = 0.5\n",
        "    flip_left_right = 0.5\n",
        "    flip_top_bottom = 0.5\n",
        "\n",
        "    if not Multiply_dataset_by >5:\n",
        "        random_zoom = 0\n",
        "        random_zoom_magnification = 0.9\n",
        "        random_distortion = 0\n",
        "        image_shear = 0\n",
        "        max_image_shear = 10\n",
        "        skew_image = 0\n",
        "        skew_image_magnitude = 0\n",
        "\n",
        "    if Multiply_dataset_by >5:\n",
        "        random_zoom = 0.1\n",
        "        random_zoom_magnification = 0.9\n",
        "        random_distortion = 0.5\n",
        "        image_shear = 0.2\n",
        "        max_image_shear = 5\n",
        "        skew_image = 0.2\n",
        "        skew_image_magnitude = 0.4\n",
        "\n",
        "    if Multiply_dataset_by >25:\n",
        "        random_zoom = 0.5\n",
        "        random_zoom_magnification = 0.8\n",
        "        random_distortion = 0.5\n",
        "        image_shear = 0.5\n",
        "        max_image_shear = 20\n",
        "        skew_image = 0.5\n",
        "        skew_image_magnitude = 0.6\n",
        "\n",
        "\n",
        "list_files = os.listdir(Training_source)\n",
        "Nb_files = len(list_files)\n",
        "\n",
        "Nb_augmented_files = (Nb_files * Multiply_dataset_by)\n",
        "\n",
        "\n",
        "if Use_Data_augmentation:\n",
        "    print(\"Data augmentation enabled\")\n",
        "# Here we set the path for the various folder were the augmented images will be loaded\n",
        "\n",
        "# All images are first saved into the augmented folder\n",
        "  #Augmented_folder = \"/content/Augmented_Folder\"\n",
        "\n",
        "    if not Save_augmented_images:\n",
        "        Saving_path= \"/content\"\n",
        "\n",
        "    Augmented_folder =  Saving_path+\"/Augmented_Folder\"\n",
        "    if os.path.exists(Augmented_folder):\n",
        "        shutil.rmtree(Augmented_folder)\n",
        "    os.makedirs(Augmented_folder)\n",
        "\n",
        "    #Training_source_augmented = \"/content/Training_source_augmented\"\n",
        "    Training_source_augmented = Saving_path+\"/Training_source_augmented\"\n",
        "\n",
        "    if os.path.exists(Training_source_augmented):\n",
        "        shutil.rmtree(Training_source_augmented)\n",
        "    os.makedirs(Training_source_augmented)\n",
        "\n",
        "    #Training_target_augmented = \"/content/Training_target_augmented\"\n",
        "    Training_target_augmented = Saving_path+\"/Training_target_augmented\"\n",
        "\n",
        "    if os.path.exists(Training_target_augmented):\n",
        "        shutil.rmtree(Training_target_augmented)\n",
        "    os.makedirs(Training_target_augmented)\n",
        "\n",
        "\n",
        "# Here we generate the augmented images\n",
        "#Load the images\n",
        "    p = Augmentor.Pipeline(Training_source, Augmented_folder)\n",
        "\n",
        "#Define the matching images\n",
        "    p.ground_truth(Training_target)\n",
        "#Define the augmentation possibilities\n",
        "    if not rotate_90_degrees == 0:\n",
        "        p.rotate90(probability=rotate_90_degrees)\n",
        "\n",
        "    if not rotate_270_degrees == 0:\n",
        "        p.rotate270(probability=rotate_270_degrees)\n",
        "\n",
        "    if not flip_left_right == 0:\n",
        "        p.flip_left_right(probability=flip_left_right)\n",
        "\n",
        "    if not flip_top_bottom == 0:\n",
        "        p.flip_top_bottom(probability=flip_top_bottom)\n",
        "\n",
        "    if not random_zoom == 0:\n",
        "        p.zoom_random(probability=random_zoom, percentage_area=random_zoom_magnification)\n",
        "\n",
        "    if not random_distortion == 0:\n",
        "        p.random_distortion(probability=random_distortion, grid_width=4, grid_height=4, magnitude=8)\n",
        "\n",
        "    if not image_shear == 0:\n",
        "        p.shear(probability=image_shear,max_shear_left=20,max_shear_right=20)\n",
        "\n",
        "    if not skew_image == 0:\n",
        "        p.skew(probability=skew_image,magnitude=skew_image_magnitude)\n",
        "\n",
        "    p.sample(int(Nb_augmented_files))\n",
        "\n",
        "    print(int(Nb_augmented_files),\"matching images generated\")\n",
        "\n",
        "# Here we sort through the images and move them back to augmented trainning source and targets folders\n",
        "\n",
        "    augmented_files = os.listdir(Augmented_folder)\n",
        "\n",
        "    for f in augmented_files:\n",
        "\n",
        "        if (f.startswith(\"_groundtruth_(1)_\")):\n",
        "            shortname_noprefix = f[17:]\n",
        "            shutil.copyfile(Augmented_folder+\"/\"+f, Training_target_augmented+\"/\"+shortname_noprefix)\n",
        "        if not (f.startswith(\"_groundtruth_(1)_\")):\n",
        "            shutil.copyfile(Augmented_folder+\"/\"+f, Training_source_augmented+\"/\"+f)\n",
        "\n",
        "\n",
        "    for filename in os.listdir(Training_source_augmented):\n",
        "        # TODO: remove this chdir\n",
        "        os.chdir(Training_source_augmented)\n",
        "        os.rename(filename, filename.replace('_original', ''))\n",
        "\n",
        "    #Here we clean up the extra files\n",
        "    shutil.rmtree(Augmented_folder)\n",
        "\n",
        "if not Use_Data_augmentation:\n",
        "    print(bcolors.WARNING+\"Data augmentation disabled\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbrkdhUO1w3H"
      },
      "source": [
        "\n",
        "## **3.3. Using weights from a pre-trained model as initial weights**\n",
        "---\n",
        "<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a pix2pix model**.\n",
        "\n",
        "<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R3xBGf9L1w3I"
      },
      "outputs": [],
      "source": [
        "# @markdown ##Loading weights from a pre-trained network\n",
        "\n",
        "\n",
        "Use_pretrained_model = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown ###If yes, please provide the path to the model folder:\n",
        "pretrained_model_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# --------------------- Check if we load a previously trained model ------------------------\n",
        "if Use_pretrained_model:\n",
        "    h5_file_path = os.path.join(pretrained_model_path, \"latest_net_G.pth\")\n",
        "    # --------------------- Check the model exist ------------------------\n",
        "\n",
        "    if not os.path.exists(h5_file_path):\n",
        "        print(bcolors.WARNING+'WARNING: Pretrained model does not exist')\n",
        "        Use_pretrained_model = False\n",
        "        print(bcolors.WARNING+'No pretrained network will be used.')\n",
        "\n",
        "    if os.path.exists(h5_file_path):\n",
        "        print(\"Pretrained model \"+os.path.basename(pretrained_model_path)+\" was found and will be loaded prior to training.\")\n",
        "else:\n",
        "    print(bcolors.WARNING+'No pretrained network will be used.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTFo3ECz1w3I"
      },
      "source": [
        "# **4. Train the network**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3DrbrSK1w3I"
      },
      "source": [
        "## **4.1. Prepare the training data for training**\n",
        "---\n",
        "<font size = 4>Here, we use the information from Section 3 to prepare the training data into a suitable format for training. **Your data will be copied in the google Colab \"content\" folder which may take some time depending on the size of your dataset.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dj-IzrWz1w3I"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Prepare the data for training\n",
        "\n",
        "\n",
        "# --------------------- Here we load the augmented data or the raw data ------------------------\n",
        "\n",
        "if Use_Data_augmentation:\n",
        "    Training_source_dir = Training_source_augmented\n",
        "    Training_target_dir = Training_target_augmented\n",
        "\n",
        "if not Use_Data_augmentation:\n",
        "    Training_source_dir = Training_source\n",
        "    Training_target_dir = Training_target\n",
        "# --------------------- ------------------------------------------------\n",
        "\n",
        "print(\"Data preparation in progress\")\n",
        "\n",
        "if os.path.exists(model_path+'/'+model_name):\n",
        "    shutil.rmtree(model_path+'/'+model_name)\n",
        "os.makedirs(model_path+'/'+model_name)\n",
        "\n",
        "#--------------- Here we move the files to train A and train B ---------\n",
        "\n",
        "\n",
        "print('Copying training source data...')\n",
        "for f in tqdm(os.listdir(Training_source_dir)):\n",
        "    shutil.copyfile(Training_source_dir+\"/\"+f, TrainA_Folder+\"/\"+f)\n",
        "\n",
        "print('Copying training target data...')\n",
        "for f in tqdm(os.listdir(Training_target_dir)):\n",
        "    shutil.copyfile(Training_target_dir+\"/\"+f, TrainB_Folder+\"/\"+f)\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "#--------------- Here we combined A and B images---------\n",
        "# TODO: check if we can remove this chdir\n",
        "os.chdir(pix2pix_code_dir)\n",
        "!python3 pytorch-CycleGAN-and-pix2pix/datasets/combine_A_and_B.py --fold_A \"$imageA_folder\" --fold_B \"$imageB_folder\" --fold_AB \"$imageAB_folder\"\n",
        "\n",
        "\n",
        "\n",
        "# pix2pix uses EPOCH without lr decay and EPOCH with lr decay, here we automatically choose half and half\n",
        "\n",
        "number_of_epochs_lr_stable = int(number_of_epochs/2)\n",
        "number_of_epochs_lr_decay = int(number_of_epochs/2)\n",
        "\n",
        "if Use_pretrained_model :\n",
        "    for f in os.listdir(pretrained_model_path):\n",
        "        if (f.startswith(\"latest_net_\")):\n",
        "            shutil.copyfile(pretrained_model_path+\"/\"+f, model_path+'/'+model_name+\"/\"+f)\n",
        "\n",
        "#Export of pdf summary of training parameters\n",
        "pdf_export(augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model,\n",
        "           Saving_path=os.path.join(pix2pix_working_directory, model_name))\n",
        "\n",
        "print('------------------------')\n",
        "print(\"Data ready for training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pEg3UzF1w3J"
      },
      "source": [
        "## **4.2. Start Training**\n",
        "---\n",
        "<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n",
        "\n",
        "<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches or continue the training in a second Colab session. **Pix2pix will save model checkpoints every 5 epochs.**\n",
        "\n",
        "<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder as all data can be erased at the next training if using the same folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "92BQcn-w1w3J",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@markdown ##Start training\n",
        "\n",
        "# get number of channels\n",
        "if number_channels == \"1\":\n",
        "    nc = 1\n",
        "elif number_channels == \"3\":\n",
        "    nc = 3\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "#--------------------------------- Command line inputs to change pix2pix paramaters------------\n",
        "\n",
        "       # basic parameters\n",
        "        #('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n",
        "        #('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n",
        "        #('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
        "        #('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
        "\n",
        "       # model parameters\n",
        "        #('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n",
        "        #('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n",
        "        #('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n",
        "        #('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n",
        "        #('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n",
        "        #('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n",
        "        #('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n",
        "        #('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n",
        "        #('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n",
        "        #('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n",
        "        #('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
        "        #('--no_dropout', action='store_true', help='no dropout for the generator')\n",
        "\n",
        "       # dataset parameters\n",
        "        #('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n",
        "        #('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n",
        "        #('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
        "        #('--num_threads', default=4, type=int, help='# threads for loading data')\n",
        "        #('--batch_size', type=int, default=1, help='input batch size')\n",
        "        #('--load_size', type=int, default=286, help='scale images to this size')\n",
        "        #('--crop_size', type=int, default=256, help='then crop to this size')\n",
        "        #('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
        "        #('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n",
        "        #('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
        "        #('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n",
        "\n",
        "       # additional parameters\n",
        "        #('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
        "        #('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n",
        "        #('--verbose', action='store_true', help='if specified, print more debugging information')\n",
        "        #('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n",
        "\n",
        "       # visdom and HTML visualization parameters\n",
        "        #('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n",
        "        #('--display_ncols', type=int, default=4, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n",
        "        #('--display_id', type=int, default=1, help='window id of the web display')\n",
        "        #('--display_server', type=str, default=\"http://localhost\", help='visdom server of the web display')\n",
        "        #('--display_env', type=str, default='main', help='visdom display environment name (default is \"main\")')\n",
        "        #('--display_port', type=int, default=8097, help='visdom port of the web display')\n",
        "        #('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n",
        "        #('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
        "        #('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n",
        "\n",
        "       # network saving and loading parameters\n",
        "        #('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n",
        "        #('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n",
        "        #('--save_by_iter', action='store_true', help='whether saves model by iteration')\n",
        "        #('--continue_train', action='store_true', help='continue training: load the latest model')\n",
        "        #('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
        "        #('--phase', type=str, default='train', help='train, val, test, etc')\n",
        "\n",
        "       # training parameters\n",
        "        #('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')\n",
        "        #('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')\n",
        "        #('--beta1', type=float, default=0.5, help='momentum term of adam')\n",
        "        #('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
        "        #('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n",
        "        #('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n",
        "        #('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')\n",
        "        #('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations'\n",
        "\n",
        "#---------------------------------------------------------\n",
        "\n",
        "#----- Start the training ------------------------------------\n",
        "if not Use_pretrained_model:\n",
        "    !python3 pytorch-CycleGAN-and-pix2pix/train.py --dataroot \"$imageAB_folder\" --name $model_name --model pix2pix --batch_size $batch_size --preprocess scale_width_and_crop --load_size $Image_min_dim --crop_size $patch_size --checkpoints_dir \"$model_path\"  --no_html --n_epochs $number_of_epochs_lr_stable --n_epochs_decay $number_of_epochs_lr_decay --lr $initial_learning_rate --display_id 0 --save_epoch_freq 5 --input_nc \"$nc\" --output_nc \"$nc\" --dataset_mode \"aligned\"\n",
        "\n",
        "if Use_pretrained_model:\n",
        "    !python3 pytorch-CycleGAN-and-pix2pix/train.py --dataroot \"$imageAB_folder\" --name $model_name --model pix2pix --batch_size $batch_size --preprocess scale_width_and_crop --load_size $Image_min_dim --crop_size $patch_size --checkpoints_dir \"$model_path\"  --no_html --n_epochs $number_of_epochs_lr_stable --n_epochs_decay $number_of_epochs_lr_decay --lr $initial_learning_rate --display_id 0 --save_epoch_freq 5 --continue_train --input_nc \"$nc\" --output_nc \"$nc\" --dataset_mode \"aligned\"\n",
        "\n",
        "\n",
        "#---------------------------------------------------------\n",
        "\n",
        "print(\"Training, done.\")\n",
        "\n",
        "# Displaying the time elapsed for training\n",
        "dt = time.time() - start\n",
        "mins, sec = divmod(dt, 60)\n",
        "hour, mins = divmod(mins, 60)\n",
        "print(\"Time elapsed:\",hour, \"hour(s)\",mins,\"min(s)\",round(sec),\"sec(s)\")\n",
        "\n",
        "# Export pdf summary after training to update document\n",
        "pdf_export(trained = True, augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model,\n",
        "           Saving_path=os.path.join(pix2pix_working_directory, model_name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMSX_rxu1w3K"
      },
      "source": [
        "# **5. Evaluate your model**\n",
        "---\n",
        "\n",
        "<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model.\n",
        "\n",
        "<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TpiOXpP1w3K"
      },
      "source": [
        "## **5.1. Choose the model you want to assess**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i6LXhHf61w3K",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# model name and path\n",
        "#@markdown ###Do you want to assess the model you just trained ?\n",
        "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ###If not, please provide the path to the model folder:\n",
        "# model name and path\n",
        "#@markdown ###Name of the model and path to model folder:\n",
        "QC_model_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#Here we define the loaded model name and path\n",
        "QC_model_name = os.path.basename(QC_model_folder)\n",
        "QC_model_path = os.path.dirname(QC_model_folder)\n",
        "\n",
        "if (Use_the_current_trained_model):\n",
        "    QC_model_name = model_name\n",
        "    QC_model_path = model_path\n",
        "else:\n",
        "    pix2pix_working_directory = os.getcwd()\n",
        "\n",
        "full_QC_model_path = QC_model_path+'/'+QC_model_name+'/'\n",
        "if os.path.exists(full_QC_model_path):\n",
        "    print(\"The \"+QC_model_name+\" network will be evaluated\")\n",
        "else:\n",
        "    W  = '\\033[0m'  # white (normal)\n",
        "    R  = '\\033[31m' # red\n",
        "    print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
        "    print('Please make sure you provide a valid model path and model name before proceeding further.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55F-b-gO1w3K"
      },
      "source": [
        "## **5.2. Identify the best checkpoint to use to make predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sL8A89z1w3K"
      },
      "source": [
        "<font size = 4> Pix2pix save model checkpoints every five epochs. Due to the stochastic nature of GAN networks, the last checkpoint is not always the best one to use. As a consequence, it can be challenging to choose the most suitable checkpoint to use to make predictions.\n",
        "\n",
        "<font size = 4>This section allows you to perform predictions using all the saved checkpoints and to estimate the quality of these predictions by comparing them to the provided ground truths images. Metric used include:\n",
        "\n",
        "\n",
        "<font size = 4>**`patch_size_QC`:** pix2pix can only have input images with dimensions that are multiple of 256 (for example 1024x1024). If this is not the case of the images in the test dataset, they will be rescaled automatically to the given size (1024 by default). Check the image resolution of the images in the training and if needed, consider croping the images to an appropiate size. Note that the images used for the training were not rescaled.\n",
        "\n",
        "<font size = 4>**1. The SSIM (structural similarity) map**\n",
        "\n",
        "<font size = 4>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info).\n",
        "\n",
        "<font size=4>**mSSIM** is the SSIM value calculated across the entire window of both images.\n",
        "\n",
        "<font size=4>**The output below shows the SSIM maps with the mSSIM**\n",
        "\n",
        "<font size = 4>**2. The RSE (Root Squared Error) map**\n",
        "\n",
        "<font size = 4>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n",
        "\n",
        "\n",
        "<font size =4>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n",
        "\n",
        "<font size = 4>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n",
        "\n",
        "<font size=4>**The output below shows the RSE maps with the NRMSE and PSNR values.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eHUa1opwWi9G",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Choose the folders that contain your Quality Control dataset\n",
        "#@markdown ###Path to images:\n",
        "Source_QC_folder = \"\"  #@param{type:\"string\"}\n",
        "Target_QC_folder = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown ###Do you need to prepare test data again?\n",
        "prepare_testdata = True #@param {type:\"boolean\"}\n",
        "#@markdown If not, provide the path containing the images, e.g., \"/content/my_model_images/QC\"\n",
        "path2im = ''#@param{type:\"string\"}\n",
        "\n",
        "#@markdown ###Number of channels:\n",
        "\n",
        "number_channels = \"1\" #@param [\"1\", \"3\"]\n",
        "\n",
        "# get number of channels\n",
        "if number_channels == \"1\":\n",
        "    nc = 1\n",
        "elif number_channels == \"3\":\n",
        "    nc = 3\n",
        "\n",
        "#@markdown ###Image normalisation (the same as for the training dataset is highly recommended):\n",
        "Normalisation_QC_source = \"Contrast stretching\" #@param [\"None\", \"Contrast stretching\", \"Adaptive Equalization\"]\n",
        "Normalisation_QC_target = \"Contrast stretching\" #@param [\"None\", \"Contrast stretching\", \"Adaptive Equalization\"]\n",
        "\n",
        "#@markdown ###Did you evaluate all the checkpoints and only want to visualise the results? (It reduces significantly the processing time if you did it already)\n",
        "avoid_evaluating_again = False #@param {type:\"boolean\"}\n",
        "#@markdown ####Choose the frequency of checkpoints to evaluate. If 1, it will evaluate all the model checkpoints available.\n",
        "QC_evaluation_checkpoint_freq = 1  #@param {type:\"number\"}\n",
        "QC_freq = np.int16(QC_evaluation_checkpoint_freq)\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "patch_size_QC =  1024#@param {type:\"number\"} # in pixels\n",
        "Do_lpips_analysis = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "if not avoid_evaluating_again:\n",
        "  # Create a quality control folder\n",
        "  if os.path.exists(os.path.join(QC_model_path, QC_model_name, \"Quality Control\")):\n",
        "      shutil.rmtree(os.path.join(QC_model_path, QC_model_name, \"Quality Control\"))\n",
        "  os.makedirs(os.path.join(QC_model_path, QC_model_name, \"Quality Control\"))\n",
        "\n",
        "# Create a quality control/Prediction Folder\n",
        "if prepare_testdata or not avoid_evaluating_again:\n",
        "    QC_prediction_results, path2im = prepare_qc_dir(QC_model_path, QC_model_name, pix2pix_working_directory)\n",
        "else:\n",
        "    QC_prediction_results = os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \"Prediction\")\n",
        "\n",
        "if not avoid_evaluating_again:\n",
        "\n",
        "  print(\"-------------------------------------------------------------\")\n",
        "  print(\"Path where the predictions are stored\")\n",
        "  print(QC_prediction_results)\n",
        "  print(\"Path where test images are prepared for testing\")\n",
        "  print(path2im)\n",
        "  print(\"-------------------------------------------------------------\")\n",
        "\n",
        "  # Here we count how many images are in our folder to be predicted and we had a few\n",
        "  Nb_files_Data_folder = len(os.listdir(os.path.join(path2im, \"A\", \"test\")))\n",
        "  #Nb_files_Data_folder = len(os.listdir(Source_QC_folder)) + 10\n",
        "\n",
        "  # List images in Source_QC_folder\n",
        "\n",
        "  #Here we copy and normalise the data\n",
        "  if prepare_testdata:\n",
        "      normalise_data(Source_QC_folder, Target_QC_folder, Normalisation_QC_source, Normalisation_QC_target, path2im)\n",
        "else:\n",
        "  # Here we count how many images are in our folder to be predicted and we had a few\n",
        "  Nb_files_Data_folder = len(os.listdir(Source_QC_folder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "egpLYnYp1w3L"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Run Quality Control\n",
        "if not avoid_evaluating_again:\n",
        "  # Here we count how many images are in our folder to be predicted and we had a few\n",
        "  Nb_files_Data_folder = len(os.listdir(os.path.join(path2im, \"A\", \"test\")))\n",
        "  #Nb_files_Data_folder = len(os.listdir(Source_QC_folder)) + 10\n",
        "\n",
        "  # List images in Source_QC_folder\n",
        "  #Here we create a merged folder containing only imageA\n",
        "  # TODO: check if we can remove this chdir\n",
        "  os.chdir(pix2pix_code_dir)\n",
        "  imageA_folder = os.path.join(path2im, \"A\")\n",
        "  imageB_folder = os.path.join(path2im, \"B\")\n",
        "  imageAB_folder = os.path.join(path2im, \"AB\")\n",
        "  !python3 pytorch-CycleGAN-and-pix2pix/datasets/combine_A_and_B.py --fold_A \"$imageA_folder\" --fold_B \"$imageB_folder\" --fold_AB \"$imageAB_folder\"\n",
        "\n",
        "if not patch_size_QC % 256 == 0:\n",
        "    patch_size_QC = ((int(patch_size_QC / 256)) * 256)\n",
        "    print (\" Your image dimensions are not divisible by 256; therefore your images have now been resized to:\",patch_size_QC)\n",
        "\n",
        "if patch_size_QC < 256:\n",
        "    patch_size_QC = 256\n",
        "\n",
        "# Nb_Checkpoint = len(glob.glob(os.path.join(full_QC_model_path, '*G.pth')))\n",
        "Nb_Checkpoint = [int(s.split(\"_net_\")[0]) for s in os.listdir(full_QC_model_path) if s.__contains__('G.pth') and not s.__contains__('latest')]\n",
        "Nb_Checkpoint.sort()\n",
        "Nb_Checkpoint.append(\"latest\")\n",
        "\n",
        "## Initiate lists\n",
        "Checkpoint_list = []\n",
        "Average_ssim_score_list = []\n",
        "Average_lpips_score_list = []\n",
        "\n",
        "for j in range(0, len(Nb_Checkpoint) + 1, QC_freq):\n",
        "    if j >= len(Nb_Checkpoint):\n",
        "        checkpoints = \"latest\"\n",
        "    else:\n",
        "        checkpoints = Nb_Checkpoint[j] # j*QC_freq\n",
        "\n",
        "    #if checkpoints == len(Nb_Checkpoint): # Nb_Checkpoint*QC_freq:\n",
        "    #    checkpoints = \"latest\"\n",
        "\n",
        "    print(\"The checkpoint currently analysed is =\"+str(checkpoints))\n",
        "\n",
        "    Checkpoint_list.append(checkpoints)\n",
        "\n",
        "    # Create a quality control/Prediction Folder\n",
        "    QC_prediction_results = os.path.join(QC_model_path, QC_model_name, \"Quality Control\", str(checkpoints))\n",
        "    if not avoid_evaluating_again:\n",
        "      if os.path.exists(QC_prediction_results):\n",
        "          shutil.rmtree(QC_prediction_results)\n",
        "      os.makedirs(QC_prediction_results)\n",
        "\n",
        "      #---------------------------- Predictions are performed here ----------------------\n",
        "      !python3 pytorch-CycleGAN-and-pix2pix/test.py --dataroot \"$imageAB_folder\" --name \"$QC_model_name\" --model pix2pix --epoch \"$checkpoints\" --no_dropout --preprocess scale_width --load_size \"$patch_size_QC\" --crop_size \"$patch_size_QC\" --results_dir \"$QC_prediction_results\" --checkpoints_dir \"$QC_model_path\" --direction AtoB --num_test \"$Nb_files_Data_folder\" --input_nc \"$nc\" --output_nc \"$nc\" --dataset_mode \"aligned\"\n",
        "      #-----------------------------------------------------------------------------------\n",
        "\n",
        "      #Here we need to move the data again and remove all the unnecessary folders\n",
        "\n",
        "      Checkpoint_name = \"test_\"+str(checkpoints)\n",
        "      QC_results_images = os.path.join(QC_prediction_results, QC_model_name, Checkpoint_name, \"images\")\n",
        "      QC_results_images_files = os.listdir(QC_results_images)\n",
        "\n",
        "      for f in QC_results_images_files:\n",
        "          shutil.copyfile(os.path.join(QC_results_images, f), os.path.join(QC_prediction_results, f))\n",
        "\n",
        "      #Here we clean up the extra files\n",
        "      shutil.rmtree(os.path.join(QC_prediction_results, QC_model_name))\n",
        "\n",
        "  #-------------------------------- QC for RGB ------------------------------------\n",
        "    if number_channels == \"3\":\n",
        "\n",
        "        Average_SSIM_checkpoint, Average_lpips_checkpoint = QC_RGB(Source_QC_folder, QC_prediction_results)\n",
        "\n",
        "        Average_ssim_score_list.append(Average_SSIM_checkpoint)\n",
        "        Average_lpips_score_list.append(Average_lpips_checkpoint)\n",
        "#------------------------------------------- QC for Grayscale ----------------------------------------------\n",
        "    if number_channels == \"1\":\n",
        "        Average_SSIM_checkpoint, Average_lpips_checkpoint = QC_singlechannel(Source_QC_folder, QC_prediction_results)\n",
        "        Average_ssim_score_list.append(Average_SSIM_checkpoint)\n",
        "        Average_lpips_score_list.append(Average_lpips_checkpoint)\n",
        "\n",
        "        # All data is now processed saved\n",
        "# -------------------------------- Display --------------------------------\n",
        "\n",
        "# Display the IoV vs Checkpoint plot\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(Checkpoint_list, Average_ssim_score_list, label=\"SSIM\")\n",
        "plt.title('Checkpoints vs. SSIM')\n",
        "plt.ylabel('SSIM')\n",
        "plt.xlabel('Checkpoints')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(full_QC_model_path,'Quality Control','SSIMvsCheckpoint_data.png'),bbox_inches='tight',pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# -------------------------------- Display --------------------------------\n",
        "\n",
        "if Do_lpips_analysis:\n",
        "    # Display the lpips vs Checkpoint plot\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.plot(Checkpoint_list, Average_lpips_score_list, label=\"lpips\")\n",
        "    plt.title('Checkpoints vs. lpips')\n",
        "    plt.ylabel('lpips')\n",
        "    plt.xlabel('Checkpoints')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(full_QC_model_path,'Quality Control','lpipsvsCheckpoint_data.png'),bbox_inches='tight',pad_inches=0)\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------------- Display RGB --------------------------------\n",
        "\n",
        "\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "if number_channels == \"3\":\n",
        "    @interact\n",
        "    def show_results(file=os.listdir(Source_QC_folder), checkpoints=Checkpoint_list):\n",
        "\n",
        "        random_choice_shortname_no_PNG = file[:-4]\n",
        "        df1 = pd.read_csv(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints),\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\"), header=0)\n",
        "        df2 = df1.set_index(\"image #\", drop = False)\n",
        "        index_SSIM_GTvsPrediction = df2.loc[file, \"Prediction v. GT mSSIM\"]\n",
        "        index_SSIM_GTvsSource = df2.loc[file, \"Input v. GT mSSIM\"]\n",
        "        lpips_GTvsPrediction = df2.loc[file, \"Prediction v. GT lpips\"]\n",
        "        lpips_GTvsSource = df2.loc[file, \"Input v. GT lpips\"]\n",
        "\n",
        "        #Setting up colours\n",
        "        cmap = None\n",
        "        plt.figure(figsize=(15,15))\n",
        "\n",
        "        # Target (Ground-truth)\n",
        "        plt.subplot(3,3,1)\n",
        "        plt.axis('off')\n",
        "        img_GT = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), random_choice_shortname_no_PNG+\"_real_B.png\"), as_gray=False, pilmode=\"RGB\")\n",
        "        plt.imshow(img_GT, cmap = cmap)\n",
        "        plt.title('Target',fontsize=15)\n",
        "\n",
        "        # Source\n",
        "        plt.subplot(3,3,2)\n",
        "        plt.axis('off')\n",
        "        img_Source = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), random_choice_shortname_no_PNG+\"_real_A.png\"), as_gray=False, pilmode=\"RGB\")\n",
        "        plt.imshow(img_Source, cmap = cmap)\n",
        "        plt.title('Source',fontsize=15)\n",
        "\n",
        "        # Prediction\n",
        "        plt.subplot(3,3,3)\n",
        "        plt.axis('off')\n",
        "        img_Prediction = io.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), random_choice_shortname_no_PNG+\"_fake_B.png\"))\n",
        "        plt.imshow(img_Prediction, cmap = cmap)\n",
        "        plt.title('Prediction',fontsize=15)\n",
        "\n",
        "        # SSIM between GT and Source\n",
        "        plt.subplot(3,3,5)\n",
        "        # plt.axis('off')\n",
        "        plt.tick_params(\n",
        "                      axis='both',      # changes apply to the x-axis and y-axis\n",
        "                      which='both',      # both major and minor ticks are affected\n",
        "                      bottom=False,      # ticks along the bottom edge are off\n",
        "                      top=False,        # ticks along the top edge are off\n",
        "                      left=False,       # ticks along the left edge are off\n",
        "                      right=False,         # ticks along the right edge are off\n",
        "                      labelbottom=False,\n",
        "                      labelleft=False)\n",
        "\n",
        "        img_SSIM_GTvsSource = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"SSIM_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "\n",
        "        imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n",
        "        # plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n",
        "        plt.title('Target vs. Source',fontsize=15)\n",
        "        plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsSource,3)),fontsize=14)\n",
        "        plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n",
        "\n",
        "        # SSIM between GT and Prediction\n",
        "        plt.subplot(3,3,6)\n",
        "        # plt.axis('off')\n",
        "        plt.tick_params(\n",
        "          axis='both',      # changes apply to the x-axis and y-axis\n",
        "          which='both',      # both major and minor ticks are affected\n",
        "          bottom=False,      # ticks along the bottom edge are off\n",
        "          top=False,        # ticks along the top edge are off\n",
        "          left=False,       # ticks along the left edge are off\n",
        "          right=False,         # ticks along the right edge are off\n",
        "          labelbottom=False,\n",
        "          labelleft=False)\n",
        "\n",
        "        img_SSIM_GTvsPrediction = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"SSIM_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "\n",
        "        imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n",
        "        # plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n",
        "        plt.title('Target vs. Prediction',fontsize=15)\n",
        "        plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsPrediction,3)),fontsize=14)\n",
        "\n",
        "        # lpips Error between GT and source\n",
        "\n",
        "        if Do_lpips_analysis:\n",
        "            plt.subplot(3,3,8)\n",
        "            # plt.axis('off')\n",
        "            plt.tick_params(\n",
        "                            axis='both',      # changes apply to the x-axis and y-axis\n",
        "                            which='both',      # both major and minor ticks are affected\n",
        "                            bottom=False,      # ticks along the bottom edge are off\n",
        "                            top=False,        # ticks along the top edge are off\n",
        "                            left=False,       # ticks along the left edge are off\n",
        "                            right=False,         # ticks along the right edge are off\n",
        "                            labelbottom=False,\n",
        "                            labelleft=False)\n",
        "\n",
        "            img_lpips_GTvsSource = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"lpips_GTvsInput_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "\n",
        "            imlpips_GTvsSource = plt.imshow(img_lpips_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n",
        "            plt.colorbar(imlpips_GTvsSource,fraction=0.046,pad=0.04)\n",
        "            plt.title('Target vs. Source',fontsize=15)\n",
        "            plt.xlabel('lpips: '+str(round(lpips_GTvsSource,3)),fontsize=14)\n",
        "            plt.ylabel('Lpips maps',fontsize=20, rotation=0, labelpad=75)\n",
        "\n",
        "\n",
        "            # lpips Error between GT and Prediction\n",
        "            plt.subplot(3,3,9)\n",
        "            # plt.axis('off')\n",
        "            plt.tick_params(\n",
        "                            axis='both',      # changes apply to the x-axis and y-axis\n",
        "                            which='both',      # both major and minor ticks are affected\n",
        "                            bottom=False,      # ticks along the bottom edge are off\n",
        "                            top=False,        # ticks along the top edge are off\n",
        "                            left=False,       # ticks along the left edge are off\n",
        "                            right=False,         # ticks along the right edge are off\n",
        "                            labelbottom=False,\n",
        "                            labelleft=False)\n",
        "\n",
        "            img_lpips_GTvsPrediction = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"lpips_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "\n",
        "            imlpips_GTvsPrediction = plt.imshow(img_lpips_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n",
        "            plt.colorbar(imlpips_GTvsPrediction,fraction=0.046,pad=0.04)\n",
        "            plt.title('Target vs. Prediction',fontsize=15)\n",
        "            plt.xlabel('lpips: '+str(round(lpips_GTvsPrediction,3)),fontsize=14)\n",
        "\n",
        "        plt.savefig(os.path.join(full_QC_model_path, 'Quality Control', 'QC_example_data.png'),bbox_inches='tight',pad_inches=0)\n",
        "\n",
        "# -------------------------------- Display Grayscale --------------------------------\n",
        "\n",
        "if number_channels == \"1\":\n",
        "    @interact\n",
        "    def show_results(file=os.listdir(Source_QC_folder), checkpoints=Checkpoint_list):\n",
        "        random_choice_shortname_no_PNG = file[:-4]\n",
        "\n",
        "        df1 = pd.read_csv(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\"), header=0)\n",
        "        df2 = df1.set_index(\"image #\", drop = False)\n",
        "        index_SSIM_GTvsPrediction = df2.loc[file, \"Prediction v. GT mSSIM\"]\n",
        "        index_SSIM_GTvsSource = df2.loc[file, \"Input v. GT mSSIM\"]\n",
        "\n",
        "        NRMSE_GTvsPrediction = df2.loc[file, \"Prediction v. GT NRMSE\"]\n",
        "        NRMSE_GTvsSource = df2.loc[file, \"Input v. GT NRMSE\"]\n",
        "        PSNR_GTvsSource = df2.loc[file, \"Input v. GT PSNR\"]\n",
        "        PSNR_GTvsPrediction = df2.loc[file, \"Prediction v. GT PSNR\"]\n",
        "        lpips_GTvsPrediction = df2.loc[file, \"Prediction v. GT lpips\"]\n",
        "        lpips_GTvsSource = df2.loc[file, \"Input v. GT lpips\"]\n",
        "\n",
        "        plt.figure(figsize=(20,20))\n",
        "        # Currently only displays the last computed set, from memory\n",
        "        # Target (Ground-truth)\n",
        "        plt.subplot(4,3,1)\n",
        "        plt.axis('off')\n",
        "        img_GT = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), random_choice_shortname_no_PNG+\"_real_B.png\"))\n",
        "\n",
        "        plt.imshow(img_GT, norm=simple_norm(img_GT, percent = 99))\n",
        "        plt.title('Target',fontsize=15)\n",
        "\n",
        "        # Source\n",
        "        plt.subplot(4,3,2)\n",
        "        plt.axis('off')\n",
        "        img_Source = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), random_choice_shortname_no_PNG+\"_real_A.png\"))\n",
        "        plt.imshow(img_Source, norm=simple_norm(img_Source, percent = 99))\n",
        "        plt.title('Source',fontsize=15)\n",
        "\n",
        "        #Prediction\n",
        "        plt.subplot(4,3,3)\n",
        "        plt.axis('off')\n",
        "        img_Prediction = io.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), random_choice_shortname_no_PNG+\"_fake_B.png\"))\n",
        "        plt.imshow(img_Prediction, norm=simple_norm(img_Prediction, percent = 99))\n",
        "        plt.title('Prediction',fontsize=15)\n",
        "\n",
        "        #Setting up colours\n",
        "        cmap = plt.cm.CMRmap\n",
        "\n",
        "        #SSIM between GT and Source\n",
        "        plt.subplot(4,3,5)\n",
        "        #plt.axis('off')\n",
        "        plt.tick_params(\n",
        "          axis='both',      # changes apply to the x-axis and y-axis\n",
        "          which='both',      # both major and minor ticks are affected\n",
        "          bottom=False,      # ticks along the bottom edge are off\n",
        "          top=False,        # ticks along the top edge are off\n",
        "          left=False,       # ticks along the left edge are off\n",
        "          right=False,         # ticks along the right edge are off\n",
        "          labelbottom=False,\n",
        "          labelleft=False)\n",
        "\n",
        "        img_SSIM_GTvsSource = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"SSIM_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "        img_SSIM_GTvsSource = img_SSIM_GTvsSource / 255\n",
        "        imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n",
        "\n",
        "        plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n",
        "        plt.title('Target vs. Source',fontsize=15)\n",
        "        plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsSource,3)),fontsize=14)\n",
        "        plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n",
        "\n",
        "        #SSIM between GT and Prediction\n",
        "        plt.subplot(4,3,6)\n",
        "        #plt.axis('off')\n",
        "        plt.tick_params(\n",
        "          axis='both',      # changes apply to the x-axis and y-axis\n",
        "          which='both',      # both major and minor ticks are affected\n",
        "          bottom=False,      # ticks along the bottom edge are off\n",
        "          top=False,        # ticks along the top edge are off\n",
        "          left=False,       # ticks along the left edge are off\n",
        "          right=False,         # ticks along the right edge are off\n",
        "          labelbottom=False,\n",
        "          labelleft=False)\n",
        "\n",
        "        img_SSIM_GTvsPrediction = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"SSIM_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "        img_SSIM_GTvsPrediction = img_SSIM_GTvsPrediction / 255\n",
        "        imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n",
        "\n",
        "        plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n",
        "        plt.title('Target vs. Prediction',fontsize=15)\n",
        "        plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsPrediction,3)),fontsize=14)\n",
        "\n",
        "        #Root Squared Error between GT and Source\n",
        "        plt.subplot(4,3,8)\n",
        "        #plt.axis('off')\n",
        "        plt.tick_params(\n",
        "          axis='both',      # changes apply to the x-axis and y-axis\n",
        "          which='both',      # both major and minor ticks are affected\n",
        "          bottom=False,      # ticks along the bottom edge are off\n",
        "          top=False,        # ticks along the top edge are off\n",
        "          left=False,       # ticks along the left edge are off\n",
        "          right=False,         # ticks along the right edge are off\n",
        "          labelbottom=False,\n",
        "          labelleft=False)\n",
        "\n",
        "        img_RSE_GTvsSource = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"RSE_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "        img_RSE_GTvsSource = img_RSE_GTvsSource / 255\n",
        "\n",
        "        imRSE_GTvsSource = plt.imshow(img_RSE_GTvsSource, cmap = cmap, vmin=0, vmax = 1)\n",
        "        plt.colorbar(imRSE_GTvsSource,fraction=0.046,pad=0.04)\n",
        "        plt.title('Target vs. Source',fontsize=15)\n",
        "        plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsSource,3))+', PSNR: '+str(round(PSNR_GTvsSource,3)),fontsize=14)\n",
        "        #plt.title('Target vs. Source PSNR: '+str(round(PSNR_GTvsSource,3)))\n",
        "        plt.ylabel('RSE maps',fontsize=20, rotation=0, labelpad=75)\n",
        "\n",
        "        #Root Squared Error between GT and Prediction\n",
        "        plt.subplot(4,3,9)\n",
        "        #plt.axis('off')\n",
        "        plt.tick_params(\n",
        "          axis='both',      # changes apply to the x-axis and y-axis\n",
        "          which='both',      # both major and minor ticks are affected\n",
        "          bottom=False,      # ticks along the bottom edge are off\n",
        "          top=False,        # ticks along the top edge are off\n",
        "          left=False,       # ticks along the left edge are off\n",
        "          right=False,         # ticks along the right edge are off\n",
        "          labelbottom=False,\n",
        "          labelleft=False)\n",
        "\n",
        "        img_RSE_GTvsPrediction = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"RSE_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "\n",
        "        img_RSE_GTvsPrediction = img_RSE_GTvsPrediction / 255\n",
        "\n",
        "        imRSE_GTvsPrediction = plt.imshow(img_RSE_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n",
        "        plt.colorbar(imRSE_GTvsPrediction,fraction=0.046,pad=0.04)\n",
        "        plt.title('Target vs. Prediction',fontsize=15)\n",
        "        plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsPrediction,3))+', PSNR: '+str(round(PSNR_GTvsPrediction,3)),fontsize=14)\n",
        "\n",
        "        #lpips Error between GT and source\n",
        "\n",
        "        if Do_lpips_analysis:\n",
        "            plt.subplot(4,3,11)\n",
        "\n",
        "            plt.tick_params(\n",
        "            axis='both',      # changes apply to the x-axis and y-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,        # ticks along the top edge are off\n",
        "            left=False,       # ticks along the left edge are off\n",
        "            right=False,         # ticks along the right edge are off\n",
        "            labelbottom=False,\n",
        "            labelleft=False)\n",
        "\n",
        "            img_lpips_GTvsSource = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"lpips_GTvsInput_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "\n",
        "            img_lpips_GTvsSource = img_lpips_GTvsSource / 255\n",
        "\n",
        "            imlpips_GTvsSource = plt.imshow(img_lpips_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n",
        "            plt.colorbar(imlpips_GTvsSource,fraction=0.046,pad=0.04)\n",
        "            plt.title('Target vs. Source',fontsize=15)\n",
        "            plt.xlabel('lpips: '+str(round(lpips_GTvsSource,3)),fontsize=14)\n",
        "            plt.ylabel('Lpips maps',fontsize=20, rotation=0, labelpad=75)\n",
        "\n",
        "            #lpips Error between GT and Prediction\n",
        "            plt.subplot(4,3,12)\n",
        "            #plt.axis('off')\n",
        "            plt.tick_params(\n",
        "            axis='both',      # changes apply to the x-axis and y-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,        # ticks along the top edge are off\n",
        "            left=False,       # ticks along the left edge are off\n",
        "            right=False,         # ticks along the right edge are off\n",
        "            labelbottom=False,\n",
        "            labelleft=False)\n",
        "\n",
        "            img_lpips_GTvsPrediction = imageio.imread(os.path.join(full_QC_model_path,'Quality Control', str(checkpoints), \"lpips_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n",
        "\n",
        "            img_lpips_GTvsPrediction = img_lpips_GTvsPrediction / 255\n",
        "\n",
        "            imlpips_GTvsPrediction = plt.imshow(img_lpips_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n",
        "            plt.colorbar(imlpips_GTvsPrediction,fraction=0.046,pad=0.04)\n",
        "            plt.title('Target vs. Prediction',fontsize=15)\n",
        "            plt.xlabel('lpips: '+str(round(lpips_GTvsPrediction,3)),fontsize=14)\n",
        "\n",
        "        plt.savefig(os.path.join(full_QC_model_path, 'Quality Control', 'QC_example_data.png'),bbox_inches='tight',pad_inches=0)\n",
        "\n",
        "#Make a pdf summary of the QC results\n",
        "qc_pdf_export()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQu4kJwAzzpE"
      },
      "source": [
        "## **5.3. Export your model into the BioImage Model Zoo format**\n",
        "\n",
        "<font size = 4>This section exports the model into the [BioImage Model Zoo](https://bioimage.io/#/) format so it can be used directly with other community partners such as deepImageJ, Ilastik or BiaPy.\n",
        "\n",
        "<font size = 4>Please run the cells of previous Sections 5.1 and 5.2 before going ahead.\n",
        "\n",
        "<font size = 4>Once the cell is executed, you will find a new zip file with the name specified in `trained_model_name.bioimage.io.model` in the model folder specified at the beginning of Section 5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9WvxYBkzEa1d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ------------- User input ------------\n",
        "#@markdown ##Which model checkpoint would you like to export?\n",
        "checkpoints = \"latest\" #@param {type:\"string\"}\n",
        "\n",
        "# information about the model\n",
        "#@markdown ##Introduce the information to document your model:\n",
        "Trained_model_name    = \"\" #@param {type:\"string\"}\n",
        "Trained_model_authors =  \"\\\"[Author 1, Author 2]\\\"\" #@param {type:\"string\"}\n",
        "Trained_model_authors_affiliation =  \"\\\"[Author 1 Affiliation, Author 2 Affiliation]\\\"\" #@param {type:\"string\"}\n",
        "Trained_model_description = \"A conditional cycleGAN trained to infer XXX from XXX \" #@param {type:\"string\"}\n",
        "Trained_model_license = \"CC-BY-NC-1.0\" #@param [\"BSD-Protection\", \"CC-BY-SA-2.0\", \"Unicode-DFS-2016\", \"CC-BY-ND-2.5\", \"CC-BY-NC-ND-3.0\", \"BSD-3-Clause-LBNL\", \"NCGL-UK-2.0\", \"GPL-1.0+\", \"OSL-1.0\", \"MIT\", \"MPL-2.0\", \"CC-BY-NC-1.0\", \"CC-BY-NC-ND-2.5\", \"CC-BY-SA-2.0-UK\", \"BSD-4-Clause-Shortened\"]\n",
        "\n",
        "\n",
        "\n",
        "Trained_model_references = [\"Isola et al. arXiv in 2016\",\n",
        "                            \"Lucas von Chamier et al. biorXiv 2020\"]\n",
        "Trained_model_DOI = [\"https://arxiv.org/abs/1611.07004\",\n",
        "                     \"https://doi.org/10.1101/2020.03.20.000133\"]\n",
        "\n",
        "# create the author spec input\n",
        "auth_names = Trained_model_authors[1:-1].split(\",\")\n",
        "auth_affs = Trained_model_authors_affiliation[1:-1].split(\",\")\n",
        "assert len(auth_names) == len(auth_affs)\n",
        "authors = [{\"name\": auth_name, \"affiliation\": auth_aff} for auth_name, auth_aff in zip(auth_names, auth_affs)]\n",
        "# create the citation input spec\n",
        "assert len(Trained_model_DOI) == len(Trained_model_references)\n",
        "citations = [{'text': text, 'doi': doi} for text, doi in zip(Trained_model_references, Trained_model_DOI)]\n",
        "\n",
        "\n",
        "# Training data\n",
        "# ---------------------------------------\n",
        "#@markdown ##Include information about training data (optional):\n",
        "include_training_data = False #@param {type: \"boolean\"}\n",
        "#@markdown ### - If it is published in the BioImage Model Zoo, please, provide the ID (e.g., `zero/dataset_fnet_3d_zerocostdl4mic`)\n",
        "data_from_bioimage_model_zoo = False #@param {type: \"boolean\"}\n",
        "training_data_ID = ''#@param {type:\"string\"}\n",
        "#@markdown ### - If not, please provide the URL tot he data and a short description\n",
        "training_data_source = ''#@param {type:\"string\"}\n",
        "training_data_description = ''#@param {type:\"string\"}\n",
        "\n",
        "# create the training data\n",
        "if include_training_data:\n",
        "    if data_from_bioimage_model_zoo:\n",
        "      training_data = {\"id\": training_data_ID}\n",
        "    else:\n",
        "      training_data = {\"source\": training_data_source,\n",
        "                       \"description\": training_data_description}\n",
        "else:\n",
        "    training_data={}\n",
        "\n",
        "# Add example image information\n",
        "# ---------------------------------------\n",
        "\n",
        "#@markdown ##Include and example image to test the model:\n",
        "\n",
        "default_example_image_from_QC = True #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please input:\n",
        "fileID    =  \"\" #@param {type:\"string\"}\n",
        "#@markdown ###Pixel size (in microns) of the example image:\n",
        "# information about the example image\n",
        "PixelSize = 1 #@param {type:\"number\"}\n",
        "\n",
        "if default_example_image_from_QC:\n",
        "    fileID = os.path.join(Source_QC_folder, os.listdir(Source_QC_folder)[0])\n",
        "\n",
        "# attach the QC report to the model (if it exists)\n",
        "# ---------------------------------------\n",
        "\n",
        "qc_path = os.path.join(full_QC_model_path, 'Quality Control', 'training_evaluation.csv')\n",
        "if os.path.exists(qc_path):\n",
        "  attachments = {\"files\": [qc_path]}\n",
        "else:\n",
        "  attachments = None\n",
        "\n",
        "# Preprocessing\n",
        "# ---------------------------------------\n",
        "if Normalisation_QC_source == \"None\":\n",
        "  bmz_preprpocess = [[{\"name\": \"scale_range\", \"kwargs\": {\"min_percentile\": 0.,\n",
        "                                  \"max_percentile\": 100.,\n",
        "                                  \"mode\": \"per_sample\",\n",
        "                                  \"axes\": \"xy\"}}, {\"name\": \"scale_linear\",\n",
        "                                                   \"kwargs\": {\"gain\": 2, \"offset\": -1,\n",
        "                                                              \"axes\": \"xy\"}}]]\n",
        "if Normalisation_QC_source == \"Contrast stretching\":\n",
        "  bmz_preprpocess = [[{\"name\": \"scale_range\", \"kwargs\": {\"min_percentile\": 1.,\n",
        "                                                         \"max_percentile\": 99.9,\n",
        "                                                         \"mode\": \"per_sample\",\n",
        "                                                         \"axes\": \"xy\"}},\n",
        "                      {\"name\": \"scale_linear\", \"kwargs\": {\"gain\": 2,\n",
        "                                                          \"offset\": -1,\n",
        "                                                          \"axes\": \"xy\"}}]]\n",
        "\n",
        "\n",
        "if Normalisation_QC_source == \"Adaptive Equalization\":\n",
        "  print(\"Please note that Adaptive Histogram Equalization is not implemented in the BioImage Model Zoo format. This step needs to be implemented manually by the user of the software before using the model.\")\n",
        "  bmz_preprpocess = None\n",
        "\n",
        "#bmz_postprocess = [[{\"name\": \"scale_linear\", \"kwargs\": {\"gain\": 0.5,\n",
        "                                                        #\"offset\": 0.5,\n",
        "                                                        #\"axes\": \"xy\"}}]]\n",
        "bmz_postprocess = None\n",
        "# Input & output specs\n",
        "# ---------------------------------------\n",
        "pixel_size = {\"x\": PixelSize, \"y\": PixelSize}\n",
        "\n",
        "kwargs = dict(\n",
        "  input_names=[\"input\"],\n",
        "  input_data_range=[[0., 255.]],\n",
        "  input_axes=[\"bcyx\"],\n",
        "  pixel_sizes=[pixel_size],\n",
        "  preprocessing = bmz_preprpocess)\n",
        "\n",
        "output_spec = dict(\n",
        "  output_names=[\"output\"],\n",
        "  output_data_range=[[-np.inf, np.inf]],\n",
        "  output_axes=[\"bcyx\"],\n",
        "  postprocessing=bmz_postprocess,\n",
        "  output_reference=[\"input\"],\n",
        "  output_scale=[4*[1]], # consider changing it if the input has more than one channel\n",
        "  output_offset=[4*[0]]\n",
        ")\n",
        "kwargs.update(output_spec)\n",
        "\n",
        "\n",
        "# Export the model\n",
        "# ---------------------------------------\n",
        "\n",
        "# where to save the model\n",
        "output_root = os.path.join(full_QC_model_path, 'bioimage.io.model')\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "output_path = os.path.join(output_root, f\"{Trained_model_name}.zip\")\n",
        "\n",
        "# export torchscript\n",
        "path_model_checkpoint = os.path.join(output_root, f'{checkpoints}_net_G_torchscript.pt')\n",
        "export_cyclegan_torchscript_model(path_model_checkpoint)\n",
        "os.chdir(pix2pix_code_dir)\n",
        "\n",
        "!python3 pytorch-CycleGAN-and-pix2pix/cyclegan_model_export.py --dataroot \"$pix2pix_code_dir\" --name \"$QC_model_name\" --model pix2pix --epoch \"$checkpoints\" --no_dropout --preprocess scale_width --load_size \"$patch_size_QC\" --crop_size \"$patch_size_QC\" --results_dir \"$QC_prediction_results\" --checkpoints_dir \"$QC_model_path\" --num_test \"$Nb_files_Data_folder\" --input_nc \"$nc\" --output_nc \"$nc\" --dataset_mode \"aligned\"\n",
        "\n",
        "\n",
        "# Create a markdown readme with information\n",
        "# Content for README.md with detailed information on Pix2Pix\n",
        "readme_content = \"\"\"\n",
        "#Conditional generative model for Image-to-Image Translation (Pix2Pix)\n",
        "\n",
        "## Description\n",
        "This model consists on a conditional cycle-Generative Adversarial Network (cycleGAN), popularly known as pix2pix, to transform images from one domain into another (e.g., to predict a DAPI nuclei staining fluorescence channel from a brightfield image of cells).\n",
        "The conditional cycleGAN allows for a supervised training of the network.\n",
        "Thus, this model was trained using a pair set of images.\n",
        "This model was trained using a ZeroCostDL4Mic notebook developed on top of the work “Image-to-Image Translation with Conditional Adversarial Networks\" by Isola et al. (https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "## Disclaimer\n",
        "This model was trained to predict images from a specific image domain and imaged living matter.\n",
        "Further details about how to train a similar model are given in https://github.com/HenriquesLab/DeepLearning_Collab/wiki\n",
        "\n",
        " ## Reference and Citation\n",
        "If you use this model for your research, please refer to the ZeroCostDL4Mic paper and the original Pix2Pix model paper:\n",
        "\n",
        "- von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0\n",
        "- Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. Image-to-Image Translation with Conditional Adversarial Networks (2016). arXiv preprint arXiv:1611.07004\n",
        "\"\"\"\n",
        "\n",
        "# Define the path for the README.md file\n",
        "readme_path = os.path.join(output_root, \"README.md\")\n",
        "\n",
        "# Write the README content to the file\n",
        "with open(readme_path, \"w\") as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "# Prepare test image data\n",
        "# ---------------------------------------\n",
        "# Make a directory to svae the input image and normalise it accordingly.\n",
        "#test_path = os.path.join(pix2pix_code_dir, \"test\")\n",
        "#os.makedirs(os.path.join(test_path, \"input\"), exist_ok=True)\n",
        "\n",
        "#test_GAN = os.path.join(test_path, \"GAN\")\n",
        "\n",
        "#test_GAN_A = os.path.join(test_GAN, \"A\")\n",
        "#os.makedirs(os.path.join(test_GAN_A, \"test\"), exist_ok=True)\n",
        "\n",
        "#test_GAN_B = os.path.join(test_GAN, \"B\")\n",
        "#os.makedirs(os.path.join(test_GAN_B, \"test\"), exist_ok=True)\n",
        "\n",
        "#test_GAN_AB = os.path.join(test_GAN, \"AB\")\n",
        "#os.makedirs(os.path.join(test_GAN_AB, \"test\"), exist_ok=True)\n",
        "\n",
        "x = io.imread(fileID)\n",
        "# We crop an image of a shape that can be processed with pix2pix without tiling\n",
        "# x may be a 3 channel image. we assume that x,y have the highest dimensions\n",
        "patch_size_QC = np.min([np.int(np.floor(i/512) * 512) for i in x.shape[:2]])\n",
        "print(f\"The test image is cropped to a size of {patch_size_QC}x{patch_size_QC}\")\n",
        "x = x[:patch_size_QC, :patch_size_QC]\n",
        "\n",
        "#io.imsave(os.path.join(test_path, \"input\", \"test-input.tif\"), x)\n",
        "\n",
        "if nc==1:\n",
        "  x = np.expand_dims(x, axis = [0, 1]) # add batch and channel dimensions\n",
        "else:\n",
        "  x = np.expand_dims(x, axis = 0) # add batch dimension\n",
        "np.save(os.path.join(output_root, \"test-input.npy\"), x)\n",
        "\n",
        "# only normalise input\n",
        "#normalise_data(os.path.join(test_path, \"input\"),\n",
        "#               os.path.join(test_path, \"input\"),\n",
        "#               Normalisation_QC_source,\n",
        "#               Normalisation_QC_source,\n",
        "#               test_GAN)\n",
        "# process the example image\n",
        "#os.chdir(pix2pix_code_dir)\n",
        "#!python3 pytorch-CycleGAN-and-pix2pix/datasets/combine_A_and_B.py --fold_A \"$test_GAN_A\" --fold_B \"$test_GAN_B\" --fold_AB \"$test_GAN_AB\"\n",
        "#!python3 pytorch-CycleGAN-and-pix2pix/test.py --dataroot \"$test_GAN_AB\" --name \"$QC_model_name\" --model pix2pix --epoch $checkpoints --no_dropout --preprocess scale_width --load_size $patch_size_QC --crop_size $patch_size_QC --results_dir \"$test_path\" --checkpoints_dir \"$QC_model_path\" --num_test 1  --input_nc \"$nc\" --output_nc \"$nc\" --dataset_mode \"aligned\"\n",
        "\n",
        "#filename = fileID.split(\"/\")[-1]\n",
        "#y = io.imread(os.path.join(test_path, QC_model_name, f\"test_{checkpoints}\", \"images\", \"test-input_fake_B.png\")) # this image is saved as RGB by pix2pix\n",
        "#y = y[...,0]\n",
        "#if nc==1:\n",
        "#  y = np.expand_dims(y, axis=[0, 1]) # add batch and channel dimensions\n",
        "#else:\n",
        "#  y = np.expand_dims(y, axis=[0, 1]) # add batch dimension\n",
        "#np.save(os.path.join(output_root, \"test-output.npy\"), y)\n",
        "np.save(os.path.join(output_root, \"test-output.npy\"), x.astype(np.float32))\n",
        "# Build the bioimage model zoo model\n",
        "# ---------------------------------------\n",
        "for i in range(2):\n",
        "  # we create the model, process the input image and create the model again with the correct output.\n",
        "  build_model(\n",
        "      name = Trained_model_name,\n",
        "      description = Trained_model_description,\n",
        "      # additional metadata about authors, licenses, citation etc.\n",
        "      authors = authors,\n",
        "      license = Trained_model_license,\n",
        "      documentation = readme_path,\n",
        "      # the weight file and the type of the weights\n",
        "      weight_uri = path_model_checkpoint,\n",
        "      weight_type = \"torchscript\",\n",
        "      # the test input and output data as well as the description of the tensors\n",
        "      # these are passed as list because we support multiple inputs / outputs per model\n",
        "      test_inputs = [os.path.join(output_root, \"test-input.npy\")],\n",
        "      test_outputs =  [os.path.join(output_root, \"test-output.npy\")],\n",
        "      # where to save the model zip, how to call the model and a short description of it\n",
        "      output_path = output_path,\n",
        "      tags=[\"in-silico-labeling\",\"pytorch\", \"cyclegan\", \"conditional-gan\",\n",
        "            \"zerocostdl4mic\", \"deepimagej\", \"actin\", \"dapi\", \"cells\", \"nuclei\",\n",
        "            \"fluorescence-light-microscopy\", \"2d\"],  # the tags are used to make models more findable on the website\n",
        "      cite = citations,\n",
        "      training_data = training_data,\n",
        "      attachments = attachments,\n",
        "      add_deepimagej_config=True,\n",
        "      **kwargs\n",
        "      )\n",
        "  if i == 0:\n",
        "\n",
        "    predict_image(model_rdf = output_path, inputs = os.path.join(output_root, \"test-input.npy\"), outputs = os.path.join(output_root, \"test-output.npy\"))\n",
        "\n",
        "# check that the model works for keras and tensorflow\n",
        "res = test_model(output_path, weight_format=\"torchscript\")\n",
        "success = True\n",
        "if res[-1][\"error\"] is not None:\n",
        "  success = False\n",
        "  print(\"test-model failed:\", res[-1][\"error\"])\n",
        "\n",
        "if success:\n",
        "  print(\"The bioimage.io model was successfully exported to\", output_path)\n",
        "else:\n",
        "  print(\"The bioimage.io model was exported to\", output_path)\n",
        "  print(\"Some tests of the model did not work!l.\")\n",
        "  print(\"You can still download and test the model, but it may not work as expected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTN0p7BY1w3O"
      },
      "source": [
        "# **6. Using the trained model**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju5OiN0h1w3P"
      },
      "source": [
        "## **6.1. Generate prediction(s) from unseen dataset**\n",
        "---\n",
        "\n",
        "<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** as PNG images, under the model name, in a subfolder called `results_<checkpoint>`.\n",
        "\n",
        "<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n",
        "\n",
        "<font size = 4>**`Result_folder`:** This folder will contain the predicted output images.\n",
        "\n",
        "<font size = 4>**`checkpoint`:** Choose the checkpoint number you would like to use to perform predictions. To use the \"latest\" checkpoint, input \"latest\".\n",
        "\n",
        "<font size = 4>**`input_rescale_size`:** pix2pix can only have input images with dimensions that are multiple of 256 (for example 1024x1024). If that is not the case of the new images, these will be reshaped to the given size. 1024 by default. Consider the input shape and resolution used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qR5TUONe1w3P"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Provide the path to your dataset and to the folder where the predictions are saved, then play the cell to predict outputs from your unseen images.\n",
        "import glob\n",
        "\n",
        "latest = \"latest\"\n",
        "\n",
        "Data_folder = \"\" #@param {type:\"string\"}\n",
        "Result_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###Number of channels:\n",
        "\n",
        "number_channels = \"1\" #@param [\"1\", \"3\"]\n",
        "\n",
        "# get number of channels\n",
        "if number_channels == \"1\":\n",
        "  nc = 1\n",
        "elif number_channels == \"3\":\n",
        "  nc = 3\n",
        "\n",
        "#@markdown ###Image normalisation:\n",
        "\n",
        "Normalisation_prediction_source = \"Contrast stretching\" #@param [\"None\", \"Contrast stretching\", \"Adaptive Equalization\"]\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Do you want to use the current trained model?\n",
        "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ###If not, please provide the path to the model folder:\n",
        "\n",
        "Prediction_model_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###What model checkpoint would you like to use?\n",
        "\n",
        "checkpoint = latest#@param {type:\"raw\"}\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "\n",
        "input_rescale_size =  1024#@param {type:\"number\"} # in pixels\n",
        "patch_size = input_rescale_size\n",
        "if not patch_size % 256 == 0:\n",
        "    patch_size = ((int(patch_size / 256)) * 256)\n",
        "    print (\" Your image dimensions are not divisible by 256; therefore your images have now been resized to:\",patch_size)\n",
        "\n",
        "if patch_size < 256:\n",
        "    patch_size = 256\n",
        "\n",
        "#Here we find the loaded model name and parent path\n",
        "Prediction_model_name = os.path.basename(Prediction_model_folder)\n",
        "Prediction_model_path = os.path.dirname(Prediction_model_folder)\n",
        "\n",
        "#here we check if we use the newly trained network or not\n",
        "if (Use_the_current_trained_model):\n",
        "    try:\n",
        "        print(\"Using current trained network\")\n",
        "        Prediction_model_name = model_name\n",
        "        Prediction_model_path = model_path\n",
        "    except:\n",
        "        print(\"Using current tested network in the QC\")\n",
        "        Prediction_model_name = QC_model_name\n",
        "        Prediction_model_path = QC_model_path\n",
        "\n",
        "if not patch_size % 256 == 0:\n",
        "  patch_size = ((int(patch_size / 256)) * 256)\n",
        "  print (\" Your image dimensions are not divisible by 256; therefore your images have now been resized to:\",patch_size_QC)\n",
        "\n",
        "if patch_size < 256:\n",
        "  patch_size = 256\n",
        "\n",
        "#here we check if the model exists\n",
        "full_Prediction_model_path = Prediction_model_path+'/'+Prediction_model_name+'/'\n",
        "\n",
        "if os.path.exists(full_Prediction_model_path):\n",
        "  print(\"The \"+Prediction_model_name+\" network will be used.\")\n",
        "else:\n",
        "  W  = '\\033[0m'  # white (normal)\n",
        "  R  = '\\033[31m' # red\n",
        "  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
        "  print('Please make sure you provide a valid model path and model name before proceeding further.')\n",
        "\n",
        "Nb_Checkpoint = len(glob.glob(os.path.join(full_Prediction_model_path, '*G.pth')))+1\n",
        "\n",
        "if not checkpoint == \"latest\":\n",
        "\n",
        "  if  checkpoint < 10:\n",
        "    checkpoint = 5\n",
        "\n",
        "  if not checkpoint % 5 == 0:\n",
        "    checkpoint = ((int(checkpoint / 5)-1) * 5)\n",
        "    print (bcolors.WARNING + \" Your chosen checkpoints is not divisible by 5; therefore the checkpoints chosen is now:\",checkpoints)\n",
        "\n",
        "  if checkpoint == Nb_Checkpoint*5:\n",
        "    checkpoint = \"latest\"\n",
        "\n",
        "  if checkpoint > Nb_Checkpoint*5:\n",
        "    checkpoint = \"latest\"\n",
        "\n",
        "# Here we need to move the data to be analysed so that pix2pix can find them\n",
        "\n",
        "Saving_path_prediction= os.path.join(pix2pix_working_directory, Prediction_model_name)\n",
        "\n",
        "if os.path.exists(Saving_path_prediction):\n",
        "  shutil.rmtree(Saving_path_prediction)\n",
        "os.makedirs(Saving_path_prediction)\n",
        "\n",
        "imageA_folder = os.path.join(Saving_path_prediction, \"A\")\n",
        "os.makedirs(imageA_folder)\n",
        "\n",
        "imageB_folder = os.path.join(Saving_path_prediction, \"B\")\n",
        "os.makedirs(imageB_folder)\n",
        "\n",
        "imageAB_folder = os.path.join(Saving_path_prediction, \"AB\")\n",
        "os.makedirs(imageAB_folder)\n",
        "\n",
        "testAB_Folder = os.path.join(imageAB_folder, \"test\")\n",
        "os.makedirs(testAB_Folder)\n",
        "\n",
        "testA_Folder = os.path.join(imageA_folder, \"test\")\n",
        "os.makedirs(testA_Folder)\n",
        "\n",
        "testB_Folder = os.path.join(imageB_folder, \"test\")\n",
        "os.makedirs(testB_Folder)\n",
        "\n",
        "#Here we copy and normalise the data\n",
        "\n",
        "if Normalisation_prediction_source == \"Contrast stretching\":\n",
        "\n",
        "  for filename in os.listdir(Data_folder):\n",
        "    img = io.imread(os.path.join(Data_folder,filename)).astype(np.float32)\n",
        "    short_name = os.path.splitext(filename)\n",
        "\n",
        "    p2, p99 = np.percentile(img, (1., 99.9))\n",
        "    img = exposure.rescale_intensity(img, in_range=(p2, p99))\n",
        "\n",
        "    img = 255 * img # Now scale by 255\n",
        "    img = img.astype(np.uint8)\n",
        "    cv2.imwrite(os.path.join(testA_Folder, short_name[0]+\".png\"), img)\n",
        "    cv2.imwrite(os.path.join(testB_Folder, short_name[0]+\".png\"), img)\n",
        "\n",
        "if Normalisation_prediction_source == \"Adaptive Equalization\":\n",
        "\n",
        "  for filename in os.listdir(Data_folder):\n",
        "\n",
        "    img = io.imread(os.path.join(Data_folder,filename))\n",
        "    short_name = os.path.splitext(filename)\n",
        "\n",
        "    img = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
        "\n",
        "    img = 255 * img # Now scale by 255\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    cv2.imwrite(os.path.join(testA_Folder, short_name[0]+\".png\"), img)\n",
        "    cv2.imwrite(os.path.join(testB_Folder, short_name[0]+\".png\"), img)\n",
        "\n",
        "if Normalisation_prediction_source == \"None\":\n",
        "  for filename in os.listdir(Data_folder):\n",
        "    img = io.imread(os.path.join(Data_folder,filename))\n",
        "    short_name = os.path.splitext(filename)\n",
        "    cv2.imwrite(os.path.join(testA_Folder, short_name[0]+\".png\"), img)\n",
        "    cv2.imwrite(os.path.join(testB_Folder, short_name[0]+\".png\"), img)\n",
        "\n",
        "# Here we create a merged A / A image for the prediction\n",
        "# TODO: check if we can remove this chdir\n",
        "os.chdir(pix2pix_code_dir)\n",
        "!python3 pytorch-CycleGAN-and-pix2pix/datasets/combine_A_and_B.py --fold_A \"$imageA_folder\" --fold_B \"$imageB_folder\" --fold_AB \"$imageAB_folder\"\n",
        "\n",
        "# Here we count how many images are in our folder to be predicted and we add a few\n",
        "Nb_files_Data_folder = len(os.listdir(Data_folder)) +10\n",
        "\n",
        "# This will find the image dimension of a randomly choosen image in Data_folder\n",
        "random_choice = random.choice(os.listdir(Data_folder))\n",
        "x = imageio.imread(os.path.join(Data_folder, random_choice))\n",
        "\n",
        "#Find image XY dimension\n",
        "Image_Y = x.shape[0]\n",
        "Image_X = x.shape[1]\n",
        "\n",
        "Image_min_dim = min(Image_Y, Image_X)\n",
        "\n",
        "\n",
        "#-------------------------------- Perform predictions -----------------------------\n",
        "\n",
        "#-------------------------------- Options that can be used to perform predictions -----------------------------\n",
        "\n",
        "# basic parameters\n",
        "        #('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n",
        "        #('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n",
        "        #('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
        "        #('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
        "\n",
        "# model parameters\n",
        "        #('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n",
        "        #('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n",
        "        #('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n",
        "        #('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n",
        "        #('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n",
        "        #('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n",
        "        #('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n",
        "        #('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n",
        "        #('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n",
        "        #('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n",
        "        #('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
        "        #('--no_dropout', action='store_true', help='no dropout for the generator')\n",
        "\n",
        "# dataset parameters\n",
        "        #('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n",
        "        #('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n",
        "        #('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
        "        #('--num_threads', default=4, type=int, help='# threads for loading data')\n",
        "        #('--batch_size', type=int, default=1, help='input batch size')\n",
        "        #('--load_size', type=int, default=286, help='scale images to this size')\n",
        "        #('--crop_size', type=int, default=256, help='then crop to this size')\n",
        "        #('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
        "        #('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n",
        "        #('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
        "        #('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n",
        "\n",
        "# additional parameters\n",
        "        #('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
        "        #('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n",
        "        #('--verbose', action='store_true', help='if specified, print more debugging information')\n",
        "        #('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n",
        "\n",
        "\n",
        "        #('--ntest', type=int, default=float(\"inf\"), help='# of test examples.')\n",
        "        #('--results_dir', type=str, default='./results/', help='saves results here.')\n",
        "        #('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n",
        "        #('--phase', type=str, default='test', help='train, val, test, etc')\n",
        "\n",
        "# Dropout and Batchnorm has different behavioir during training and test.\n",
        "        #('--eval', action='store_true', help='use eval mode during test time.')\n",
        "        #('--num_test', type=int, default=50, help='how many test images to run')\n",
        "        # rewrite devalue values\n",
        "\n",
        "# To avoid cropping, the load_size should be the same as crop_size\n",
        "        #parser.set_defaults(load_size=parser.get_default('crop_size'))\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#---------------------------- Predictions are performed here ----------------------\n",
        "# TODO: check if we can remove this chdir\n",
        "os.chdir(pix2pix_code_dir)\n",
        "\n",
        "!python3 pytorch-CycleGAN-and-pix2pix/test.py --dataroot \"$imageAB_folder\" --name \"$Prediction_model_name\" --model pix2pix --no_dropout --preprocess scale_width --load_size $patch_size --crop_size $patch_size --results_dir \"$Result_folder\" --checkpoints_dir \"$Prediction_model_path\" --num_test $Nb_files_Data_folder --epoch $checkpoint --input_nc \"$nc\" --output_nc \"$nc\" --dataset_mode \"aligned\"\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Checkpoint_name = \"results_\"+str(checkpoint)\n",
        "\n",
        "\n",
        "Prediction_results_folder = os.path.join(Result_folder, Prediction_model_name, Checkpoint_name, \"images\")\n",
        "\n",
        "Prediction_results_images = os.listdir(Prediction_results_folder)\n",
        "\n",
        "for f in Prediction_results_images:\n",
        "  if (f.endswith(\"_real_B.png\")):\n",
        "    os.remove(Prediction_results_folder+\"/\"+f)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdnb77E15zLE"
      },
      "source": [
        "## **6.2. Inspect the predicted output**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CrEBdt9T53Eh"
      },
      "outputs": [],
      "source": [
        "# @markdown ##Run this cell to display a randomly chosen input and its corresponding predicted output.\n",
        "import os\n",
        "# This will display a randomly chosen dataset input and predicted output\n",
        "random_choice = random.choice(os.listdir(Data_folder))\n",
        "random_choice_no_extension = os.path.splitext(random_choice)\n",
        "\n",
        "results_path_test = os.path.join(Result_folder, Prediction_model_name, \"test_\"+str(checkpoint), \"images\")\n",
        "x = imageio.imread(os.path.join(results_path_test, random_choice_no_extension[0]+\"_real_A.png\"))\n",
        "y = imageio.imread(os.path.join(results_path_test,random_choice_no_extension[0]+\"_fake_B.png\"))\n",
        "\n",
        "f=plt.figure(figsize=(16,8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(x, interpolation='nearest')\n",
        "plt.title('Input')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y, interpolation='nearest')\n",
        "plt.title('Prediction')\n",
        "plt.axis('off');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvkd66PldsXB"
      },
      "source": [
        "## **6.3. Download your predictions**\n",
        "---\n",
        "\n",
        "<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD0yZaIhUhth"
      },
      "source": [
        "# **7. Version log**\n",
        "---\n",
        "\n",
        "<font size = 4>**v1.17.6**:\n",
        "\n",
        "*   Update ``git clone`` to ensure specific commit is used.\n",
        "\n",
        "<font size = 4>**v1.17.5**:\n",
        "\n",
        "*  Remove `tifffile` import.\n",
        "\n",
        "<font size = 4>**v1.17.4**:\n",
        "\n",
        "*   Update README.md for the BioImageIO model.\n",
        "\n",
        "\n",
        "<font size = 4>**v1.17.3**:\n",
        "\n",
        "*   Change LICENSE code cell for a documentation cell.\n",
        "\n",
        "<font size = 4>**v1.17.1**:\n",
        "* Identify correct patch/input size for the inference on new data\n",
        "\n",
        "<font size = 4>**v1.17.0**:\n",
        "* Normalisation process corrected for Adaptive Equalisation. Results from models trained previously may differ when compared to this version.\n",
        "\n",
        "<font size = 4>**v1.16.4**:\n",
        "* Corrects data type and range in the bioimageio specs.\n",
        "* Corrects for RGB data quality control SSIM.\n",
        "\n",
        "<font size = 4>**v1.16.3**:\n",
        "* Import the `bioimageio.core` library and export bioimage model zoo compatible models.\n",
        "\n",
        "<font size = 4>**v1.16.2**:\n",
        "* Loads data safely without temporary files starting with \".\"\n",
        "\n",
        "<font size = 4>**v1.16.1**:\n",
        "* Removes references to ?/content/'\n",
        "* Uses predefined functions for the new images to make the code more readable\n",
        "\n",
        "\n",
        "<font size = 4>**v1.15.1**:\n",
        "* Many bug fixes by **Johanna Rahm**\n",
        "* Number of channels\n",
        "\n",
        "the number of channels for training/inferring with pix2pix is set to 3 by default. When using the notebook with grayscale images (=1 channel), color is predicted (see attached image). Setting the number of channels to 1 for grayscale data fixes this problem.\n",
        "\n",
        "* Dataset mode\n",
        "\n",
        "the default dataset_mode is \"unaligned\", which matches cyclegan network with non-matching source and target images.\n",
        "\n",
        "Added --input_nc --output_nc and --dataset_mode flags to all training and prediction calls. The number of channels is set by the user by selecting grayscale or rgb from a dropdown menu in training, qc, and prediction.\n",
        "\n",
        "* Image loading and normalisation fixes\n",
        "\n",
        "Images used to be loaded with tifffile.imread(). Now it is changed to imageio.imread(), and added a description about the normalization parameters. Furthermore, in the training and qc section contrast streching was applied even though adaptive equalization was chosen, which is also fixed here.\n",
        "\n",
        "* PDF librairy changed from fpdf to fpdf2\n",
        "\n",
        "\n",
        "\n",
        "<font size = 4>**v1.13**:\n",
        "\n",
        "\n",
        "\n",
        "*  The section 1 and 2 are now swapped for better export of *requirements.txt*.\n",
        "This version also now includes built-in version check and the version log that\n",
        "\n",
        "*   This version also now includes built-in version check and the version log that you're reading now.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvSlTaH14s3t"
      },
      "source": [
        "\n",
        "# **Thank you for using pix2pix!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
