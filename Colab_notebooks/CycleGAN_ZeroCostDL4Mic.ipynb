{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CycleGAN_ZeroCostDL4Mic.ipynb","provenance":[{"file_id":"1V02Qd1PuJ2RECkl24136fhLizyX2KkrL","timestamp":1602673365922},{"file_id":"1mqcexfPBaIWuvMWWbJZUFtPoZoJJwrEA","timestamp":1589278334507},{"file_id":"159ARwlQE7-zi0EHxunOF_YPFLt-ZVU5x","timestamp":1587562499898},{"file_id":"1W-7NHehG5MRFILvZZzhPWWnOdJMkadb2","timestamp":1586332290412},{"file_id":"1pUetEQICxYWkYVaQIgdRH1EZBTl7oc2A","timestamp":1586292199692},{"file_id":"1MD36ZkM6XR9EuV12zimJmfCjzyeYZFWq","timestamp":1586269469061},{"file_id":"16A2mbaHzlEElntS8qkFBOsBvZG-mUeY6","timestamp":1586253795726},{"file_id":"1gJlcjOiSxr2buDOxmcFbT_d-GqwLjXtK","timestamp":1583343225796},{"file_id":"10yGI51WzHfgWgZAyE-EbkZFEvIOd6CP6","timestamp":1583171396283}],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"V9zNGvape2-I"},"source":["# **CycleGAN**\n","\n","---\n","\n","<font size = 4>CycleGAN is a method that can capture the characteristics of one image domain and learn how these characteristics can be translated into another image domain, all in the absence of any paired training examples. It was first published by [Zhu *et al.* in 2017](https://arxiv.org/abs/1703.10593). Unlike pix2pix, the image transformation performed does not require paired images for training (unsupervised learning) and is made possible here by using a set of two Generative Adversarial Networks (GANs) that learn to transform images both from the first domain to the second and vice-versa.\n","\n","<font size = 4> **This particular notebook enables unpaired image-to-image translation. If your dataset is paired, you should also consider using the pix2pix notebook.**\n","\n","---\n","\n","<font size = 4>*Disclaimer*:\n","\n","<font size = 4>This notebook is part of the *Zero-Cost Deep-Learning to Enhance Microscopy* project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories.\n","\n","<font size = 4>This notebook is based on the following paper: \n","\n","<font size = 4> **Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks** from Zhu *et al.* published in arXiv in 2018 (https://arxiv.org/abs/1703.10593)\n","\n","<font size = 4>The source code of the CycleGAN PyTorch implementation  can be found in: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n","\n","<font size = 4>**Please also cite this original paper when using or developing this notebook.**"]},{"cell_type":"markdown","metadata":{"id":"N3azwKB9O0oW"},"source":["# **License**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"ByW6Vqdn9sYV","cellView":"form"},"source":["#@markdown ##Double click to see the license information\n","\n","#------------------------- LICENSE FOR ZeroCostDL4Mic------------------------------------\n","#This ZeroCostDL4Mic notebook is distributed under the MIT licence\n","\n","\n","\n","#------------------------- LICENSE FOR CycleGAN ------------------------------------\n","\n","#Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\n","#All rights reserved.\n","\n","#Redistribution and use in source and binary forms, with or without\n","#modification, are permitted provided that the following conditions are met:\n","\n","#* Redistributions of source code must retain the above copyright notice, this\n","#  list of conditions and the following disclaimer.\n","\n","#* Redistributions in binary form must reproduce the above copyright notice,\n","#  this list of conditions and the following disclaimer in the documentation\n","#  and/or other materials provided with the distribution.\n","\n","#THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n","#AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n","#IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n","#DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n","#FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n","#DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n","#SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n","#CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n","#OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n","#OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","\n","\n","#--------------------------- LICENSE FOR pix2pix --------------------------------\n","#BSD License\n","\n","#For pix2pix software\n","#Copyright (c) 2016, Phillip Isola and Jun-Yan Zhu\n","#All rights reserved.\n","\n","#Redistribution and use in source and binary forms, with or without\n","#modification, are permitted provided that the following conditions are met:\n","\n","#* Redistributions of source code must retain the above copyright notice, this\n","#  list of conditions and the following disclaimer.\n","\n","#* Redistributions in binary form must reproduce the above copyright notice,\n","#  this list of conditions and the following disclaimer in the documentation\n","#  and/or other materials provided with the distribution.\n","\n","#----------------------------- LICENSE FOR DCGAN --------------------------------\n","#BSD License\n","\n","#For dcgan.torch software\n","\n","#Copyright (c) 2015, Facebook, Inc. All rights reserved.\n","\n","#Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n","\n","#Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n","\n","#Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n","\n","#Neither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n","\n","#THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWAz2i7RdxUV"},"source":["# **How to use this notebook?**\n","\n","---\n","\n","<font size = 4>Video describing how to use our notebooks are available on youtube:\n","  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n","  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n","\n","\n","---\n","###**Structure of a notebook**\n","\n","<font size = 4>The notebook contains two types of cell:  \n","\n","<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n","\n","<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n","\n","---\n","###**Table of contents, Code snippets** and **Files**\n","\n","<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n","\n","<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n","\n","<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n","\n","<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here. \n","\n","<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n","\n","<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n","\n","---\n","###**Making changes to the notebook**\n","\n","<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n","\n","<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n","You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."]},{"cell_type":"markdown","metadata":{"id":"vNMDQHm0Ah-Z"},"source":["#**0. Before getting started**\n","---\n","<font size = 4> To train CycleGAN, **you only need two folders containing PNG images**. The images do not need to be paired.\n","\n","<font size = 4>While you do not need paired images to train CycleGAN, if possible, **we strongly recommend that you generate  a paired dataset. This means that the same image needs to be acquired in the two conditions. These images can be used to assess the quality of your trained model (Quality control dataset)**. The quality control assessment can be done directly in this notebook.\n","\n","\n","<font size = 4> Please note that you currently can **only use .png files!**\n","\n","\n","<font size = 4>Here's a common data structure that can work:\n","*   Experiment A\n","    - **Training dataset (non-matching images) **\n","      - Training_source\n","        - img_1.png, img_2.png, ...\n","      - Training_target\n","        - img_1.png, img_2.png, ...\n","    - **Quality control dataset (matching images)**\n","     - Training_source\n","        - img_1.png, img_2.png\n","      - Training_target\n","        - img_1.png, img_2.png\n","    - **Data to be predicted**\n","    - **Results**\n","\n","---\n","<font size = 4>**Important note**\n","\n","<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to run predictions using the model that you trained.\n","\n","<font size = 4>- If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n","\n","<font size = 4>- If you only wish to **run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n","---"]},{"cell_type":"markdown","metadata":{"id":"DMNHVZfHmbKb"},"source":["# **1. Initialise the Colab session**\n","---\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BCPhV-pe-syw"},"source":["\n","## **1.1. Check for GPU access**\n","---\n","\n","By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n","\n","<font size = 4>Go to **Runtime -> Change the Runtime type**\n","\n","<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n","\n","<font size = 4>**Accelator: GPU** *(Graphics processing unit)*\n"]},{"cell_type":"code","metadata":{"id":"VNZetvLiS1qV","cellView":"form"},"source":["#@markdown ##Run this cell to check if you have GPU access\n","\n","\n","import tensorflow as tf\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBrnApIUBgxv"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. \n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","metadata":{"id":"01Djr8v-5pPk","cellView":"form"},"source":["#@markdown ##Run this cell to connect your Google Drive to Colab\n","\n","#@markdown * Click on the URL. \n","\n","#@markdown * Sign in your Google Account. \n","\n","#@markdown * Copy the authorization code. \n","\n","#@markdown * Enter the authorization code. \n","\n","#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n","\n","#mounts user's Google Drive to Google Colab.\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4yWFoJNnoin"},"source":["# **2. Install CycleGAN and dependencies**\n","---\n"]},{"cell_type":"code","metadata":{"id":"3u2mXn3XsWzd","cellView":"form"},"source":["Notebook_version = ['1.11']\n","\n","\n","\n","#@markdown ##Install CycleGAN and dependencies\n","\n","\n","#------- Code from the cycleGAN demo notebook starts here -------\n","\n","#Here, we install libraries which are not already included in Colab.\n","\n","\n","\n","!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n","\n","import os\n","os.chdir('pytorch-CycleGAN-and-pix2pix/')\n","!pip install -r requirements.txt\n","!pip install fpdf\n","\n","import imageio\n","from skimage import data\n","from skimage import exposure\n","from skimage.exposure import match_histograms\n","\n","from skimage.util import img_as_int\n","\n","\n","\n","\n","# ------- Common variable to all ZeroCostDL4Mic notebooks -------\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import urllib\n","import os, random\n","import shutil \n","import zipfile\n","from tifffile import imread, imsave\n","import time\n","import sys\n","from pathlib import Path\n","import pandas as pd\n","import csv\n","from glob import glob\n","from scipy import signal\n","from scipy import ndimage\n","from skimage import io\n","from sklearn.linear_model import LinearRegression\n","from skimage.util import img_as_uint\n","import matplotlib as mpl\n","from skimage.metrics import structural_similarity\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from astropy.visualization import simple_norm\n","from skimage import img_as_float32\n","from skimage.util import img_as_ubyte\n","from tqdm import tqdm \n","from fpdf import FPDF, HTMLMixin\n","from datetime import datetime\n","from pip._internal.operations.freeze import freeze\n","import subprocess\n","\n","# Colors for the warning messages\n","class bcolors:\n","  WARNING = '\\033[31m'\n","\n","#Disable some of the tensorflow warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print(\"Libraries installed\")\n","\n","# Check if this is the latest version of the notebook\n","Latest_notebook_version = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_ZeroCostDL4Mic_Release.csv\")\n","\n","if Notebook_version == list(Latest_notebook_version.columns):\n","  print(\"This notebook is up-to-date.\")\n","\n","if not Notebook_version == list(Latest_notebook_version.columns):\n","  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n","\n","!pip freeze > requirements.txt\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fw0kkTU6CsU4"},"source":["# **3. Select your parameters and paths**\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"BLmBseWbRvxL"},"source":["## **3.1. Setting main training parameters**\n","---\n","<font size = 4>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CB6acvUFtWqd"},"source":["<font size = 5> **Paths for training, predictions and results**\n","\n","<font size = 4>**`Training_source:`, `Training_target`:** These are the paths to your folders containing the Training_source and Training_target training data respecively. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n","\n","<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n","\n","<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n","\n","<font size = 5>**Training Parameters**\n","\n","<font size = 4>**`number_of_epochs`:**Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10) epochs, but a full training should run for 200 epochs or more. Evaluate the performance after training (see 5). **Default value: 200**\n","\n","\n","<font size = 5>**Advanced Parameters - experienced users only**\n","\n","<font size = 4>**`patch_size`:** CycleGAN divides the image into patches for training. Input the size of the patches (length of a side). The value should be smaller than the dimensions of the image and divisible by 4. **Default value: 512**\n","\n","<font size = 4>**When choosing the patch_size, the value should be i) large enough that it will enclose many instances, ii) small enough that the resulting patches fit into the RAM.**<font size = 4> \n","\n","<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 1**\n","\n","<font size = 4>**`initial_learning_rate`:** Input the initial value to be used as learning rate. **Default value: 0.0002**"]},{"cell_type":"code","metadata":{"id":"pIrTwJjzwV-D","cellView":"form"},"source":["\n","\n","#@markdown ###Path to training images:\n","\n","Training_source = \"\" #@param {type:\"string\"}\n","InputFile = Training_source+\"/*.png\"\n","\n","Training_target = \"\" #@param {type:\"string\"}\n","OutputFile = Training_target+\"/*.png\"\n","\n","\n","#Define where the patch file will be saved\n","base = \"/content\"\n","\n","\n","# model name and path\n","#@markdown ###Name of the model and path to model folder:\n","model_name = \"\" #@param {type:\"string\"}\n","model_path = \"\" #@param {type:\"string\"}\n","\n","# other parameters for training.\n","#@markdown ###Training Parameters\n","#@markdown Number of epochs:\n","number_of_epochs =  200#@param {type:\"number\"}\n","\n","#@markdown ###Advanced Parameters\n","\n","Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n","#@markdown ###If not, please input:\n","patch_size =  512#@param {type:\"number\"} # in pixels\n","batch_size =  1#@param {type:\"number\"}\n","initial_learning_rate = 0.0002 #@param {type:\"number\"}\n","\n","\n","if (Use_Default_Advanced_Parameters): \n","  print(\"Default advanced parameters enabled\")\n","  batch_size = 1\n","  patch_size =  512\n","  initial_learning_rate = 0.0002\n","\n","#here we check that no model with the same name already exist, if so delete\n","if os.path.exists(model_path+'/'+model_name):\n","  print(bcolors.WARNING +\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n","  print(bcolors.WARNING +\"To continue training \"+model_name+\", choose a new model_name here, and load \"+model_name+\" in section 3.3\")\n","  \n","\n","\n","#To use Cyclegan we need to organise the data in a way the model can understand\n","\n","Saving_path= \"/content/\"+model_name\n","#Saving_path= model_path+\"/\"+model_name\n","\n","if os.path.exists(Saving_path):\n","  shutil.rmtree(Saving_path)\n","os.makedirs(Saving_path)\n","\n","TrainA_Folder = Saving_path+\"/trainA\"\n","if os.path.exists(TrainA_Folder):\n","  shutil.rmtree(TrainA_Folder)\n","os.makedirs(TrainA_Folder)\n","  \n","TrainB_Folder = Saving_path+\"/trainB\"\n","if os.path.exists(TrainB_Folder):\n","  shutil.rmtree(TrainB_Folder)\n","os.makedirs(TrainB_Folder)\n","\n","# Here we disable pre-trained model by default (in case the  cell is not ran)\n","Use_pretrained_model = False\n","\n","# Here we disable data augmentation by default (in case the cell is not ran)\n","\n","Use_Data_augmentation = True\n","\n","\n","# This will display a randomly chosen dataset input and output\n","random_choice = random.choice(os.listdir(Training_source))\n","x = imageio.imread(Training_source+\"/\"+random_choice)\n","\n","\n","#Find image XY dimension\n","Image_Y = x.shape[0]\n","Image_X = x.shape[1]\n","\n","Image_min_dim = min(Image_Y, Image_X)\n","\n","\n","\n","#Hyperparameters failsafes\n","if patch_size > min(Image_Y, Image_X):\n","  patch_size = min(Image_Y, Image_X)\n","  print (bcolors.WARNING + \" Your chosen patch_size is bigger than the xy dimension of your image; therefore the patch_size chosen is now:\",patch_size)\n","\n","# Here we check that patch_size is divisible by 4\n","if not patch_size % 4 == 0:\n","    patch_size = ((int(patch_size / 4)-1) * 4)\n","    print (bcolors.WARNING + \" Your chosen patch_size is not divisible by 4; therefore the patch_size chosen is now:\",patch_size)\n","\n","\n","random_choice_2 = random.choice(os.listdir(Training_target))\n","y = imageio.imread(Training_target+\"/\"+random_choice_2)\n","\n","f=plt.figure(figsize=(16,8))\n","plt.subplot(1,2,1)\n","plt.imshow(x, interpolation='nearest')\n","plt.title('Training source')\n","plt.axis('off');\n","\n","plt.subplot(1,2,2)\n","plt.imshow(y, interpolation='nearest')\n","plt.title('Training target')\n","plt.axis('off');\n","plt.savefig('/content/TrainingDataExample_cycleGAN.png',bbox_inches='tight',pad_inches=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FX6uxFvI-CsQ"},"source":["## **3.2. Data augmentation**\n","---\n","<font size = 4>\n"]},{"cell_type":"markdown","metadata":{"id":"CwMaFU1T-GtN"},"source":["<font size = 4>Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation is not necessary for training and if your training dataset is large you should disable it.\n","\n","<font size = 4>Data augmentation is performed here by flipping the patches. \n","\n","<font size = 4> By default data augmentation is enabled."]},{"cell_type":"code","metadata":{"id":"kLtHIATT-0un","cellView":"form"},"source":["#Data augmentation\n","\n","#@markdown ##Play this cell to enable or disable data augmentation: \n","\n","Use_Data_augmentation = True #@param {type:\"boolean\"}\n","\n","if Use_Data_augmentation:\n","  print(\"Data augmentation enabled\")\n","\n","if not Use_Data_augmentation:\n","  print(\"Data augmentation disabled\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-leE8pEWRkn"},"source":["\n","## **3.3. Using weights from a pre-trained model as initial weights**\n","---\n","<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a CycleGAN model**. \n","\n","<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**.\n","\n","<font size = 4> In order to continue training from the point where the pre-trained model left off, it is adviseable to also **load the learning rate** that was used when the training ended. This is automatically saved for models trained with ZeroCostDL4Mic and will be loaded here. If no learning rate can be found in the model folder provided, the default learning rate will be used. "]},{"cell_type":"code","metadata":{"id":"CbOcS3wiWV9w","cellView":"form"},"source":["# @markdown ##Loading weights from a pre-trained network\n","\n","\n","Use_pretrained_model = False #@param {type:\"boolean\"}\n","\n","\n","#@markdown ###If not, please provide the path to the model folder:\n","pretrained_model_path = \"\" #@param {type:\"string\"}\n","\n","# --------------------- Check if we load a previously trained model ------------------------\n","if Use_pretrained_model:\n","\n","  h5_file_path_A = os.path.join(pretrained_model_path, \"latest_net_G_A.pth\")\n","  h5_file_path_B = os.path.join(pretrained_model_path, \"latest_net_G_B.pth\")\n","\n","# --------------------- Check the model exist ------------------------\n","\n","  if not os.path.exists(h5_file_path_A) and os.path.exists(h5_file_path_B):\n","    print(bcolors.WARNING+'WARNING: Pretrained model does not exist')\n","    Use_pretrained_model = False\n","    print(bcolors.WARNING+'No pretrained network will be used.')\n","\n","  if os.path.exists(h5_file_path_A) and os.path.exists(h5_file_path_B):\n","    print(\"Pretrained model \"+os.path.basename(pretrained_model_path)+\" was found and will be loaded prior to training.\")\n","    \n","else:\n","  print(bcolors.WARNING+'No pretrained network will be used.')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQndJj70FzfL"},"source":["# **4. Train the network**\n","---"]},{"cell_type":"markdown","metadata":{"id":"-A4ipz8gs3Ew"},"source":["## **4.1. Prepare the training data for training**\n","---\n","<font size = 4>Here, we use the information from 3. to prepare the training data into a suitable format for training."]},{"cell_type":"code","metadata":{"id":"_V2ujGB60gDv","cellView":"form"},"source":["#@markdown ##Prepare the data for training\n","\n","print(\"Data preparation in progress\")\n","\n","if os.path.exists(model_path+'/'+model_name):\n","  shutil.rmtree(model_path+'/'+model_name)\n","os.makedirs(model_path+'/'+model_name)\n","\n","#--------------- Here we move the files to trainA and train B ---------\n","\n","\n","for f in os.listdir(Training_source):\n","  shutil.copyfile(Training_source+\"/\"+f, TrainA_Folder+\"/\"+f)\n","\n","for files in os.listdir(Training_target):\n","  shutil.copyfile(Training_target+\"/\"+files, TrainB_Folder+\"/\"+files)\n","\n","#---------------------------------------------------------------------\n","\n","# CycleGAN use number of EPOCH withouth lr decay and number of EPOCH with lr decay\n","\n","\n","number_of_epochs_lr_stable = int(number_of_epochs/2)\n","number_of_epochs_lr_decay = int(number_of_epochs/2)\n","\n","if Use_pretrained_model :\n","  for f in os.listdir(pretrained_model_path):\n","    if (f.startswith(\"latest_net_\")):      \n","      shutil.copyfile(pretrained_model_path+\"/\"+f, model_path+'/'+model_name+\"/\"+f)\n","\n","print(\"Data ready for training\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQPz0F6JlvJR"},"source":["## **4.2. Start Training**\n","---\n","<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n","\n","<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches or continue the training in a second Colab session."]},{"cell_type":"code","metadata":{"id":"eBD50tAgv5qf","cellView":"form"},"source":["\n","#@markdown ##Start training\n","\n","start = time.time()\n","\n","os.chdir(\"/content\")\n","\n","#--------------------------------- Command line inputs to change CycleGAN paramaters------------\n","\n","       # basic parameters\n","        #('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n","        #('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n","        #('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n","        #('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n","       \n","       # model parameters\n","        #('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n","        #('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n","        #('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n","        #('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n","        #('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n","        #('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n","        #('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n","        #('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n","        #('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n","        #('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n","        #('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n","        #('--no_dropout', action='store_true', help='no dropout for the generator')\n","        \n","       # dataset parameters\n","        #('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n","        #('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n","        #('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n","        #('--num_threads', default=4, type=int, help='# threads for loading data')\n","        #('--batch_size', type=int, default=1, help='input batch size')\n","        #('--load_size', type=int, default=286, help='scale images to this size')\n","        #('--crop_size', type=int, default=256, help='then crop to this size')\n","        #('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n","        #('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n","        #('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n","        #('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n","        \n","       # additional parameters\n","        #('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n","        #('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n","        #('--verbose', action='store_true', help='if specified, print more debugging information')\n","        #('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n","        \n","       # visdom and HTML visualization parameters\n","        #('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n","        #('--display_ncols', type=int, default=4, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n","        #('--display_id', type=int, default=1, help='window id of the web display')\n","        #('--display_server', type=str, default=\"http://localhost\", help='visdom server of the web display')\n","        #('--display_env', type=str, default='main', help='visdom display environment name (default is \"main\")')\n","        #('--display_port', type=int, default=8097, help='visdom port of the web display')\n","        #('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n","        #('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n","        #('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n","        \n","       # network saving and loading parameters\n","        #('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n","        #('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n","        #('--save_by_iter', action='store_true', help='whether saves model by iteration')\n","        #('--continue_train', action='store_true', help='continue training: load the latest model')\n","        #('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n","        #('--phase', type=str, default='train', help='train, val, test, etc')\n","        \n","       # training parameters\n","        #('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')\n","        #('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')\n","        #('--beta1', type=float, default=0.5, help='momentum term of adam')\n","        #('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n","        #('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n","        #('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n","        #('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')\n","        #('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations'\n","\n","#---------------------------------------------------------\n","\n","#----- Start the training ------------------------------------\n","if not Use_pretrained_model:\n","  if Use_Data_augmentation:\n","    !python pytorch-CycleGAN-and-pix2pix/train.py --dataroot \"$Saving_path\" --input_nc 3  --name $model_name --model cycle_gan --batch_size $batch_size --preprocess scale_width_and_crop --load_size $Image_min_dim --crop_size $patch_size --checkpoints_dir \"$model_path\"  --no_html --n_epochs $number_of_epochs_lr_stable --n_epochs_decay $number_of_epochs_lr_decay --lr $initial_learning_rate --display_id 0 --save_epoch_freq 5\n","  if not Use_Data_augmentation:\n","    !python pytorch-CycleGAN-and-pix2pix/train.py --dataroot \"$Saving_path\" --input_nc 3 --name $model_name --model cycle_gan --batch_size $batch_size --preprocess scale_width_and_crop --load_size $Image_min_dim --crop_size $patch_size --checkpoints_dir \"$model_path\"  --no_html --n_epochs $number_of_epochs_lr_stable --n_epochs_decay $number_of_epochs_lr_decay --lr $initial_learning_rate --display_id 0 --save_epoch_freq 5 --no_flip\n","\n","if Use_pretrained_model:\n","  if Use_Data_augmentation:\n","    !python pytorch-CycleGAN-and-pix2pix/train.py --dataroot \"$Saving_path\" --input_nc 3 --name $model_name --model cycle_gan --batch_size $batch_size --preprocess scale_width_and_crop --load_size $Image_min_dim --crop_size $patch_size --checkpoints_dir \"$model_path\"  --no_html --n_epochs $number_of_epochs_lr_stable --n_epochs_decay $number_of_epochs_lr_decay --lr $initial_learning_rate --display_id 0 --save_epoch_freq 5 --continue_train\n","  \n","  if not Use_Data_augmentation:\n","    !python pytorch-CycleGAN-and-pix2pix/train.py --dataroot \"$Saving_path\" --input_nc 3 --name $model_name --model cycle_gan --batch_size $batch_size --preprocess scale_width_and_crop --load_size $Image_min_dim --crop_size $patch_size --checkpoints_dir \"$model_path\"  --no_html --n_epochs $number_of_epochs_lr_stable --n_epochs_decay $number_of_epochs_lr_decay --lr $initial_learning_rate --display_id 0 --save_epoch_freq 5 --continue_train --no_flip\n","\n","#---------------------------------------------------------\n","\n","print(\"Training, done.\")\n","\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","mins, sec = divmod(dt, 60) \n","hour, mins = divmod(mins, 60) \n","print(\"Time elapsed:\",hour, \"hour(s)\",mins,\"min(s)\",round(sec),\"sec(s)\")\n","\n","# save FPDF() class into a  \n","# variable pdf \n","from datetime import datetime\n","\n","class MyFPDF(FPDF, HTMLMixin):\n","    pass\n","\n","pdf = MyFPDF()\n","pdf.add_page()\n","pdf.set_right_margin(-1)\n","pdf.set_font(\"Arial\", size = 11, style='B') \n","\n","Network = 'cycleGAN'\n","day = datetime.now()\n","datetime_str = str(day)[0:10]\n","\n","Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n","pdf.multi_cell(180, 5, txt = Header, align = 'L') \n","  \n","# add another cell \n","training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n","pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n","pdf.ln(1)\n","\n","Header_2 = 'Information for your materials and method:'\n","pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n","\n","all_packages = ''\n","for requirement in freeze(local_only=True):\n","  all_packages = all_packages+requirement+', '\n","#print(all_packages)\n","\n","#Main Packages\n","main_packages = ''\n","version_numbers = []\n","for name in ['tensorflow','numpy','torch']:\n","  find_name=all_packages.find(name)\n","  main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n","  #Version numbers only here:\n","  version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n","\n","cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n","cuda_version = cuda_version.stdout.decode('utf-8')\n","cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n","gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n","gpu_name = gpu_name.stdout.decode('utf-8')\n","gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n","#print(cuda_version[cuda_version.find(', V')+3:-1])\n","#print(gpu_name)\n","\n","shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n","dataset_size = len(os.listdir(Training_source))\n","\n","text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(dataset_size)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+' and a least-square GAN loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), torch (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n","\n","if Use_pretrained_model:\n","  text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(dataset_size)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+' and an least-square GAN loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was retrained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), torch (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n","\n","pdf.set_font('')\n","pdf.set_font_size(10.)\n","pdf.multi_cell(190, 5, txt = text, align='L')\n","pdf.set_font('')\n","pdf.set_font('Arial', size = 10, style = 'B')\n","pdf.ln(1)\n","pdf.cell(28, 5, txt='Augmentation: ', ln=0)\n","pdf.set_font('')\n","if Use_Data_augmentation:\n","  aug_text = 'The dataset was augmented by default'\n","else:\n","  aug_text = 'No augmentation was used for training.'\n","pdf.multi_cell(190, 5, txt=aug_text, align='L')\n","pdf.set_font('Arial', size = 11, style = 'B')\n","pdf.ln(1)\n","pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n","pdf.set_font('')\n","pdf.set_font_size(10.)\n","if Use_Default_Advanced_Parameters:\n","  pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n","pdf.cell(200, 5, txt='The following parameters were used for training:')\n","pdf.ln(1)\n","html = \"\"\" \n","<table width=40% style=\"margin-left:0px;\">\n","  <tr>\n","    <th width = 50% align=\"left\">Parameter</th>\n","    <th width = 50% align=\"left\">Value</th>\n","  </tr>\n","  <tr>\n","    <td width = 50%>number_of_epochs</td>\n","    <td width = 50%>{0}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>patch_size</td>\n","    <td width = 50%>{1}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>batch_size</td>\n","    <td width = 50%>{2}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>initial_learning_rate</td>\n","    <td width = 50%>{3}</td>\n","  </tr>\n","</table>\n","\"\"\".format(number_of_epochs,str(patch_size)+'x'+str(patch_size),batch_size,initial_learning_rate)\n","pdf.write_html(html)\n","\n","#pdf.multi_cell(190, 5, txt = text_2, align='L')\n","pdf.set_font(\"Arial\", size = 11, style='B')\n","pdf.ln(1)\n","pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n","pdf.set_font('')\n","pdf.set_font('Arial', size = 10, style = 'B')\n","pdf.cell(30, 5, txt= 'Training_source:', align = 'L', ln=0)\n","pdf.set_font('')\n","pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n","pdf.set_font('')\n","pdf.set_font('Arial', size = 10, style = 'B')\n","pdf.cell(29, 5, txt= 'Training_target:', align = 'L', ln=0)\n","pdf.set_font('')\n","pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n","#pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n","pdf.ln(1)\n","pdf.set_font('')\n","pdf.set_font('Arial', size = 10, style = 'B')\n","pdf.cell(22, 5, txt= 'Model Path:', align = 'L', ln=0)\n","pdf.set_font('')\n","pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n","pdf.ln(1)\n","pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n","pdf.ln(1)\n","exp_size = io.imread('/content/TrainingDataExample_cycleGAN.png').shape\n","pdf.image('/content/TrainingDataExample_cycleGAN.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n","pdf.ln(1)\n","ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy.\" BioRxiv (2020).'\n","pdf.multi_cell(190, 5, txt = ref_1, align='L')\n","ref_2 = '- cycleGAN: Zhu, Jun-Yan, et al. \"Unpaired image-to-image translation using cycle-consistent adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017.'\n","pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","# if Use_Data_augmentation:\n","#   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n","#   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n","pdf.ln(3)\n","reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n","pdf.set_font('Arial', size = 11, style='B')\n","pdf.multi_cell(190, 5, txt=reminder, align='C')\n","\n","pdf.output(model_path+'/'+model_name+'/'+model_name+\"_training_report.pdf\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQjQb_J_Qyku"},"source":["##**4.3. Download your model(s) from Google Drive**\n","\n","\n","---\n","<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder as all data can be erased at the next training if using the same folder."]},{"cell_type":"markdown","metadata":{"id":"2HbZd7rFqAad"},"source":["# **5. Evaluate your model**\n","---\n","\n","<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model. \n","\n","<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n","\n","<font size = 4>Unfortunately loss functions curve are not very informative for GAN network. Therefore we perform the QC here using a test dataset.\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PhcOwcgH3JAD"},"source":["## **5.1. Choose the model you want to assess**"]},{"cell_type":"code","metadata":{"id":"EdcnkCr9Nbl8","cellView":"form"},"source":["# model name and path\n","#@markdown ###Do you want to assess the model you just trained ?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, please provide the path to the model folder:\n","\n","QC_model_folder = \"\" #@param {type:\"string\"}\n","\n","#Here we define the loaded model name and path\n","QC_model_name = os.path.basename(QC_model_folder)\n","QC_model_path = os.path.dirname(QC_model_folder)\n","\n","if (Use_the_current_trained_model): \n","  QC_model_name = model_name\n","  QC_model_path = model_path\n","\n","full_QC_model_path = QC_model_path+'/'+QC_model_name+'/'\n","if os.path.exists(full_QC_model_path):\n","  print(\"The \"+QC_model_name+\" network will be evaluated\")\n","else:\n","  W  = '\\033[0m'  # white (normal)\n","  R  = '\\033[31m' # red\n","  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","  print('Please make sure you provide a valid model path and model name before proceeding further.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E4Yp7ogh3NGD"},"source":["## **5.2. Identify the best checkpoint to use to make predictions**"]},{"cell_type":"markdown","metadata":{"id":"1yauWCc78HKD"},"source":["<font size = 4> CycleGAN save model checkpoints every five epochs. Due to the stochastic nature of GAN networks, the last checkpoint is not always the best one to use. As a consequence, it can be challenging to choose the most suitable checkpoint to use to make predictions.\n","\n","<font size = 4>This section allows you to perform predictions using all the saved checkpoints and to estimate the quality of these predictions by comparing them to the provided ground truths images. Metric used include:\n","\n","<font size = 4>**1. The SSIM (structural similarity) map** \n","\n","<font size = 4>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info). \n","\n","<font size=4>**mSSIM** is the SSIM value calculated across the entire window of both images.\n","\n","<font size=4>**The output below shows the SSIM maps with the mSSIM**\n","\n","<font size = 4>**2. The RSE (Root Squared Error) map** \n","\n","<font size = 4>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n","\n","\n","<font size =4>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n","\n","<font size = 4>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n","\n","<font size=4>**The output below shows the RSE maps with the NRMSE and PSNR values.**\n","\n"]},{"cell_type":"code","metadata":{"id":"2nBPucJdK3KS","cellView":"form"},"source":["#@markdown ##Choose the folders that contain your Quality Control dataset\n","\n","Source_QC_folder = \"\" #@param{type:\"string\"}\n","Target_QC_folder = \"\" #@param{type:\"string\"}\n","\n","Image_type = \"Grayscale\" #@param [\"Grayscale\", \"RGB\"]\n","\n","# average function\n","def Average(lst): \n","    return sum(lst) / len(lst) \n","\n","\n","# Create a quality control folder\n","\n","if os.path.exists(QC_model_path+\"/\"+QC_model_name+\"/Quality Control\"):\n","  shutil.rmtree(QC_model_path+\"/\"+QC_model_name+\"/Quality Control\")\n","\n","os.makedirs(QC_model_path+\"/\"+QC_model_name+\"/Quality Control\")\n","\n","# List images in Source_QC_folder\n","# This will find the image dimension of a randomly choosen image in Source_QC_folder \n","random_choice = random.choice(os.listdir(Source_QC_folder))\n","x = imageio.imread(Source_QC_folder+\"/\"+random_choice)\n","\n","#Find image XY dimension\n","Image_Y = x.shape[0]\n","Image_X = x.shape[1]\n","\n","Image_min_dim = min(Image_Y, Image_X)\n","\n","\n","# Here we need to move the data to be analysed so that cycleGAN can find them\n","\n","Saving_path_QC= \"/content/\"+QC_model_name\n","\n","if os.path.exists(Saving_path_QC):\n","  shutil.rmtree(Saving_path_QC)\n","os.makedirs(Saving_path_QC)\n","\n","Saving_path_QC_folder = Saving_path_QC+\"_images\"\n","\n","if os.path.exists(Saving_path_QC_folder):\n","  shutil.rmtree(Saving_path_QC_folder)\n","os.makedirs(Saving_path_QC_folder)\n","\n","\n","#Here we copy and rename the all the checkpoint to be analysed\n","\n","for f in os.listdir(full_QC_model_path):\n","  shortname = f[:-6]\n","  shortname = shortname + \".pth\"\n","  if f.endswith(\"net_G_A.pth\"):\n","    shutil.copyfile(full_QC_model_path+f, Saving_path_QC+\"/\"+shortname)\n","\n","\n","for files in os.listdir(Source_QC_folder):\n","  shutil.copyfile(Source_QC_folder+\"/\"+files, Saving_path_QC_folder+\"/\"+files)\n","  \n","\n","# This will find the image dimension of a randomly chosen image in Source_QC_folder \n","random_choice = random.choice(os.listdir(Source_QC_folder))\n","x = imageio.imread(Source_QC_folder+\"/\"+random_choice)\n","\n","#Find image XY dimension\n","Image_Y = x.shape[0]\n","Image_X = x.shape[1]\n","\n","Image_min_dim = int(min(Image_Y, Image_X))\n","\n","Nb_Checkpoint = len(os.listdir(Saving_path_QC))\n","\n","print(Nb_Checkpoint)\n","\n","\n","\n","## Initiate list\n","\n","Checkpoint_list = []\n","Average_ssim_score_list = []\n","\n","\n","for j in range(1, len(os.listdir(Saving_path_QC))+1):\n","  checkpoints = j*5\n","\n","  if checkpoints == Nb_Checkpoint*5:\n","    checkpoints = \"latest\"\n","\n","\n","  print(\"The checkpoint currently analysed is =\"+str(checkpoints))\n","\n","  Checkpoint_list.append(checkpoints)\n","\n","\n","  # Create a quality control/Prediction Folder\n","\n","  QC_prediction_results = QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)\n","\n","  if os.path.exists(QC_prediction_results):\n","    shutil.rmtree(QC_prediction_results)\n","\n","  os.makedirs(QC_prediction_results)\n","\n","\n","\n","#---------------------------- Predictions are performed here ----------------------\n","\n","  os.chdir(\"/content\")\n","\n","  !python pytorch-CycleGAN-and-pix2pix/test.py --dataroot \"$Saving_path_QC_folder\" --name \"$QC_model_name\" --model test --epoch $checkpoints --no_dropout --preprocess scale_width --load_size $Image_min_dim --crop_size $Image_min_dim --results_dir \"$QC_prediction_results\" --checkpoints_dir \"/content/\"\n","\n","#-----------------------------------------------------------------------------------\n","\n","#Here we need to move the data again and remove all the unnecessary folders\n","\n","  Checkpoint_name = \"test_\"+str(checkpoints)\n","\n","  QC_results_images = QC_prediction_results+\"/\"+QC_model_name+\"/\"+Checkpoint_name+\"/images\"\n","\n","  QC_results_images_files = os.listdir(QC_results_images)\n","\n","  for f in QC_results_images_files:  \n","    shutil.copyfile(QC_results_images+\"/\"+f, QC_prediction_results+\"/\"+f)\n","\n","  os.chdir(\"/content\")  \n","\n","  #Here we clean up the extra files\n","  shutil.rmtree(QC_prediction_results+\"/\"+QC_model_name)\n","\n","\n","#-------------------------------- QC for RGB ------------------------------------\n","  if Image_type == \"RGB\":\n","# List images in Source_QC_folder\n","# This will find the image dimension of a randomly choosen image in Source_QC_folder \n","    random_choice = random.choice(os.listdir(Source_QC_folder))\n","    x = imageio.imread(Source_QC_folder+\"/\"+random_choice)\n","\n","    def ssim(img1, img2):\n","      return structural_similarity(img1,img2,data_range=1.,full=True, multichannel=True)\n","\n","# Open and create the csv file that will contain all the QC metrics\n","    with open(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/\"+\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\", \"w\", newline='') as file:\n","        writer = csv.writer(file)\n","\n","    # Write the header in the csv file\n","        writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\"])\n","        \n","        \n","    # Initiate list\n","        ssim_score_list = []  \n","\n","\n","    # Let's loop through the provided dataset in the QC folders\n","\n","\n","        for i in os.listdir(Source_QC_folder):\n","          if not os.path.isdir(os.path.join(Source_QC_folder,i)):\n","            print('Running QC on: '+i)\n","\n","            shortname_no_PNG = i[:-4]\n","        \n","      # -------------------------------- Target test data (Ground truth) --------------------------------\n","            test_GT = imageio.imread(os.path.join(Target_QC_folder, i), as_gray=False, pilmode=\"RGB\")\n","\n","      # -------------------------------- Source test data --------------------------------\n","            test_source = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints),shortname_no_PNG+\"_real.png\"))\n","        \n","     \n","      # -------------------------------- Prediction --------------------------------\n","      \n","            test_prediction = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints),shortname_no_PNG+\"_fake.png\"))\n","          \n","          #--------------------------- Here we normalise using histograms matching--------------------------------\n","            test_prediction_matched = match_histograms(test_prediction, test_GT, multichannel=True)\n","            test_source_matched = match_histograms(test_source, test_GT, multichannel=True)\n","            \n","      # -------------------------------- Calculate the metric maps and save them --------------------------------\n","\n","      # Calculate the SSIM maps\n","            index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT, test_prediction_matched)\n","            index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT, test_source_matched)\n","\n","            ssim_score_list.append(index_SSIM_GTvsPrediction)\n","\n","      #Save ssim_maps\n","            img_SSIM_GTvsPrediction_8bit = (img_SSIM_GTvsPrediction* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/SSIM_GTvsPrediction_\"+shortname_no_PNG+'.tif',img_SSIM_GTvsPrediction_8bit)\n","            img_SSIM_GTvsSource_8bit = (img_SSIM_GTvsSource* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/SSIM_GTvsSource_\"+shortname_no_PNG+'.tif',img_SSIM_GTvsSource_8bit)\n","      \n","      \n","            writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource)])\n","\n","      #Here we calculate the ssim average for each image in each checkpoints\n","\n","        Average_SSIM_checkpoint = Average(ssim_score_list)\n","        Average_ssim_score_list.append(Average_SSIM_checkpoint)\n","\n","\n","\n","\n","#------------------------------------------- QC for Grayscale ----------------------------------------------\n","\n","  if Image_type == \"Grayscale\":\n","    def ssim(img1, img2):\n","      return structural_similarity(img1,img2,data_range=1.,full=True, gaussian_weights=True, use_sample_covariance=False, sigma=1.5)\n","\n","\n","    def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n","\n","\n","      mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n","      ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n","      return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n","\n","\n","    def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n","  \n","      if dtype is not None:\n","        x   = x.astype(dtype,copy=False)\n","        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n","        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n","        eps = dtype(eps)\n","\n","        try:\n","            import numexpr\n","            x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n","        except ImportError:\n","            x =                   (x - mi) / ( ma - mi + eps )\n","\n","        if clip:\n","            x = np.clip(x,0,1)\n","\n","        return x\n","\n","    def norm_minmse(gt, x, normalize_gt=True):\n","    \n","      if normalize_gt:\n","        gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n","        x = x.astype(np.float32, copy=False) - np.mean(x)\n","        #x = x - np.mean(x)\n","        gt = gt.astype(np.float32, copy=False) - np.mean(gt)\n","        #gt = gt - np.mean(gt)\n","        scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n","        return gt, scale * x\n","\n","# Open and create the csv file that will contain all the QC metrics\n","    with open(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/\"+\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\", \"w\", newline='') as file:\n","        writer = csv.writer(file)\n","\n","    # Write the header in the csv file\n","        writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\", \"Prediction v. GT NRMSE\", \"Input v. GT NRMSE\", \"Prediction v. GT PSNR\", \"Input v. GT PSNR\"])  \n","\n","      \n","    \n","    # Let's loop through the provided dataset in the QC folders\n","\n","\n","        for i in os.listdir(Source_QC_folder):\n","          if not os.path.isdir(os.path.join(Source_QC_folder,i)):\n","            print('Running QC on: '+i)\n","\n","            ssim_score_list = []\n","            shortname_no_PNG = i[:-4]\n","      # -------------------------------- Target test data (Ground truth) --------------------------------\n","            test_GT_raw = imageio.imread(os.path.join(Target_QC_folder, i), as_gray=False, pilmode=\"RGB\")\n","          \n","            test_GT = test_GT_raw[:,:,2]\n","\n","      # -------------------------------- Source test data --------------------------------\n","            test_source_raw = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints),shortname_no_PNG+\"_real.png\"))\n","          \n","            test_source = test_source_raw[:,:,2]\n","\n","      # Normalize the images wrt each other by minimizing the MSE between GT and Source image\n","            test_GT_norm,test_source_norm = norm_minmse(test_GT, test_source, normalize_gt=True)\n","\n","      # -------------------------------- Prediction --------------------------------\n","            test_prediction_raw = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints),shortname_no_PNG+\"_fake.png\"))\n","          \n","            test_prediction = test_prediction_raw[:,:,2]\n","\n","      # Normalize the images wrt each other by minimizing the MSE between GT and prediction\n","            test_GT_norm,test_prediction_norm = norm_minmse(test_GT, test_prediction, normalize_gt=True)        \n","\n","\n","      # -------------------------------- Calculate the metric maps and save them --------------------------------\n","\n","      # Calculate the SSIM maps\n","            index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT_norm, test_prediction_norm)\n","            index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT_norm, test_source_norm)\n","\n","            ssim_score_list.append(index_SSIM_GTvsPrediction)\n","\n","      #Save ssim_maps\n","          \n","            img_SSIM_GTvsPrediction_8bit = (img_SSIM_GTvsPrediction* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/SSIM_GTvsPrediction_\"+shortname_no_PNG+'.tif',img_SSIM_GTvsPrediction_8bit)\n","            img_SSIM_GTvsSource_8bit = (img_SSIM_GTvsSource* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/SSIM_GTvsSource_\"+shortname_no_PNG+'.tif',img_SSIM_GTvsSource_8bit)\n","      \n","      # Calculate the Root Squared Error (RSE) maps\n","            img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n","            img_RSE_GTvsSource = np.sqrt(np.square(test_GT_norm - test_source_norm))\n","\n","      # Save SE maps\n","            img_RSE_GTvsPrediction_8bit = (img_RSE_GTvsPrediction* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/RSE_GTvsPrediction_\"+shortname_no_PNG+'.tif',img_RSE_GTvsPrediction_8bit)\n","            img_RSE_GTvsSource_8bit = (img_RSE_GTvsSource* 255).astype(\"uint8\")\n","            io.imsave(QC_model_path+'/'+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/RSE_GTvsSource_\"+shortname_no_PNG+'.tif',img_RSE_GTvsSource_8bit)\n","\n","\n","      # -------------------------------- Calculate the RSE metrics and save them --------------------------------\n","\n","      # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n","            NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n","            NRMSE_GTvsSource = np.sqrt(np.mean(img_RSE_GTvsSource))\n","        \n","      # We can also measure the peak signal to noise ratio between the images\n","            PSNR_GTvsPrediction = psnr(test_GT_norm,test_prediction_norm,data_range=1.0)\n","            PSNR_GTvsSource = psnr(test_GT_norm,test_source_norm,data_range=1.0)\n","\n","            writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource),str(NRMSE_GTvsPrediction),str(NRMSE_GTvsSource),str(PSNR_GTvsPrediction),str(PSNR_GTvsSource)])\n","\n","          #Here we calculate the ssim average for each image in each checkpoints\n","\n","        Average_SSIM_checkpoint = Average(ssim_score_list)\n","        Average_ssim_score_list.append(Average_SSIM_checkpoint)\n","\n","\n","# All data is now processed saved\n","  \n","\n","# -------------------------------- Display --------------------------------\n","\n","# Display the IoV vs Threshold plot\n","plt.figure(figsize=(20,5))\n","plt.plot(Checkpoint_list, Average_ssim_score_list, label=\"SSIM\")\n","plt.title('Checkpoints vs. SSIM')\n","plt.ylabel('SSIM')\n","plt.xlabel('Checkpoints')\n","plt.legend()\n","plt.savefig(full_QC_model_path+'Quality Control/SSIMvsCheckpoint_data.png',bbox_inches='tight',pad_inches=0)\n","plt.show()\n","\n","\n","\n","# -------------------------------- Display RGB --------------------------------\n","\n","from ipywidgets import interact\n","import ipywidgets as widgets\n","\n","\n","if Image_type == \"RGB\":\n","  random_choice_shortname_no_PNG = shortname_no_PNG\n","\n","  @interact\n","  def show_results(file=os.listdir(Source_QC_folder), checkpoints=Checkpoint_list):\n","\n","    random_choice_shortname_no_PNG = file[:-4]\n","\n","    df1 = pd.read_csv(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/\"+\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\", header=0)\n","    df2 = df1.set_index(\"image #\", drop = False)\n","    index_SSIM_GTvsPrediction = df2.loc[file, \"Prediction v. GT mSSIM\"]\n","    index_SSIM_GTvsSource = df2.loc[file, \"Input v. GT mSSIM\"]\n","\n","#Setting up colours\n","    \n","    cmap = None\n","\n","    plt.figure(figsize=(10,10))\n","\n","# Target (Ground-truth)\n","    plt.subplot(3,3,1)\n","    plt.axis('off')\n","    img_GT = imageio.imread(os.path.join(Target_QC_folder, file), as_gray=False, pilmode=\"RGB\")\n","    plt.imshow(img_GT, cmap = cmap)\n","    plt.title('Target',fontsize=15)\n","\n","# Source\n","    plt.subplot(3,3,2)\n","    plt.axis('off')\n","    img_Source = imageio.imread(os.path.join(Source_QC_folder, file), as_gray=False, pilmode=\"RGB\")\n","    plt.imshow(img_Source, cmap = cmap)\n","    plt.title('Source',fontsize=15)\n","\n","#Prediction\n","    plt.subplot(3,3,3)\n","    plt.axis('off')\n","\n","    img_Prediction = io.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_fake.png\"))\n","\n","    plt.imshow(img_Prediction, cmap = cmap)\n","    plt.title('Prediction',fontsize=15)\n","\n","\n","#SSIM between GT and Source\n","    plt.subplot(3,3,5)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","\n","    img_SSIM_GTvsSource = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"SSIM_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n","\n","    imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n","#plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n","    plt.title('Target vs. Source',fontsize=15)\n","    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsSource,3)),fontsize=14)\n","    plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n","\n","#SSIM between GT and Prediction\n","    plt.subplot(3,3,6)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False) \n","\n","    img_SSIM_GTvsPrediction = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"SSIM_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n","\n","    imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n","#plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n","    plt.title('Target vs. Prediction',fontsize=15)\n","    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsPrediction,3)),fontsize=14)\n","    plt.savefig(full_QC_model_path+'Quality Control/QC_example_data.png',bbox_inches='tight',pad_inches=0)\n","\n","# -------------------------------- Display Grayscale --------------------------------\n","\n","if Image_type == \"Grayscale\":\n","  random_choice_shortname_no_PNG = shortname_no_PNG\n","\n","  @interact\n","  def show_results(file=os.listdir(Source_QC_folder), checkpoints=Checkpoint_list):\n","\n","    random_choice_shortname_no_PNG = file[:-4]\n","\n","    df1 = pd.read_csv(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints)+\"/\"+\"QC_metrics_\"+QC_model_name+str(checkpoints)+\".csv\", header=0)\n","    df2 = df1.set_index(\"image #\", drop = False)\n","    index_SSIM_GTvsPrediction = df2.loc[file, \"Prediction v. GT mSSIM\"]\n","    index_SSIM_GTvsSource = df2.loc[file, \"Input v. GT mSSIM\"]\n","\n","    NRMSE_GTvsPrediction = df2.loc[file, \"Prediction v. GT NRMSE\"]\n","    NRMSE_GTvsSource = df2.loc[file, \"Input v. GT NRMSE\"]\n","    PSNR_GTvsSource = df2.loc[file, \"Input v. GT PSNR\"]\n","    PSNR_GTvsPrediction = df2.loc[file, \"Prediction v. GT PSNR\"]\n"," \n","\n","    plt.figure(figsize=(15,15))\n","\n","    cmap = None\n","  \n","  # Target (Ground-truth)\n","    plt.subplot(3,3,1)\n","    plt.axis('off')\n","    img_GT = imageio.imread(os.path.join(Target_QC_folder, file), as_gray=True, pilmode=\"RGB\")\n","\n","    plt.imshow(img_GT, norm=simple_norm(img_GT, percent = 99), cmap = 'gray')\n","    plt.title('Target',fontsize=15)\n","\n","# Source\n","    plt.subplot(3,3,2)\n","    plt.axis('off')\n","    img_Source = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_real.png\"))\n","    plt.imshow(img_Source, norm=simple_norm(img_Source, percent = 99))\n","    plt.title('Source',fontsize=15)\n","\n","#Prediction\n","    plt.subplot(3,3,3)\n","    plt.axis('off')\n","    img_Prediction = io.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), random_choice_shortname_no_PNG+\"_fake.png\"))\n","    plt.imshow(img_Prediction, norm=simple_norm(img_Prediction, percent = 99))\n","    plt.title('Prediction',fontsize=15)\n","\n","#Setting up colours\n","    cmap = plt.cm.CMRmap\n","\n","#SSIM between GT and Source\n","    plt.subplot(3,3,5)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","\n","    img_SSIM_GTvsSource = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"SSIM_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n","    img_SSIM_GTvsSource = img_SSIM_GTvsSource / 255\n","    imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n","\n","  \n","    plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n","    plt.title('Target vs. Source',fontsize=15)\n","    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsSource,3)),fontsize=14)\n","    plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n","\n","#SSIM between GT and Prediction\n","    plt.subplot(3,3,6)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)  \n","  \n","  \n","    img_SSIM_GTvsPrediction = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"SSIM_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n","    img_SSIM_GTvsPrediction = img_SSIM_GTvsPrediction / 255\n","    imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n","\n","  \n","    plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n","    plt.title('Target vs. Prediction',fontsize=15)\n","    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsPrediction,3)),fontsize=14)\n","\n","#Root Squared Error between GT and Source\n","    plt.subplot(3,3,8)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","\n","    img_RSE_GTvsSource = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"RSE_GTvsSource_\"+random_choice_shortname_no_PNG+\".tif\"))\n","    img_RSE_GTvsSource = img_RSE_GTvsSource / 255\n","  \n","\n","    imRSE_GTvsSource = plt.imshow(img_RSE_GTvsSource, cmap = cmap, vmin=0, vmax = 1)\n","    plt.colorbar(imRSE_GTvsSource,fraction=0.046,pad=0.04)\n","    plt.title('Target vs. Source',fontsize=15)\n","    plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsSource,3))+', PSNR: '+str(round(PSNR_GTvsSource,3)),fontsize=14)\n","#plt.title('Target vs. Source PSNR: '+str(round(PSNR_GTvsSource,3)))\n","    plt.ylabel('RSE maps',fontsize=20, rotation=0, labelpad=75)\n","\n","#Root Squared Error between GT and Prediction\n","    plt.subplot(3,3,9)\n","#plt.axis('off')\n","    plt.tick_params(\n","      axis='both',      # changes apply to the x-axis and y-axis\n","      which='both',      # both major and minor ticks are affected\n","      bottom=False,      # ticks along the bottom edge are off\n","      top=False,        # ticks along the top edge are off\n","      left=False,       # ticks along the left edge are off\n","      right=False,         # ticks along the right edge are off\n","      labelbottom=False,\n","      labelleft=False)\n","\n","    img_RSE_GTvsPrediction = imageio.imread(os.path.join(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/\"+str(checkpoints), \"RSE_GTvsPrediction_\"+random_choice_shortname_no_PNG+\".tif\"))\n","\n","    img_RSE_GTvsPrediction = img_RSE_GTvsPrediction / 255\n","\n","    imRSE_GTvsPrediction = plt.imshow(img_RSE_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n","    plt.colorbar(imRSE_GTvsPrediction,fraction=0.046,pad=0.04)\n","    plt.title('Target vs. Prediction',fontsize=15)\n","    plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsPrediction,3))+', PSNR: '+str(round(PSNR_GTvsPrediction,3)),fontsize=14)\n","    plt.savefig(full_QC_model_path+'/Quality Control/QC_example_data.png',bbox_inches='tight',pad_inches=0)\n","\n","\n","#Make a pdf summary of the QC results\n","\n","from datetime import datetime\n","\n","class MyFPDF(FPDF, HTMLMixin):\n","    pass\n","\n","pdf = MyFPDF()\n","pdf.add_page()\n","pdf.set_right_margin(-1)\n","pdf.set_font(\"Arial\", size = 11, style='B') \n","\n","Network = 'cycleGAN'\n","\n","\n","day = datetime.now()\n","datetime_str = str(day)[0:10]\n","\n","Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n","pdf.multi_cell(180, 5, txt = Header, align = 'L') \n","\n","all_packages = ''\n","for requirement in freeze(local_only=True):\n","  all_packages = all_packages+requirement+', '\n","\n","pdf.set_font('')\n","pdf.set_font('Arial', size = 11, style = 'B')\n","pdf.ln(2)\n","pdf.cell(190, 5, txt = 'Development of Training Losses', ln=1, align='L')\n","pdf.ln(1)\n","exp_size = io.imread(full_QC_model_path+'Quality Control/SSIMvsCheckpoint_data.png').shape\n","pdf.image(full_QC_model_path+'Quality Control/SSIMvsCheckpoint_data.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n","pdf.ln(2)\n","pdf.set_font('')\n","pdf.set_font('Arial', size = 10, style = 'B')\n","pdf.ln(3)\n","pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n","pdf.ln(1)\n","exp_size = io.imread(full_QC_model_path+'Quality Control/QC_example_data.png').shape\n","if Image_type == 'RGB':\n","  pdf.image(full_QC_model_path+'Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/5), h = round(exp_size[0]/5))\n","if Image_type == 'Grayscale':\n","  pdf.image(full_QC_model_path+'Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n","pdf.ln(1)\n","pdf.set_font('')\n","pdf.set_font('Arial', size = 11, style = 'B')\n","pdf.ln(1)\n","pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n","pdf.set_font('')\n","pdf.set_font_size(10.)\n","\n","pdf.ln(1)\n","for checkpoint in os.listdir(full_QC_model_path+'Quality Control'):\n","  if os.path.isdir(os.path.join(full_QC_model_path,'Quality Control',checkpoint)):\n","    pdf.set_font('')\n","    pdf.set_font('Arial', size = 10, style = 'B')\n","    pdf.cell(70, 5, txt = 'Metrics for checkpoint: '+ str(checkpoint), align='L', ln=1)\n","    html = \"\"\"\n","    <body>\n","    <font size=\"8\" face=\"Courier New\" >\n","    <table width=95% style=\"margin-left:0px;\">\"\"\"\n","    with open(full_QC_model_path+'Quality Control/'+str(checkpoint)+'/QC_metrics_'+QC_model_name+str(checkpoint)+'.csv', 'r') as csvfile:\n","      metrics = csv.reader(csvfile)\n","      header = next(metrics)\n","      image = header[0]\n","      mSSIM_PvsGT = header[1]\n","      mSSIM_SvsGT = header[2]\n","      header = \"\"\"\n","      <tr>\n","      <th width = 60% align=\"left\">{0}</th>\n","      <th width = 20% align=\"center\">{1}</th>\n","      <th width = 20% align=\"center\">{2}</th>\n","      </tr>\"\"\".format(image,mSSIM_PvsGT,mSSIM_SvsGT)\n","      html = html+header\n","      for row in metrics:\n","        image = row[0]\n","        mSSIM_PvsGT = row[1]\n","        mSSIM_SvsGT = row[2]\n","        cells = \"\"\"\n","          <tr>\n","            <td width = 60% align=\"left\">{0}</td>\n","            <td width = 20% align=\"center\">{1}</td>\n","            <td width = 20% align=\"center\">{2}</td>\n","          </tr>\"\"\".format(image,str(round(float(mSSIM_PvsGT),3)),str(round(float(mSSIM_SvsGT),3)))\n","        html = html+cells\n","      html = html+\"\"\"</body></table>\"\"\"\n","    pdf.write_html(html)\n","    pdf.ln(2)\n","  else:\n","    continue\n","\n","pdf.ln(1)\n","pdf.set_font('')\n","pdf.set_font_size(10.)\n","ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy.\" BioRxiv (2020).'\n","pdf.multi_cell(190, 5, txt = ref_1, align='L')\n","ref_2 = '- cycleGAN: Zhu, Jun-Yan, et al. \"Unpaired image-to-image translation using cycle-consistent adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017.'\n","pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","\n","pdf.ln(3)\n","reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n","\n","pdf.set_font('Arial', size = 11, style='B')\n","pdf.multi_cell(190, 5, txt=reminder, align='C')\n","\n","pdf.output(full_QC_model_path+'Quality Control/'+QC_model_name+'_QC_report.pdf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Esqnbew8uznk"},"source":["# **6. Using the trained model**\n","\n","---\n","\n","<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"d8wuQGjoq6eN"},"source":["## **6.1. Generate prediction(s) from unseen dataset**\n","---\n","\n","<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as PNG images.\n","\n","<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n","\n","<font size = 4>**`Result_folder`:** This folder will contain the predicted output images.\n","\n","<font size = 4>**`checkpoint`:** Choose the checkpoint number you would like to use to perform predictions. To use the \"latest\" checkpoint, input \"latest\"."]},{"cell_type":"code","metadata":{"id":"yb3suNkfpNA9","cellView":"form"},"source":["#@markdown ### Provide the path to your dataset and to the folder where the predictions are saved, then play the cell to predict outputs from your unseen images.\n","\n","import glob\n","import os.path\n","\n","\n","latest = \"latest\"\n","\n","Data_folder = \"\" #@param {type:\"string\"}\n","Result_folder = \"\" #@param {type:\"string\"}\n","\n","\n","# model name and path\n","#@markdown ###Do you want to use the current trained model?\n","Use_the_current_trained_model = False #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, please provide the path to the model folder:\n","\n","Prediction_model_folder = \"\" #@param {type:\"string\"}\n","\n","#@markdown ###What model checkpoint would you like to use?\n","\n","checkpoint = latest#@param {type:\"raw\"}\n","\n","\n","#Here we find the loaded model name and parent path\n","Prediction_model_name = os.path.basename(Prediction_model_folder)\n","Prediction_model_path = os.path.dirname(Prediction_model_folder)\n","\n","#here we check if we use the newly trained network or not\n","if (Use_the_current_trained_model): \n","  print(\"Using current trained network\")\n","  Prediction_model_name = model_name\n","  Prediction_model_path = model_path\n","\n","#here we check if the model exists\n","full_Prediction_model_path = Prediction_model_path+'/'+Prediction_model_name+'/'\n","\n","if os.path.exists(full_Prediction_model_path):\n","  print(\"The \"+Prediction_model_name+\" network will be used.\")\n","else:\n","  W  = '\\033[0m'  # white (normal)\n","  R  = '\\033[31m' # red\n","  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","  print('Please make sure you provide a valid model path and model name before proceeding further.')\n","\n","\n","# Here we check that checkpoint exist, if not the closest one will be chosen \n","\n","Nb_Checkpoint = len(glob.glob(os.path.join(full_Prediction_model_path, '*G_A.pth')))\n","print(Nb_Checkpoint)\n","\n","\n","if not checkpoint == \"latest\":\n","\n","  if  checkpoint < 10:\n","    checkpoint = 5\n","\n","  if not checkpoint % 5 == 0:\n","    checkpoint = ((int(checkpoint / 5)-1) * 5)\n","    print (bcolors.WARNING + \" Your chosen checkpoints is not divisible by 5; therefore the checkpoints chosen is now:\",checkpoints)\n"," \n","  if checkpoint > Nb_Checkpoint*5:\n","    checkpoint = \"latest\"\n","\n","  if checkpoint == Nb_Checkpoint*5:\n","    checkpoint = \"latest\"\n","\n","\n","\n","\n","# Here we need to move the data to be analysed so that cycleGAN can find them\n","\n","Saving_path_prediction= \"/content/\"+Prediction_model_name\n","\n","if os.path.exists(Saving_path_prediction):\n","  shutil.rmtree(Saving_path_prediction)\n","os.makedirs(Saving_path_prediction)\n","\n","Saving_path_Data_folder = Saving_path_prediction+\"/testA\"\n","\n","if os.path.exists(Saving_path_Data_folder):\n","  shutil.rmtree(Saving_path_Data_folder)\n","os.makedirs(Saving_path_Data_folder)\n","\n","for files in os.listdir(Data_folder):\n","    shutil.copyfile(Data_folder+\"/\"+files, Saving_path_Data_folder+\"/\"+files)\n","\n","\n","Nb_files_Data_folder = len(os.listdir(Data_folder)) +10\n","\n","\n","\n","#Here we copy and rename the checkpoint to be used\n","\n","shutil.copyfile(full_Prediction_model_path+\"/\"+str(checkpoint)+\"_net_G_A.pth\", full_Prediction_model_path+\"/\"+str(checkpoint)+\"_net_G.pth\")\n","\n","\n","# This will find the image dimension of a randomly choosen image in Data_folder \n","random_choice = random.choice(os.listdir(Data_folder))\n","x = imageio.imread(Data_folder+\"/\"+random_choice)\n","\n","#Find image XY dimension\n","Image_Y = x.shape[0]\n","Image_X = x.shape[1]\n","\n","Image_min_dim = min(Image_Y, Image_X)\n","\n","print(Image_min_dim)\n","\n","\n","\n","#-------------------------------- Perform predictions -----------------------------\n","\n","#-------------------------------- Options that can be used to perform predictions -----------------------------\n","\n","# basic parameters\n","        #('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n","        #('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n","        #('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n","        #('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n","\n","# model parameters\n","        #('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n","        #('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n","        #('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n","        #('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n","        #('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n","        #('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n","        #('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n","        #('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n","        #('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n","        #('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n","        #('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n","        #('--no_dropout', action='store_true', help='no dropout for the generator')\n","        \n","# dataset parameters\n","        #('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n","        #('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n","        #('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n","        #('--num_threads', default=4, type=int, help='# threads for loading data')\n","        #('--batch_size', type=int, default=1, help='input batch size')\n","        #('--load_size', type=int, default=286, help='scale images to this size')\n","        #('--crop_size', type=int, default=256, help='then crop to this size')\n","        #('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n","        #('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n","        #('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n","        #('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n","        \n","# additional parameters\n","        #('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n","        #('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n","        #('--verbose', action='store_true', help='if specified, print more debugging information')\n","        #('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n","        \n","\n","        #('--ntest', type=int, default=float(\"inf\"), help='# of test examples.')\n","        #('--results_dir', type=str, default='./results/', help='saves results here.')\n","        #('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n","        #('--phase', type=str, default='test', help='train, val, test, etc')\n","\n","# Dropout and Batchnorm has different behavioir during training and test.\n","        #('--eval', action='store_true', help='use eval mode during test time.')\n","        #('--num_test', type=int, default=50, help='how many test images to run')\n","        # rewrite devalue values\n","        \n","# To avoid cropping, the load_size should be the same as crop_size\n","        #parser.set_defaults(load_size=parser.get_default('crop_size'))\n","\n","#------------------------------------------------------------------------\n","\n","\n","#---------------------------- Predictions are performed here ----------------------\n","\n","os.chdir(\"/content\")\n","\n","!python pytorch-CycleGAN-and-pix2pix/test.py --dataroot \"$Saving_path_Data_folder\" --name \"$Prediction_model_name\" --model test --no_dropout --preprocess scale_width --load_size $Image_min_dim --crop_size $Image_min_dim --results_dir \"$Result_folder\" --checkpoints_dir \"$Prediction_model_path\" --num_test $Nb_files_Data_folder --epoch $checkpoint\n","\n","#-----------------------------------------------------------------------------------\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIe3CRD7XUxa"},"source":["## **6.2. Inspect the predicted output**\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"LmDP8xiwXTTL","cellView":"form"},"source":["# @markdown ##Run this cell to display a randomly chosen input and its corresponding predicted output.\n","import os\n","# This will display a randomly chosen dataset input and predicted output\n","random_choice = random.choice(os.listdir(Data_folder))\n","\n","\n","random_choice_no_extension = os.path.splitext(random_choice)\n","\n","\n","x = imageio.imread(Result_folder+\"/\"+Prediction_model_name+\"/test_\"+str(checkpoint)+\"/images/\"+random_choice_no_extension[0]+\"_real.png\")\n","\n","\n","y = imageio.imread(Result_folder+\"/\"+Prediction_model_name+\"/test_\"+str(checkpoint)+\"/images/\"+random_choice_no_extension[0]+\"_fake.png\")\n","\n","f=plt.figure(figsize=(16,8))\n","plt.subplot(1,2,1)\n","plt.imshow(x, interpolation='nearest')\n","plt.title('Input')\n","plt.axis('off');\n","\n","plt.subplot(1,2,2)\n","plt.imshow(y, interpolation='nearest')\n","plt.title('Prediction')\n","plt.axis('off');\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvkd66PldsXB"},"source":["## **6.3. Download your predictions**\n","---\n","\n","<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."]},{"cell_type":"markdown","metadata":{"id":"Rn9zpWpo0xNw"},"source":["\n","#**Thank you for using CycleGAN!**"]}]}