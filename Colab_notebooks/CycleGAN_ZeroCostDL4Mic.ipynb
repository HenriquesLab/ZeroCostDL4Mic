{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkSguVy8Xv83"
   },
   "source": [
    "# **CycleGAN**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 4>CycleGAN is a method that can capture the characteristics of one image domain and learn how these characteristics can be translated into another image domain, all in the absence of any paired training examples. It was first published by [Zhu *et al.* in 2017](https://arxiv.org/abs/1703.10593). Unlike pix2pix, the image transformation performed does not require paired images for training (unsupervised learning) and is made possible here by using a set of two Generative Adversarial Networks (GANs) that learn to transform images both from the first domain to the second and vice-versa.\n",
    "\n",
    "<font size = 4> **This particular notebook enables unpaired image-to-image translation. If your dataset is paired, you should also consider using the pix2pix notebook.**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 4>*Disclaimer*:\n",
    "\n",
    "<font size = 4>This notebook is part of the *Zero-Cost Deep-Learning to Enhance Microscopy* project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories.\n",
    "\n",
    "<font size = 4>This notebook is based on the following paper:\n",
    "\n",
    "<font size = 4> **Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks** from Zhu *et al.* published in arXiv in 2018 (https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "<font size = 4>The source code of the CycleGAN PyTorch implementation  can be found in: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
    "\n",
    "<font size = 4>**Please also cite this original paper when using or developing this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqvkQQkcuMmM"
   },
   "source": [
    "# **Licenses**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### License for ZeroCostDL4Mic:\n",
    "<details>\n",
    "<summary> (click for more details) </summary>\n",
    "\n",
    "This ZeroCostDL4Mic notebook is distributed under the MIT licence\n",
    "\n",
    "</details>\n",
    "\n",
    "### License for CycleGAN:\n",
    "\n",
    "<details>\n",
    "<summary> (click for more details) </summary>\n",
    "\n",
    "Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "* Redistributions of source code must retain the above copyright notice, this\n",
    "  list of conditions and the following disclaimer.\n",
    "\n",
    "* Redistributions in binary form must reproduce the above copyright notice,\n",
    "  this list of conditions and the following disclaimer in the documentation\n",
    "  and/or other materials provided with the distribution.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "</details>\n",
    "\n",
    "### License for pix2pix:\n",
    "\n",
    "<details>\n",
    "<summary> (click for more details) </summary>\n",
    "\n",
    "BSD License\n",
    "\n",
    "For pix2pix software\n",
    "Copyright (c) 2016, Phillip Isola and Jun-Yan Zhu\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "* Redistributions of source code must retain the above copyright notice, this\n",
    "  list of conditions and the following disclaimer.\n",
    "\n",
    "* Redistributions in binary form must reproduce the above copyright notice,\n",
    "  this list of conditions and the following disclaimer in the documentation\n",
    "  and/or other materials provided with the distribution.\n",
    "\n",
    "</details>\n",
    "\n",
    "### License for DCGAN:\n",
    "\n",
    "<details>\n",
    "<summary> (click for more details) </summary>\n",
    "\n",
    "BSD License\n",
    "\n",
    "For dcgan.torch software\n",
    "\n",
    "Copyright (c) 2015, Facebook, Inc. All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "\n",
    "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "\n",
    "Neither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWAz2i7RdxUV"
   },
   "source": [
    "# **0. Before getting started**\n",
    "---\n",
    "<font size = 4> To train CycleGAN, **you only need two folders containing PNG images**. The images do not need to be paired.\n",
    "\n",
    "<font size = 4>While you do not need paired images to train CycleGAN, if possible, **we strongly recommend that you generate  a paired dataset. This means that the same image needs to be acquired in the two conditions. These images can be used to assess the quality of your trained model (Quality control dataset)**. The quality control assessment can be done directly in this notebook.\n",
    "\n",
    "\n",
    "<font size = 4> Please note that you currently can **only use .png files!**\n",
    "\n",
    "\n",
    "<font size = 4>Here's a common data structure that can work:\n",
    "*   Experiment A\n",
    "    - **Training dataset (non-matching images)**\n",
    "      - Training_source\n",
    "        - img_1.png, img_2.png, ...\n",
    "      - Training_target\n",
    "        - img_1.png, img_2.png, ...\n",
    "    - **Quality control dataset (matching images)**\n",
    "     - Training_source\n",
    "        - img_1.png, img_2.png\n",
    "      - Training_target\n",
    "        - img_1.png, img_2.png\n",
    "    - **Data to be predicted**\n",
    "    - **Results**\n",
    "\n",
    "---\n",
    "<font size = 4>**Important note**\n",
    "\n",
    "<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to run predictions using the model that you trained.\n",
    "\n",
    "<font size = 4>- If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n",
    "\n",
    "<font size = 4>- If you only wish to **run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdN8B91xZO0x"
   },
   "source": [
    "# **1. Install CycleGAN and dependencies**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r25ArYNc67kR"
   },
   "source": [
    "## **1.1. Install key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cyfr_vP967kS"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Install CycleGAN and dependencies\n",
    "\n",
    "print(\"We are installing fdf2, bioimageio.core, bioimageio.spec and pydantic.\")\n",
    "print(\"This might takes some minutes ...\")\n",
    "\n",
    "# Package for generating PDF documents\n",
    "!pip install -q fpdf2==2.8.1\n",
    "\n",
    "# The [bioimageio] extras are installed for bioimageio model support\n",
    "!pip install -q bioimageio.core==0.7.0\n",
    "!pip install -q bioimageio.spec==0.5.3.5\n",
    "\n",
    "# Install an older version of scikit-image\n",
    "# !pip install -q scikit-image==0.18.0\n",
    "\n",
    "# Install an older version of imageio\n",
    "# !pip install -q imageio==2.17.0\n",
    "\n",
    "# Install fixed version of pydantic to avoid bugs on 2.10.0\n",
    "!pip install -q pydantic==2.9.2\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHFcn4XT67kT"
   },
   "source": [
    "## **1.2. Load key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fq21zJVFNASx"
   },
   "outputs": [],
   "source": [
    "Notebook_version = '1.14.1'\n",
    "Network = 'CycleGAN'\n",
    "\n",
    "from builtins import any as b_any\n",
    "\n",
    "def get_requirements_path():\n",
    "    # Store requirements file in 'contents' directory\n",
    "    current_dir = os.getcwd()\n",
    "    dir_count = current_dir.count('/') - 1\n",
    "    path = '../' * (dir_count) + 'requirements.txt'\n",
    "    return path\n",
    "\n",
    "def filter_files(file_list, filter_list):\n",
    "    filtered_list = []\n",
    "    for fname in file_list:\n",
    "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
    "            filtered_list.append(fname)\n",
    "    return filtered_list\n",
    "\n",
    "def build_requirements_file(before, after):\n",
    "    path = get_requirements_path()\n",
    "\n",
    "    # Exporting requirements.txt for local run\n",
    "    !pip freeze > $path\n",
    "\n",
    "    # Get minimum requirements file\n",
    "    df = pd.read_csv(path)\n",
    "    mod_list = [m.split('.')[0] for m in after if not m in before]\n",
    "    req_list_temp = df.values.tolist()\n",
    "    req_list = [x[0] for x in req_list_temp]\n",
    "\n",
    "    # Replace with package name and handle cases where import name is different to module name\n",
    "    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
    "    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n",
    "    filtered_list = filter_files(req_list, mod_replace_list)\n",
    "\n",
    "    file=open(path,'w')\n",
    "    for item in filtered_list:\n",
    "        file.writelines(item)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "# Function to check source and target file paths include base path\n",
    "def check_base_path(base_path, path_to_data):\n",
    "  if base_path not in path_to_data:\n",
    "    if path_to_data[0] == '/':\n",
    "      path_to_data = path_to_data[1:]\n",
    "    path_to_data = os.path.join(base_path, path_to_data)\n",
    "  return path_to_data\n",
    "\n",
    "import sys\n",
    "before = [str(m) for m in sys.modules]\n",
    "\n",
    "#@markdown ##Load key dependencies\n",
    "\n",
    "#Create a variable to get and store relative base path\n",
    "import os\n",
    "base_path = os.getcwd()\n",
    "\n",
    "#------- Code from the cycleGAN demo notebook starts here -------\n",
    "\n",
    "#Here, we install libraries which are not already included in Colab.\n",
    "\n",
    "pix2pix_path = os.path.join(base_path, 'pytorch-CycleGAN-and-pix2pix')\n",
    "pix2pix_requirements_path = os.path.join(pix2pix_path, 'requirements.txt')\n",
    "\n",
    "if os.path.exists(pix2pix_path):\n",
    "  shutil.rmtree(pix2pix_path)\n",
    "\n",
    "!git clone --quiet https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix $pix2pix_path\n",
    "!pip install -q -r $pix2pix_requirements_path\n",
    "\n",
    "import imageio\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "# ------- Common variable to all ZeroCostDL4Mic notebooks -------\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from pip._internal.operations.freeze import freeze\n",
    "from skimage.metrics import structural_similarity\n",
    "from astropy.visualization import simple_norm\n",
    "from fpdf import FPDF, HTMLMixin\n",
    "from ipywidgets import interact\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "# BioImage Model Zoo dependencies\n",
    "\n",
    "import bioimageio.core as bioimageio_core\n",
    "from bioimageio.core.digest_spec import create_sample_for_model\n",
    "\n",
    "import bioimageio.spec\n",
    "import bioimageio.spec.model.v0_5 as bioimageio_spec\n",
    "from bioimageio.spec import save_bioimageio_package\n",
    "from bioimageio.spec.utils import download\n",
    "from bioimageio.spec._internal.io import FileDescr\n",
    "from bioimageio.spec.pretty_validation_errors import (\n",
    "    enable_pretty_validation_errors_in_ipynb,\n",
    ")\n",
    "enable_pretty_validation_errors_in_ipynb()\n",
    "\n",
    "# Colors for the warning messages\n",
    "class bcolors:\n",
    "  WARNING = '\\033[31m'\n",
    "  ENDC = '\\033[0m'\n",
    "\n",
    "#Disable some of the tensorflow warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Libraries installed\")\n",
    "\n",
    "# Check if this is the latest version of the notebook\n",
    "All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n",
    "print('Notebook version: '+Notebook_version)\n",
    "Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n",
    "print('Latest notebook version: '+Latest_Notebook_version)\n",
    "if Notebook_version == Latest_Notebook_version:\n",
    "  print(\"This notebook is up-to-date.\")\n",
    "else:\n",
    "  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\" + bcolors.ENDC)\n",
    "\n",
    "\n",
    "def export_cyclegan_torchscript_model(path_model_checkpoint):\n",
    "    ## A new text file needs to be created to read the model using the cycleGAN code and exporting it.\n",
    "\n",
    "    # Base script:\n",
    "    export_code = f\"\"\"import os\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    print('Warning: wandb package cannot be found. The option \"--use_wandb\" will result in error.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opt = TestOptions().parse()  # get test options\n",
    "    # hard-code some parameters for test\n",
    "    opt.num_threads = 0   # test code only supports num_threads = 0\n",
    "    opt.batch_size = 1    # test code only supports batch_size = 1\n",
    "    opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "    opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n",
    "    opt.display_id = -1   # no visdom display; the test code saves the results to a HTML file.\n",
    "    model = create_model(opt)      # create a model given opt.model and other options\n",
    "    model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
    "\n",
    "    # test with eval mode. This only affects layers like batchnorm and dropout.\n",
    "    # For [pix2pix]: we use batchnorm and dropout in the original pix2pix. You can experiment it with and without eval() mode.\n",
    "    # For [CycleGAN]: It should not affect CycleGAN as CycleGAN uses instancenorm without dropout.\n",
    "    model.eval()\n",
    "    import torch\n",
    "    net = getattr(model, 'netG_A')\n",
    "    if opt.gpu_ids:\n",
    "      model = torch.jit.script(net.module.cpu())\n",
    "    else:\n",
    "      model = torch.jit.script(net)\n",
    "    # save the model weights\\n\n",
    "    model.save(\\'{path_model_checkpoint}\\')\n",
    "    \"\"\"\n",
    "\n",
    "    # Write the new python exporting script\n",
    "    with open(os.path.join(base_path, \"pytorch-CycleGAN-and-pix2pix\", 'cyclegan_model_export.py'), 'w') as f:\n",
    "        f.write(export_code)\n",
    "\n",
    "\n",
    "def pdf_export(trained = False, augmentation = False, pretrained_model = False):\n",
    "  class MyFPDF(FPDF, HTMLMixin):\n",
    "    pass\n",
    "\n",
    "  pdf = MyFPDF()\n",
    "  pdf.add_page()\n",
    "  pdf.set_right_margin(-1)\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "\n",
    "  day = datetime.now()\n",
    "  datetime_str = str(day)[0:10]\n",
    "\n",
    "  Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
    "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  # add another cell\n",
    "  if trained:\n",
    "    training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n",
    "    pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  Header_2 = 'Information for your materials and method:'\n",
    "  pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n",
    "\n",
    "  all_packages = ''\n",
    "  for requirement in freeze(local_only=True):\n",
    "    all_packages = all_packages+requirement+', '\n",
    "  # print(all_packages)\n",
    "\n",
    "  #Main Packages\n",
    "  main_packages = ''\n",
    "  version_numbers = []\n",
    "  for name in ['tensorflow','numpy','torch']:\n",
    "    find_name=all_packages.find(name)\n",
    "    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
    "    #Version numbers only here:\n",
    "    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
    "\n",
    "  try:\n",
    "    cuda_version = subprocess.run([\"nvcc\",\"--version\"],stdout=subprocess.PIPE)\n",
    "    cuda_version = cuda_version.stdout.decode('utf-8')\n",
    "    cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
    "  except:\n",
    "    cuda_version = ' - No cuda found - '\n",
    "  try:\n",
    "    gpu_name = subprocess.run([\"nvidia-smi\"],stdout=subprocess.PIPE)\n",
    "    gpu_name = gpu_name.stdout.decode('utf-8')\n",
    "    gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
    "  except:\n",
    "    gpu_name = ' - No GPU found - '\n",
    "  #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
    "  #print(gpu_name)\n",
    "\n",
    "  shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n",
    "  dataset_size = len(os.listdir(Training_source))\n",
    "\n",
    "  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(dataset_size)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+' and a least-square GAN loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), torch (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
    "\n",
    "  if pretrained_model:\n",
    "    text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(dataset_size)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+' and an least-square GAN loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was retrained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), torch (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
    "\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  pdf.multi_cell(190, 5, txt = text, align='L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(28, 5, txt='Augmentation: ', ln=0)\n",
    "  pdf.set_font('')\n",
    "  if augmentation:\n",
    "    aug_text = 'The dataset was augmented by default'\n",
    "  else:\n",
    "    aug_text = 'No augmentation was used for training.'\n",
    "  pdf.multi_cell(190, 5, txt=aug_text, align='L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  if Use_Default_Advanced_Parameters:\n",
    "    pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
    "  pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\"\n",
    "  <table style=\"margin-left:0px;\">\n",
    "    <tr>\n",
    "      <th align=\"left\">Parameter</th>\n",
    "      <th align=\"left\">Value</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>number_of_epochs</td>\n",
    "      <td>{0}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>patch_size</td>\n",
    "      <td>{1}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>batch_size</td>\n",
    "      <td>{2}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>initial_learning_rate</td>\n",
    "      <td>{3}</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  \"\"\".format(number_of_epochs,str(patch_size)+'x'+str(patch_size),batch_size,initial_learning_rate)\n",
    "  pdf.write_html(html)\n",
    "\n",
    "  #pdf.multi_cell(190, 5, txt = text_2, align='L')\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(30, 5, txt= 'Training_source:', align = 'L', ln=0)\n",
    "  pdf.set_font('')\n",
    "  pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(29, 5, txt= 'Training_target:', align = 'L', ln=0)\n",
    "  pdf.set_font('')\n",
    "  pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n",
    "  #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(22, 5, txt= 'Model Path:', align = 'L', ln=0)\n",
    "  pdf.set_font('')\n",
    "  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread(os.path.join(base_path, 'TrainingDataExample_cycleGAN.png')).shape\n",
    "  pdf.image(base_path + '/TrainingDataExample_cycleGAN.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(1)\n",
    "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
    "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
    "  pdf.ln(1)\n",
    "  ref_2 = '- cycleGAN: Zhu, Jun-Yan, et al. \"Unpaired image-to-image translation using cycle-consistent adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017.'\n",
    "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
    "  pdf.ln(1)\n",
    "  # if Use_Data_augmentation:\n",
    "  #   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
    "  #   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
    "  pdf.ln(3)\n",
    "  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
    "  pdf.set_font('Arial', size = 11, style='B')\n",
    "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.output(os.path.join(model_path, model_name, f\"{model_name}_training_report.pdf\"))\n",
    "\n",
    "\n",
    "def qc_pdf_export():\n",
    "  class MyFPDF(FPDF, HTMLMixin):\n",
    "    pass\n",
    "\n",
    "  pdf = MyFPDF()\n",
    "  pdf.add_page()\n",
    "  pdf.set_right_margin(-1)\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "\n",
    "  Network = 'cycleGAN'\n",
    "\n",
    "\n",
    "  day = datetime.now()\n",
    "  datetime_str = str(day)[0:10]\n",
    "\n",
    "  Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n",
    "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  all_packages = ''\n",
    "  for requirement in freeze(local_only=True):\n",
    "    all_packages = all_packages+requirement+', '\n",
    "\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(2)\n",
    "  pdf.cell(190, 5, txt = 'Development of Training Losses', ln=1, align='L')\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread(os.path.join(full_QC_model_path, 'Quality Control', 'SSIMvsCheckpoint_data.png')).shape\n",
    "  pdf.image(os.path.join(full_QC_model_path, 'Quality Control', 'SSIMvsCheckpoint_data.png'), x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(2)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.ln(3)\n",
    "  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread(os.path.join(full_QC_model_path, 'Quality Control', 'QC_example_data.png')).shape\n",
    "  if Image_type == 'RGB':\n",
    "    pdf.image(os.path.join(full_QC_model_path, 'Quality Control', 'QC_example_data.png'),\n",
    "              x = 16, y = None, w = round(exp_size[1]/5), h = round(exp_size[0]/5))\n",
    "  if Image_type == 'Grayscale':\n",
    "    pdf.image(os.path.join(full_QC_model_path, 'Quality Control', 'QC_example_data.png'),\n",
    "              x = 16, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  for checkpoint in os.listdir(os.path.join(full_QC_model_path, 'Quality Control')):\n",
    "    if os.path.isdir(os.path.join(full_QC_model_path,'Quality Control',checkpoint)):\n",
    "      pdf.set_font('')\n",
    "      pdf.set_font('Arial', size = 10, style = 'B')\n",
    "      pdf.cell(70, 5, txt = 'Metrics for checkpoint: '+ str(checkpoint), align='L', ln=1)\n",
    "      html = \"\"\"\n",
    "      <body>\n",
    "      <font size=\"8\" face=\"Courier\" >\n",
    "      <table style=\"margin-left:0px;\">\"\"\"\n",
    "      with open(os.path.join(full_QC_model_path, 'Quality Control', str(checkpoint), 'QC_metrics_'+QC_model_name+str(checkpoint)+'.csv'), 'r') as csvfile:\n",
    "        metrics = csv.reader(csvfile)\n",
    "        header = next(metrics)\n",
    "        image = header[0]\n",
    "        mSSIM_PvsGT = header[1]\n",
    "        mSSIM_SvsGT = header[2]\n",
    "        header = \"\"\"\n",
    "        <tr>\n",
    "        <th align=\"left\">{0}</th>\n",
    "        <th align=\"center\">{1}</th>\n",
    "        <th align=\"center\">{2}</th>\n",
    "        </tr>\"\"\".format(image,mSSIM_PvsGT,mSSIM_SvsGT)\n",
    "        html = html+header\n",
    "        for row in metrics:\n",
    "          image = row[0]\n",
    "          mSSIM_PvsGT = row[1]\n",
    "          mSSIM_SvsGT = row[2]\n",
    "          cells = \"\"\"\n",
    "            <tr>\n",
    "              <td align=\"left\">{0}</td>\n",
    "              <td align=\"center\">{1}</td>\n",
    "              <td align=\"center\">{2}</td>\n",
    "            </tr>\"\"\".format(image,str(round(float(mSSIM_PvsGT),3)),str(round(float(mSSIM_SvsGT),3)))\n",
    "          html = html+cells\n",
    "        html = html+\"\"\"\n",
    "        </table>\n",
    "        </font>\n",
    "        </body>\"\"\"\n",
    "      pdf.write_html(html)\n",
    "      pdf.ln(2)\n",
    "    else:\n",
    "      continue\n",
    "\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
    "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
    "  pdf.ln(1)\n",
    "  ref_2 = '- cycleGAN: Zhu, Jun-Yan, et al. \"Unpaired image-to-image translation using cycle-consistent adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017.'\n",
    "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.ln(3)\n",
    "  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
    "\n",
    "  pdf.set_font('Arial', size = 11, style='B')\n",
    "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.output(os.path.join(full_QC_model_path, 'Quality Control', QC_model_name+'_QC_report.pdf'))\n",
    "\n",
    "\n",
    "# Build requirements file for local run\n",
    "after = [str(m) for m in sys.modules]\n",
    "build_requirements_file(before, after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4yWFoJNnoin"
   },
   "source": [
    "# **2. Initialise the Colab session**\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMNHVZfHmbKb"
   },
   "source": [
    "\n",
    "## **2.1. Check for GPU access**\n",
    "---\n",
    "\n",
    "By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n",
    "\n",
    "<font size = 4>Go to **Runtime -> Change the Runtime type**\n",
    "\n",
    "<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n",
    "\n",
    "<font size = 4>**Accelerator: GPU** *(Graphics processing unit)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zCvebubeSaGY"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Run this cell to check if you have GPU access\n",
    "\n",
    "!if type nvidia-smi >/dev/null 2>&1; then \\\n",
    "    echo \"You have GPU access\"; nvidia-smi; \\\n",
    "    else \\\n",
    "    echo -e \"You do not have GPU access.\\nDid you change your runtime?\\nIf the runtime setting is correct then Google did not allocate a GPU for your session\\nExpect slow performance. To access GPU try reconnecting later\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNIVx8_CLolt"
   },
   "source": [
    "## **2.2. Mount your Google Drive**\n",
    "---\n",
    "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
    "\n",
    "<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive.\n",
    "\n",
    "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "01Djr8v-5pPk"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
    "\n",
    "#@markdown * Click on the URL.\n",
    "\n",
    "#@markdown * Sign in your Google Account.\n",
    "\n",
    "#@markdown * Copy the authorization code.\n",
    "\n",
    "#@markdown * Enter the authorization code.\n",
    "\n",
    "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\".\n",
    "\n",
    "# mount user's Google Drive to Google Colab.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLYcZR9gMv42"
   },
   "source": [
    "# **3. Select your parameters and paths**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ_QxtSWQ7CL"
   },
   "source": [
    "## **3.1. Setting main training parameters**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuESFimvMv43"
   },
   "source": [
    "<font size = 5> **Paths for training, predictions and results**\n",
    "\n",
    "<font size = 4>**`Training_source:`, `Training_target`:** These are the paths to your folders containing the Training_source and Training_target training data respecively. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
    "\n",
    "<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n",
    "\n",
    "<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
    "\n",
    "<font size = 5>**Training Parameters**\n",
    "\n",
    "<font size = 4>**`number_of_epochs`:** Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10) epochs, but a full training should run for 200 epochs or more. Evaluate the performance after training (see 5). **Default value: 200**\n",
    "\n",
    "\n",
    "<font size = 5>**Advanced Parameters - experienced users only**\n",
    "\n",
    "<font size = 4>**`patch_size`:** CycleGAN divides the image into patches for training. Input the size of the patches (length of a side). The value should be smaller than the dimensions of the image and divisible by 4. **Default value: 256**\n",
    "\n",
    "<font size = 4>**When choosing the patch_size, the value should be i) large enough that it will enclose many instances, ii) small enough that the resulting patches fit into the RAM.**<font size = 4>\n",
    "\n",
    "<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 1**\n",
    "\n",
    "<font size = 4>**`initial_learning_rate`:** Input the initial value to be used as learning rate. **Default value: 0.0002**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ewpNJ_I0Mv47"
   },
   "outputs": [],
   "source": [
    "#@markdown ###Path to training images:\n",
    "\n",
    "Training_source = \"\" #@param {type:\"string\"}\n",
    "# Check that the base_path is on the path and otherwise add it\n",
    "Training_source = check_base_path(base_path, Training_source)\n",
    "\n",
    "Training_target = \"\" #@param {type:\"string\"}\n",
    "# Check that the base_path is on the path and otherwise add it\n",
    "Training_target = check_base_path(base_path, Training_target)\n",
    "\n",
    "# model name and path\n",
    "#@markdown ###Name of the model and path to model folder:\n",
    "model_name = \"\" #@param {type:\"string\"}\n",
    "model_path = \"\" #@param {type:\"string\"}\n",
    "\n",
    "Saving_path = os.path.join(model_path, model_name)\n",
    "\n",
    "# Here we check that no model with the same name already exist, if so delete\n",
    "if os.path.exists(Saving_path):\n",
    "  print(f\"{bcolors.WARNING} !! WARNING: {model_name} already exists and will be deleted on section 4.1. !! {bcolors.ENDC}\")\n",
    "  print(f\"{bcolors.WARNING} To continue training {model_name}, choose a new model_name here, and load {model_name} in section 3.3. {bcolors.ENDC}\")\n",
    "\n",
    "# Other parameters for training.\n",
    "#@markdown ###Training Parameters\n",
    "#@markdown Number of epochs:\n",
    "number_of_epochs = 200 #@param {type:\"number\"}\n",
    "assert number_of_epochs > 5, \"Number of epochs should be greater than 5 in order to save model checkpoints.\"\n",
    "\n",
    "#@markdown ###Advanced Parameters\n",
    "\n",
    "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
    "#@markdown ###If not, please input:\n",
    "patch_size =  256 #@param {type:\"number\"} # in pixels\n",
    "batch_size =  1 #@param {type:\"number\"}\n",
    "initial_learning_rate = 0.0002 #@param {type:\"number\"}\n",
    "\n",
    "if (Use_Default_Advanced_Parameters):\n",
    "  print(\"\\nDefault advanced parameters enabled.\\n\")\n",
    "  patch_size = 256\n",
    "  batch_size = 1\n",
    "  initial_learning_rate = 0.0002\n",
    "\n",
    "# Here we disable pre-trained model by default (in case the  cell is not ran)\n",
    "Use_pretrained_model = False\n",
    "# Here we disable data augmentation by default (in case the cell is not ran)\n",
    "Use_Data_augmentation = False\n",
    "\n",
    "# This will display a randomly chosen dataset input and output\n",
    "random_choice = random.choice(os.listdir(Training_source))\n",
    "x = imageio.imread(os.path.join(Training_source, random_choice))\n",
    "\n",
    "#Find image XY dimension\n",
    "Image_Y = x.shape[0]\n",
    "Image_X = x.shape[1]\n",
    "\n",
    "Image_min_dim = min(Image_Y, Image_X)\n",
    "\n",
    "#Hyperparameters failsafes\n",
    "if patch_size > min(Image_Y, Image_X):\n",
    "  patch_size = min(Image_Y, Image_X)\n",
    "  print (f\"{bcolors.WARNING}Your chosen patch_size is bigger than the xy dimension of your image; therefore the patch_size chosen is now: {patch_size}{bcolors.ENDC}\")\n",
    "\n",
    "# Here we check that patch_size is divisible by 4\n",
    "if not patch_size % 4 == 0:\n",
    "    patch_size = ((int(patch_size / 4)-1) * 4)\n",
    "    print (f\"{bcolors.WARNING}Your chosen patch_size is not divisible by 4; therefore the patch_size chosen is now: {patch_size}{bcolors.ENDC}\")\n",
    "\n",
    "random_choice_2 = random.choice(os.listdir(Training_target))\n",
    "y = imageio.imread(os.path.join(Training_target, random_choice_2))\n",
    "\n",
    "print(f\"Training source shape: {x.shape}\")\n",
    "print(f\"Training target shape: {y.shape}\")\n",
    "\n",
    "f=plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x, interpolation='nearest')\n",
    "plt.title('Training source')\n",
    "plt.axis('off');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(y, interpolation='nearest')\n",
    "plt.title('Training target')\n",
    "plt.axis('off');\n",
    "plt.savefig(os.path.join(base_path, 'TrainingDataExample_cycleGAN.png'),bbox_inches='tight',pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyQZKby8yFME"
   },
   "source": [
    "## **3.2. Data augmentation**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_jCy7xOx2g3"
   },
   "source": [
    "<font size = 4>Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation is not necessary for training and if your training dataset is large you should disable it.\n",
    "\n",
    "<font size = 4>Data augmentation is performed here by flipping the patches.\n",
    "\n",
    "<font size = 4> By default data augmentation is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DMqWq5-AxnFU"
   },
   "outputs": [],
   "source": [
    "#Data augmentation\n",
    "\n",
    "#@markdown ##Play this cell to enable or disable data augmentation:\n",
    "\n",
    "Use_Data_augmentation = True #@param {type:\"boolean\"}\n",
    "\n",
    "if Use_Data_augmentation:\n",
    "  print(\"\\nData augmentation enabled.\\n\")\n",
    "else:\n",
    "  print(\"\\nData augmentation disabled.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L9zSGtORKYI"
   },
   "source": [
    "\n",
    "## **3.3. Using weights from a pre-trained model as initial weights**\n",
    "---\n",
    "<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a CycleGAN model**.\n",
    "\n",
    "<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**.\n",
    "\n",
    "<font size = 4> In order to continue training from the point where the pre-trained model left off, it is adviseable to also **load the learning rate** that was used when the training ended. This is automatically saved for models trained with ZeroCostDL4Mic and will be loaded here. If no learning rate can be found in the model folder provided, the default learning rate will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9vC2n-HeLdiJ"
   },
   "outputs": [],
   "source": [
    "# @markdown ##Loading weights from a pre-trained network\n",
    "\n",
    "Use_pretrained_model = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ###If so, please provide the path to the model folder:\n",
    "#@markdown > Files named `latest_net_G_A.pth` and `latest_net_G_B.pt` need to be in that folder.\n",
    "pretrained_model_path = \"\" #@param {type:\"string\"}\n",
    "\n",
    "# --------------------- Check if we load a previously trained model ------------------------\n",
    "if Use_pretrained_model:\n",
    "\n",
    "  h5_file_path_A = os.path.join(pretrained_model_path, \"latest_net_G_A.pth\")\n",
    "  h5_file_path_B = os.path.join(pretrained_model_path, \"latest_net_G_B.pth\")\n",
    "\n",
    "  # --------------------- Check the model exist ------------------------\n",
    "\n",
    "  if os.path.exists(h5_file_path_A) and os.path.exists(h5_file_path_B):\n",
    "    print(f\"Pretrained model {os.path.basename(pretrained_model_path)} was found and will be loaded prior to training.\")\n",
    "  else:\n",
    "    print(f\"{bcolors.WARNING}WARNING: Pretrained model does not exist{bcolors.ENDC}\")\n",
    "    Use_pretrained_model = False\n",
    "    print(f\"{bcolors.WARNING}No pretrained network will be used.{bcolors.ENDC}\")\n",
    "\n",
    "else:\n",
    "  print(f\"{bcolors.WARNING}No pretrained network will be used.{bcolors.ENDC}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCGklf1vZf2M"
   },
   "source": [
    "# **4. Train the network**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KYOuygETJkT"
   },
   "source": [
    "## **4.1. Prepare the training data for training**\n",
    "---\n",
    "<font size = 4>Here, we use the information from 3. to prepare the training data into a suitable format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lIUAOJ_LMv5E"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Prepare the data for training\n",
    "\n",
    "print(\"Data preparation in progress\")\n",
    "\n",
    "if os.path.exists(Saving_path):\n",
    "  shutil.rmtree(Saving_path)\n",
    "os.makedirs(Saving_path)\n",
    "\n",
    "#To use Cyclegan we need to organise the data in a way the model can understand\n",
    "\n",
    "Training_Data_Folder = os.path.join(base_path, \"TrainingData\")\n",
    "\n",
    "if os.path.exists(Training_Data_Folder):\n",
    "  shutil.rmtree(Training_Data_Folder)\n",
    "os.makedirs(Training_Data_Folder)\n",
    "\n",
    "TrainA_Folder = os.path.join(Training_Data_Folder, \"trainA\")\n",
    "if os.path.exists(TrainA_Folder):\n",
    "  shutil.rmtree(TrainA_Folder)\n",
    "os.makedirs(TrainA_Folder)\n",
    "\n",
    "TrainB_Folder = os.path.join(Training_Data_Folder, \"trainB\")\n",
    "if os.path.exists(TrainB_Folder):\n",
    "  shutil.rmtree(TrainB_Folder)\n",
    "os.makedirs(TrainB_Folder)\n",
    "\n",
    "#--------------- Here we move the files to trainA and train B ---------\n",
    "\n",
    "for f in os.listdir(Training_source):\n",
    "  shutil.copyfile(os.path.join(Training_source, f),\n",
    "                  os.path.join(TrainA_Folder, f))\n",
    "\n",
    "for files in os.listdir(Training_target):\n",
    "  shutil.copyfile(os.path.join(Training_target, files),\n",
    "                  os.path.join(TrainB_Folder, files))\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# CycleGAN use number of EPOCH without lr decay and number of EPOCH with lr decay\n",
    "\n",
    "number_of_epochs_lr_stable = int(number_of_epochs/2)\n",
    "number_of_epochs_lr_decay = int(number_of_epochs/2)\n",
    "\n",
    "if Use_pretrained_model :\n",
    "  for f in os.listdir(pretrained_model_path):\n",
    "    if (f.startswith(\"latest_net_\")):\n",
    "      shutil.copyfile(os.path.join(pretrained_model_path, f),\n",
    "                      os.path.join(Saving_path, f))\n",
    "\n",
    "pdf_export(augmentation = Use_Data_augmentation,\n",
    "           pretrained_model = Use_pretrained_model)\n",
    "\n",
    "print(\"Data ready for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Dfn8ZsEMv5d"
   },
   "source": [
    "## **4.2. Start Training**\n",
    "---\n",
    "<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n",
    "\n",
    "<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches or continue the training in a second Colab session.\n",
    "\n",
    "<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder from Google Drive as all data can be erased at the next training if using the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iwNmp1PUzRDQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@markdown ##Start training\n",
    "\n",
    "# Get if GPU is available\n",
    "gpu_available = !if type nvidia-smi >/dev/null 2>&1; then echo 0; else echo -1; fi;\n",
    "gpu_available = str(gpu_available[0])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#--------------------------------- Command line inputs to change CycleGAN paramaters------------\n",
    "\n",
    "       # basic parameters\n",
    "        #('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n",
    "        #('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n",
    "        #('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
    "        #('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
    "\n",
    "       # model parameters\n",
    "        #('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n",
    "        #('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n",
    "        #('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n",
    "        #('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n",
    "        #('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n",
    "        #('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n",
    "        #('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n",
    "        #('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n",
    "        #('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n",
    "        #('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n",
    "        #('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
    "        #('--no_dropout', action='store_true', help='no dropout for the generator')\n",
    "\n",
    "       # dataset parameters\n",
    "        #('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n",
    "        #('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n",
    "        #('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
    "        #('--num_threads', default=4, type=int, help='# threads for loading data')\n",
    "        #('--batch_size', type=int, default=1, help='input batch size')\n",
    "        #('--load_size', type=int, default=286, help='scale images to this size')\n",
    "        #('--crop_size', type=int, default=256, help='then crop to this size')\n",
    "        #('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
    "        #('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n",
    "        #('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
    "        #('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n",
    "\n",
    "       # additional parameters\n",
    "        #('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
    "        #('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n",
    "        #('--verbose', action='store_true', help='if specified, print more debugging information')\n",
    "        #('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n",
    "\n",
    "       # visdom and HTML visualization parameters\n",
    "        #('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n",
    "        #('--display_ncols', type=int, default=4, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n",
    "        #('--display_id', type=int, default=1, help='window id of the web display')\n",
    "        #('--display_server', type=str, default=\"http://localhost\", help='visdom server of the web display')\n",
    "        #('--display_env', type=str, default='main', help='visdom display environment name (default is \"main\")')\n",
    "        #('--display_port', type=int, default=8097, help='visdom port of the web display')\n",
    "        #('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n",
    "        #('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
    "        #('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n",
    "\n",
    "       # network saving and loading parameters\n",
    "        #('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n",
    "        #('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n",
    "        #('--save_by_iter', action='store_true', help='whether saves model by iteration')\n",
    "        #('--continue_train', action='store_true', help='continue training: load the latest model')\n",
    "        #('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
    "        #('--phase', type=str, default='train', help='train, val, test, etc')\n",
    "\n",
    "       # training parameters\n",
    "        #('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')\n",
    "        #('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')\n",
    "        #('--beta1', type=float, default=0.5, help='momentum term of adam')\n",
    "        #('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
    "        #('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n",
    "        #('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n",
    "        #('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')\n",
    "        #('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations'\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#----- Start the training ------------------------------------\n",
    "\n",
    "if Use_pretrained_model:\n",
    "  if Use_Data_augmentation:\n",
    "    !python pytorch-CycleGAN-and-pix2pix/train.py \\\n",
    "          --dataroot \"$Training_Data_Folder\" --input_nc 3 \\\n",
    "          --name $model_name --model cycle_gan --batch_size $batch_size \\\n",
    "          --preprocess scale_width_and_crop --load_size $Image_min_dim \\\n",
    "          --crop_size $patch_size --checkpoints_dir \"$model_path\" \\\n",
    "          --no_html --n_epochs $number_of_epochs_lr_stable \\\n",
    "          --n_epochs_decay $number_of_epochs_lr_decay \\\n",
    "          --lr $initial_learning_rate --display_id 0 \\\n",
    "          --save_epoch_freq 5 --continue_train \\\n",
    "          --gpu_ids $gpu_available\n",
    "  else:\n",
    "    !python pytorch-CycleGAN-and-pix2pix/train.py \\\n",
    "          --dataroot \"$Training_Data_Folder\" --input_nc 3 \\\n",
    "          --name $model_name --model cycle_gan --batch_size $batch_size \\\n",
    "          --preprocess scale_width_and_crop --load_size $Image_min_dim \\\n",
    "          --crop_size $patch_size --checkpoints_dir \"$model_path\" \\\n",
    "          --no_html --n_epochs $number_of_epochs_lr_stable \\\n",
    "          --n_epochs_decay $number_of_epochs_lr_decay \\\n",
    "          --lr $initial_learning_rate --display_id 0 \\\n",
    "          --save_epoch_freq 5 --continue_train --no_flip \\\n",
    "          --gpu_ids $gpu_available\n",
    "else:\n",
    "  if Use_Data_augmentation:\n",
    "    !python pytorch-CycleGAN-and-pix2pix/train.py \\\n",
    "          --dataroot \"$Training_Data_Folder\" --input_nc 3 \\\n",
    "          --name $model_name --model cycle_gan --batch_size $batch_size \\\n",
    "          --preprocess scale_width_and_crop --load_size $Image_min_dim \\\n",
    "          --crop_size $patch_size --checkpoints_dir \"$model_path\"  \\\n",
    "          --no_html --n_epochs $number_of_epochs_lr_stable \\\n",
    "          --n_epochs_decay $number_of_epochs_lr_decay \\\n",
    "          --lr $initial_learning_rate --display_id 0 \\\n",
    "          --save_epoch_freq 5 --gpu_ids $gpu_available\n",
    "  else:\n",
    "    !python pytorch-CycleGAN-and-pix2pix/train.py \\\n",
    "          --dataroot \"$Training_Data_Folder\" --input_nc 3 \\\n",
    "          --name $model_name --model cycle_gan --batch_size $batch_size \\\n",
    "          --preprocess scale_width_and_crop --load_size $Image_min_dim \\\n",
    "          --crop_size $patch_size --checkpoints_dir \"$model_path\" \\\n",
    "          --no_html --n_epochs $number_of_epochs_lr_stable \\\n",
    "          --n_epochs_decay $number_of_epochs_lr_decay \\\n",
    "          --lr $initial_learning_rate --display_id 0 \\\n",
    "          --save_epoch_freq 5 --no_flip --gpu_ids \\\n",
    "          $gpu_available\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "print(\"Training, done.\")\n",
    "\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "mins, sec = divmod(dt, 60)\n",
    "hour, mins = divmod(mins, 60)\n",
    "print(\"Time elapsed:\",hour, \"hour(s)\",mins,\"min(s)\",round(sec),\"sec(s)\")\n",
    "\n",
    "# Save training summary as pdf\n",
    "\n",
    "pdf_export(trained = True, augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0Hynw3-xHp1"
   },
   "source": [
    "# **5. Evaluate your model**\n",
    "---\n",
    "\n",
    "<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model.\n",
    "\n",
    "<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n",
    "\n",
    "<font size = 4>Unfortunately loss functions curve are not very informative for GAN network. Therefore we perform the QC here using a test dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wext8woxt_F"
   },
   "source": [
    "## **5.1. Choose the model you want to assess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eAJzMwPA6tlH"
   },
   "outputs": [],
   "source": [
    "# model name and path\n",
    "#@markdown ###Do you want to assess the model you just trained ?\n",
    "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ###If not, please provide the path to the model folder:\n",
    "\n",
    "QC_model_folder = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#Here we define the loaded model name and path\n",
    "if (Use_the_current_trained_model):\n",
    "  QC_model_name = model_name\n",
    "  QC_model_path = model_path\n",
    "else:\n",
    "  QC_model_name = os.path.basename(QC_model_folder)\n",
    "  QC_model_path = os.path.dirname(QC_model_folder)\n",
    "\n",
    "full_QC_model_path = os.path.join(QC_model_path, QC_model_name)\n",
    "\n",
    "if os.path.exists(full_QC_model_path):\n",
    "  print(f\"The {QC_model_name} network will be evaluated\")\n",
    "else:\n",
    "  print(f\"{bcolors.WARNING}!! WARNING: The chosen model does not exist !!{bcolors.ENDC}\")\n",
    "  print('Please make sure you provide a valid model path and model name before proceeding further.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CFbjvTpx5C3"
   },
   "source": [
    "## **5.2. Identify the best checkpoint to use to make predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8tCfAadx96X"
   },
   "source": [
    "<font size = 4> CycleGAN save model checkpoints every five epochs. Due to the stochastic nature of GAN networks, the last checkpoint is not always the best one to use. As a consequence, it can be challenging to choose the most suitable checkpoint to use to make predictions.\n",
    "\n",
    "<font size = 4>This section allows you to perform predictions using all the saved checkpoints and to estimate the quality of these predictions by comparing them to the provided ground truths images. Metric used include:\n",
    "\n",
    "<font size = 4>**1. The SSIM (structural similarity) map**\n",
    "\n",
    "<font size = 4>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info).\n",
    "\n",
    "<font size=4>**mSSIM** is the SSIM value calculated across the entire window of both images.\n",
    "\n",
    "<font size=4>**The output below shows the SSIM maps with the mSSIM**\n",
    "\n",
    "<font size = 4>**2. The RSE (Root Squared Error) map**\n",
    "\n",
    "<font size = 4>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n",
    "\n",
    "\n",
    "<font size =4>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n",
    "\n",
    "<font size = 4>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n",
    "\n",
    "<font size=4>**The output below shows the RSE maps with the NRMSE and PSNR values.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "q2T4t8NNyDZ6"
   },
   "outputs": [],
   "source": [
    "#@markdown ##Choose the folders that contain your Quality Control dataset\n",
    "\n",
    "# Get if GPU is available\n",
    "gpu_available = !if type nvidia-smi >/dev/null 2>&1; then echo 0; else echo -1; fi;\n",
    "gpu_available = str(gpu_available[0])\n",
    "\n",
    "Source_QC_folder = \"\" #@param{type:\"string\"}\n",
    "Target_QC_folder = \"\" #@param{type:\"string\"}\n",
    "\n",
    "Image_type = \"Grayscale\" #@param [\"Grayscale\", \"RGB\"]\n",
    "\n",
    "# average function\n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "# Create a quality control folder\n",
    "\n",
    "if os.path.exists(os.path.join(full_QC_model_path, \"Quality Control\")):\n",
    "  shutil.rmtree(os.path.join(full_QC_model_path, \"Quality Control\"))\n",
    "os.makedirs(os.path.join(full_QC_model_path, \"Quality Control\"))\n",
    "\n",
    "# Here we need to move the data to be analysed so that cycleGAN can find them\n",
    "\n",
    "Saving_path_QC = os.path.join(base_path, QC_model_name)\n",
    "\n",
    "if os.path.exists(Saving_path_QC):\n",
    "  shutil.rmtree(Saving_path_QC)\n",
    "os.makedirs(Saving_path_QC)\n",
    "\n",
    "Saving_path_QC_folder = Saving_path_QC+\"_images\"\n",
    "\n",
    "if os.path.exists(Saving_path_QC_folder):\n",
    "  shutil.rmtree(Saving_path_QC_folder)\n",
    "os.makedirs(Saving_path_QC_folder)\n",
    "\n",
    "#Here we copy and rename the all the checkpoint to be analysed\n",
    "\n",
    "for f in os.listdir(full_QC_model_path):\n",
    "  shortname = f[:-6]\n",
    "  shortname = shortname + \".pth\"\n",
    "  if f.endswith(\"net_G_A.pth\"):\n",
    "    shutil.copyfile(os.path.join(full_QC_model_path,f),\n",
    "                    os.path.join(Saving_path_QC,shortname))\n",
    "\n",
    "\n",
    "for files in os.listdir(Source_QC_folder):\n",
    "  shutil.copyfile(os.path.join(Source_QC_folder, files),\n",
    "                  os.path.join(Saving_path_QC_folder, files))\n",
    "\n",
    "Nb_files_Data_folder = len(os.listdir(Source_QC_folder))\n",
    "\n",
    "# List images in Source_QC_folder\n",
    "\n",
    "# This will find the image dimension of a randomly chosen image in Source_QC_folder\n",
    "random_choice = random.choice(os.listdir(Source_QC_folder))\n",
    "x = imageio.imread(os.path.join(Source_QC_folder, random_choice))\n",
    "\n",
    "#Find image XY dimension\n",
    "Image_Y = x.shape[0]\n",
    "Image_X = x.shape[1]\n",
    "\n",
    "Image_min_dim = int(min(Image_Y, Image_X))\n",
    "\n",
    "Nb_Checkpoint = len(os.listdir(Saving_path_QC))\n",
    "\n",
    "print(f\"There are {Nb_Checkpoint} notebook checkpoints\")\n",
    "\n",
    "## Initiate list\n",
    "\n",
    "Checkpoint_list = []\n",
    "Average_ssim_score_list = []\n",
    "\n",
    "\n",
    "for j in range(1, len(os.listdir(Saving_path_QC))+1):\n",
    "  checkpoints = j*5\n",
    "\n",
    "  if checkpoints == Nb_Checkpoint*5:\n",
    "    checkpoints = \"latest\"\n",
    "\n",
    "  print(f\"The currently analysed checkpoint is: {checkpoints}.\")\n",
    "\n",
    "  Checkpoint_list.append(checkpoints)\n",
    "\n",
    "  # Create a quality control/Prediction Folder\n",
    "\n",
    "  QC_prediction_results = os.path.join(QC_model_path, QC_model_name, \"Quality Control\", str(checkpoints))\n",
    "\n",
    "  if os.path.exists(QC_prediction_results):\n",
    "    shutil.rmtree(QC_prediction_results)\n",
    "  os.makedirs(QC_prediction_results)\n",
    "\n",
    "#---------------------------- Predictions are performed here ----------------------\n",
    "\n",
    "  !python pytorch-CycleGAN-and-pix2pix/test.py \\\n",
    "          --dataroot \"$Saving_path_QC_folder\" --name \"$QC_model_name\" \\\n",
    "          --model test --epoch $checkpoints --no_dropout \\\n",
    "          --preprocess scale_width --load_size $Image_min_dim \\\n",
    "          --crop_size $Image_min_dim --results_dir \"$QC_prediction_results\" \\\n",
    "          --checkpoints_dir \"$base_path\" --gpu_ids $gpu_available\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "#Here we need to move the data again and remove all the unnecessary folders\n",
    "\n",
    "  Checkpoint_name = \"test_\"+str(checkpoints)\n",
    "\n",
    "  QC_results_images = os.path.join(QC_prediction_results, QC_model_name, Checkpoint_name, \"images\")\n",
    "  QC_results_images_files = os.listdir(QC_results_images)\n",
    "\n",
    "  for f in QC_results_images_files:\n",
    "    shutil.copyfile(os.path.join(QC_results_images, f),\n",
    "                    os.path.join(QC_prediction_results, f))\n",
    "\n",
    "  #Here we clean up the extra files\n",
    "  shutil.rmtree(os.path.join(QC_prediction_results, QC_model_name))\n",
    "\n",
    "#-------------------------------- QC for RGB ------------------------------------\n",
    "  if Image_type == \"RGB\":\n",
    "    # List images in Source_QC_folder\n",
    "    # This will find the image dimension of a randomly choosen image in Source_QC_folder\n",
    "    random_choice = random.choice(os.listdir(Source_QC_folder))\n",
    "    x = imageio.imread(os.path.join(Source_QC_folder, random_choice))\n",
    "\n",
    "    def ssim(img1, img2):\n",
    "      return structural_similarity(img1, img2, data_range=1.,\n",
    "                                   full=True, multichannel=True)\n",
    "\n",
    "# Open and create the csv file that will contain all the QC metrics\n",
    "    csv_file_path = os.path.join(QC_model_path, QC_model_name,\n",
    "                                 \"Quality Control\", str(checkpoints),\n",
    "                                 f\"QC_metrics_{QC_model_name+str(checkpoints)}.csv\")\n",
    "\n",
    "    with open(csv_file_path, \"w\", newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "    # Write the header in the csv file\n",
    "        writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\"])\n",
    "\n",
    "    # Initiate list\n",
    "        ssim_score_list = []\n",
    "\n",
    "    # Let's loop through the provided dataset in the QC folders\n",
    "\n",
    "        for i in os.listdir(Source_QC_folder):\n",
    "          if not os.path.isdir(os.path.join(Source_QC_folder,i)):\n",
    "            print('Running QC on: '+i)\n",
    "\n",
    "            shortname_no_PNG = i[:-4]\n",
    "\n",
    "      # -------------------------------- Target test data (Ground truth) --------------------------------\n",
    "            test_GT = imageio.imread(os.path.join(Target_QC_folder, i), mode=\"F\", pilmode=\"RGB\")\n",
    "\n",
    "      # -------------------------------- Source test data --------------------------------\n",
    "            test_source = imageio.imread(os.path.join(QC_model_path, QC_model_name,\n",
    "                                                      \"Quality Control\", str(checkpoints),\n",
    "                                                      shortname_no_PNG+\"_real.png\"))\n",
    "\n",
    "      # -------------------------------- Prediction --------------------------------\n",
    "            test_prediction = imageio.imread(os.path.join(QC_model_pat, QC_model_name,\n",
    "                                                          \"Quality Control\", str(checkpoints),\n",
    "                                                          shortname_no_PNG+\"_fake.png\"))\n",
    "\n",
    "          #--------------------------- Here we normalise using histograms matching--------------------------------\n",
    "            channel_idx = test_GT.shape.index(3)\n",
    "            test_prediction_matched = match_histograms(test_prediction, test_GT,\n",
    "                                                       channel_axis=channel_idx)\n",
    "            test_source_matched = match_histograms(test_source, test_GT,\n",
    "                                                   channel_axis=channel_idx)\n",
    "\n",
    "      # -------------------------------- Calculate the metric maps and save them --------------------------------\n",
    "\n",
    "      # Calculate the SSIM maps\n",
    "            index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT, test_prediction_matched)\n",
    "            index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT, test_source_matched)\n",
    "\n",
    "            ssim_score_list.append(index_SSIM_GTvsPrediction)\n",
    "\n",
    "      #Save ssim_maps\n",
    "            img_SSIM_GTvsPrediction_8bit = (img_SSIM_GTvsPrediction* 255).astype(\"uint8\")\n",
    "            io.imsave(os.path.join(QC_model_path, QC_model_name,\n",
    "                                   \"Quality Control\", str(checkpoints),\n",
    "                                   f\"SSIM_GTvsPrediction_{shortname_no_PNG}.tif\"),\n",
    "                      img_SSIM_GTvsPrediction_8bit)\n",
    "            img_SSIM_GTvsSource_8bit = (img_SSIM_GTvsSource* 255).astype(\"uint8\")\n",
    "            io.imsave(os.path.join(QC_model_path, QC_model_name,\n",
    "                                   \"Quality Control\", str(checkpoints),\n",
    "                                   f\"SSIM_GTvsSource_{shortname_no_PNG}.tif\"),\n",
    "                      img_SSIM_GTvsSource_8bit)\n",
    "\n",
    "\n",
    "            writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource)])\n",
    "\n",
    "      #Here we calculate the ssim average for each image in each checkpoints\n",
    "\n",
    "        Average_SSIM_checkpoint = Average(ssim_score_list)\n",
    "        Average_ssim_score_list.append(Average_SSIM_checkpoint)\n",
    "\n",
    "#------------------------------------------- QC for Grayscale ----------------------------------------------\n",
    "\n",
    "  if Image_type == \"Grayscale\":\n",
    "    def ssim(img1, img2):\n",
    "      return structural_similarity(img1,img2,data_range=1.,full=True, gaussian_weights=True, use_sample_covariance=False, sigma=1.5)\n",
    "\n",
    "    def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
    "      mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
    "      ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
    "      return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
    "\n",
    "    def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n",
    "      if dtype is not None:\n",
    "        x   = x.astype(dtype,copy=False)\n",
    "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
    "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
    "        eps = dtype(eps)\n",
    "\n",
    "        try:\n",
    "            import numexpr\n",
    "            x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
    "        except ImportError:\n",
    "            x =                   (x - mi) / ( ma - mi + eps )\n",
    "\n",
    "        if clip:\n",
    "            x = np.clip(x,0,1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def norm_minmse(gt, x, normalize_gt=True):\n",
    "      if normalize_gt:\n",
    "        gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n",
    "        x = x.astype(np.float32, copy=False) - np.mean(x)\n",
    "        #x = x - np.mean(x)\n",
    "        gt = gt.astype(np.float32, copy=False) - np.mean(gt)\n",
    "        #gt = gt - np.mean(gt)\n",
    "        scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n",
    "        return gt, scale * x\n",
    "\n",
    "    # Open and create the csv file that will contain all the QC metrics\n",
    "    QC_csv_file_path = os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                    str(checkpoints), \n",
    "                                    f\"QC_metrics_{QC_model_name+str(checkpoints)}.csv\")\n",
    "    with open(QC_csv_file_path, \"w\", newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header in the csv file\n",
    "        writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\", \"Prediction v. GT NRMSE\", \"Input v. GT NRMSE\", \"Prediction v. GT PSNR\", \"Input v. GT PSNR\"])\n",
    "\n",
    "        # Let's loop through the provided dataset in the QC folders\n",
    "        for i in os.listdir(Source_QC_folder):\n",
    "          if os.path.isfile(os.path.join(Source_QC_folder,i)) and i != \"..DS_Store\":\n",
    "            print(f\"Running QC on: {i}\")\n",
    "\n",
    "            ssim_score_list = []\n",
    "            shortname_no_PNG = os.path.splitext(i)[0] # Just the filename without extension\n",
    "\n",
    "            # ---------------- Target test data (Ground truth) ----------------\n",
    "            test_GT_raw = imageio.imread(os.path.join(Target_QC_folder, i), \n",
    "                                         mode=\"F\", pilmode=\"RGB\")\n",
    "            test_GT = test_GT_raw[:,:,2]\n",
    "\n",
    "            # ---------------- Source test data ----------------\n",
    "            test_source_raw = imageio.imread(os.path.join(QC_model_path, QC_model_name,\n",
    "                                                          \"Quality Control\",\n",
    "                                                          str(checkpoints),\n",
    "                                                          f\"{shortname_no_PNG}_real.png\"))\n",
    "            test_source = test_source_raw[:,:,2]\n",
    "\n",
    "            # Normalize the images wrt each other by minimizing the MSE between GT and Source image\n",
    "            test_GT_norm,test_source_norm = norm_minmse(test_GT, test_source, normalize_gt=True)\n",
    "\n",
    "            # ---------------- Prediction ----------------\n",
    "            test_prediction_raw = imageio.imread(os.path.join(QC_model_path, QC_model_name,\n",
    "                                                              \"Quality Control\",\n",
    "                                                              str(checkpoints),\n",
    "                                                              f\"{shortname_no_PNG}_fake.png\"))\n",
    "            test_prediction = test_prediction_raw[:,:,2]\n",
    "\n",
    "            # Normalize the images wrt each other by minimizing the MSE between GT and prediction\n",
    "            test_GT_norm,test_prediction_norm = norm_minmse(test_GT, test_prediction,\n",
    "                                                            normalize_gt=True)\n",
    "\n",
    "            # -------------------------------- Calculate the metric maps and save them --------------------------------\n",
    "            # Calculate the SSIM maps\n",
    "            index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT_norm, test_prediction_norm)\n",
    "            index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT_norm, test_source_norm)\n",
    "\n",
    "            ssim_score_list.append(index_SSIM_GTvsPrediction)\n",
    "\n",
    "            # Save ssim_maps\n",
    "            img_SSIM_GTvsPrediction_8bit = (img_SSIM_GTvsPrediction* 255).astype(\"uint8\")\n",
    "            io.imsave(os.path.join(QC_model_path, QC_model_name, \"Quality Control\",\n",
    "                                   str(checkpoints),\n",
    "                                   \"SSIM_GTvsPrediction_{shortname_no_PNG}.tif\"),\n",
    "                      img_SSIM_GTvsPrediction_8bit)\n",
    "            img_SSIM_GTvsSource_8bit = (img_SSIM_GTvsSource* 255).astype(\"uint8\")\n",
    "            io.imsave(os.path.join(QC_model_path, QC_model_name, \"Quality Control\",\n",
    "                                   str(checkpoints), \n",
    "                                   f\"SSIM_GTvsSource_{shortname_no_PNG}.tif\"),\n",
    "                      img_SSIM_GTvsSource_8bit)\n",
    "\n",
    "            # Calculate the Root Squared Error (RSE) maps\n",
    "            img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n",
    "            img_RSE_GTvsSource = np.sqrt(np.square(test_GT_norm - test_source_norm))\n",
    "\n",
    "            # Save SE maps\n",
    "            img_RSE_GTvsPrediction_8bit = (img_RSE_GTvsPrediction* 255).astype(\"uint8\")\n",
    "            io.imsave(os.path.join(QC_model_path, QC_model_name, \"Quality Control\",\n",
    "                                   str(checkpoints), \n",
    "                                   f\"RSE_GTvsPrediction_{shortname_no_PNG}.tif\"),\n",
    "                      img_RSE_GTvsPrediction_8bit)\n",
    "            img_RSE_GTvsSource_8bit = (img_RSE_GTvsSource* 255).astype(\"uint8\")\n",
    "            io.imsave(os.path.join(QC_model_path, QC_model_name, \"Quality Control\",\n",
    "                                   str(checkpoints), \n",
    "                                   f\"RSE_GTvsSource_{shortname_no_PNG}.tif\"),\n",
    "                      img_RSE_GTvsSource_8bit)\n",
    "\n",
    "            # -------------------------------- Calculate the RSE metrics and save them --------------------------------\n",
    "            # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n",
    "            NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n",
    "            NRMSE_GTvsSource = np.sqrt(np.mean(img_RSE_GTvsSource))\n",
    "\n",
    "            # We can also measure the peak signal to noise ratio between the images\n",
    "            PSNR_GTvsPrediction = psnr(test_GT_norm,test_prediction_norm,data_range=1.0)\n",
    "            PSNR_GTvsSource = psnr(test_GT_norm,test_source_norm,data_range=1.0)\n",
    "\n",
    "            writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource),str(NRMSE_GTvsPrediction),str(NRMSE_GTvsSource),str(PSNR_GTvsPrediction),str(PSNR_GTvsSource)])\n",
    "\n",
    "        #Here we calculate the ssim average for each image in each checkpoints\n",
    "        Average_SSIM_checkpoint = Average(ssim_score_list)\n",
    "        Average_ssim_score_list.append(Average_SSIM_checkpoint)\n",
    "\n",
    "\n",
    "# All data is now processed saved\n",
    "\n",
    "\n",
    "# -------------------------------- Display --------------------------------\n",
    "\n",
    "# Display the IoV vs Threshold plot\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(Checkpoint_list, Average_ssim_score_list, label=\"SSIM\")\n",
    "plt.title('Checkpoints vs. SSIM')\n",
    "plt.ylabel('SSIM')\n",
    "plt.xlabel('Checkpoints')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(full_QC_model_path, 'Quality Control', 'SSIMvsCheckpoint_data.png'),\n",
    "            bbox_inches='tight',pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------- Display RGB --------------------------------\n",
    "\n",
    "if Image_type == \"RGB\":\n",
    "  random_choice_shortname_no_PNG = shortname_no_PNG\n",
    "\n",
    "  @interact\n",
    "  def show_results(file=os.listdir(Source_QC_folder), checkpoints=Checkpoint_list):\n",
    "\n",
    "    random_choice_shortname_no_PNG = file[:-4]\n",
    "\n",
    "    df1 = pd.read_csv(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                   str(checkpoints), \n",
    "                                   f\"QC_metrics_{QC_model_name+str(checkpoints)}.csv\"), \n",
    "                      header=0)\n",
    "    df2 = df1.set_index(\"image #\", drop = False)\n",
    "    index_SSIM_GTvsPrediction = df2.loc[file, \"Prediction v. GT mSSIM\"]\n",
    "    index_SSIM_GTvsSource = df2.loc[file, \"Input v. GT mSSIM\"]\n",
    "\n",
    "#Setting up colours\n",
    "\n",
    "    cmap = None\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "# Target (Ground-truth)\n",
    "    plt.subplot(3,3,1)\n",
    "    plt.axis('off')\n",
    "    img_GT = imageio.imread(os.path.join(Target_QC_folder, file), mode=\"F\", pilmode=\"RGB\")\n",
    "    plt.imshow(img_GT, cmap = cmap)\n",
    "    plt.title('Target',fontsize=15)\n",
    "\n",
    "# Source\n",
    "    plt.subplot(3,3,2)\n",
    "    plt.axis('off')\n",
    "    img_Source = imageio.imread(os.path.join(Source_QC_folder, file), mode=\"F\", pilmode=\"RGB\")\n",
    "    plt.imshow(img_Source, cmap = cmap)\n",
    "    plt.title('Source',fontsize=15)\n",
    "\n",
    "#Prediction\n",
    "    plt.subplot(3,3,3)\n",
    "    plt.axis('off')\n",
    "\n",
    "    img_Prediction = io.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                            str(checkpoints), \n",
    "                                            f\"{random_choice_shortname_no_PNG}_fake.png\"))\n",
    "\n",
    "    plt.imshow(img_Prediction, cmap = cmap)\n",
    "    plt.title('Prediction',fontsize=15)\n",
    "\n",
    "\n",
    "#SSIM between GT and Source\n",
    "    plt.subplot(3,3,5)\n",
    "#plt.axis('off')\n",
    "    plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "\n",
    "    img_SSIM_GTvsSource = imageio.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                                      str(checkpoints), \n",
    "                                                      f\"SSIM_GTvsSource_{random_choice_shortname_no_PNG}.tif\"))\n",
    "\n",
    "    imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n",
    "#plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n",
    "    plt.title('Target vs. Source',fontsize=15)\n",
    "    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsSource,3)),fontsize=14)\n",
    "    plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n",
    "\n",
    "#SSIM between GT and Prediction\n",
    "    plt.subplot(3,3,6)\n",
    "#plt.axis('off')\n",
    "    plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "\n",
    "    img_SSIM_GTvsPrediction = imageio.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                                          str(checkpoints), \n",
    "                                                          f\"SSIM_GTvsPrediction_{random_choice_shortname_no_PNG}.tif\"))\n",
    "\n",
    "    imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n",
    "#plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n",
    "    plt.title('Target vs. Prediction',fontsize=15)\n",
    "    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsPrediction,3)),fontsize=14)\n",
    "    plt.savefig(os.path.join(full_QC_model_path, 'Quality Control', 'QC_example_data.png'),\n",
    "                bbox_inches='tight',pad_inches=0)\n",
    "\n",
    "# -------------------------------- Display Grayscale --------------------------------\n",
    "\n",
    "if Image_type == \"Grayscale\":\n",
    "  random_choice_shortname_no_PNG = shortname_no_PNG\n",
    "\n",
    "  @interact\n",
    "  def show_results(file=os.listdir(Source_QC_folder), checkpoints=Checkpoint_list):\n",
    "\n",
    "    random_choice_shortname_no_PNG = file[:-4]\n",
    "\n",
    "    df1 = pd.read_csv(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                   str(checkpoints), \n",
    "                                   f\"QC_metrics_{QC_model_name+str(checkpoints)}.csv\"), \n",
    "                      header=0)\n",
    "    df2 = df1.set_index(\"image #\", drop = False)\n",
    "    index_SSIM_GTvsPrediction = df2.loc[file, \"Prediction v. GT mSSIM\"]\n",
    "    index_SSIM_GTvsSource = df2.loc[file, \"Input v. GT mSSIM\"]\n",
    "\n",
    "    NRMSE_GTvsPrediction = df2.loc[file, \"Prediction v. GT NRMSE\"]\n",
    "    NRMSE_GTvsSource = df2.loc[file, \"Input v. GT NRMSE\"]\n",
    "    PSNR_GTvsSource = df2.loc[file, \"Input v. GT PSNR\"]\n",
    "    PSNR_GTvsPrediction = df2.loc[file, \"Prediction v. GT PSNR\"]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "\n",
    "    cmap = None\n",
    "\n",
    "  # Target (Ground-truth)\n",
    "    plt.subplot(3,3,1)\n",
    "    plt.axis('off')\n",
    "    img_GT = imageio.imread(os.path.join(Target_QC_folder, file), mode=\"F\", pilmode=\"RGB\")\n",
    "\n",
    "    plt.imshow(img_GT, norm=simple_norm(img_GT, percent = 99), cmap = 'gray')\n",
    "    plt.title('Target',fontsize=15)\n",
    "\n",
    "# Source\n",
    "    plt.subplot(3,3,2)\n",
    "    plt.axis('off')\n",
    "    img_Source = imageio.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                             str(checkpoints), \n",
    "                                             f\"{random_choice_shortname_no_PNG}_real.png\"))\n",
    "    plt.imshow(img_Source, norm=simple_norm(img_Source, percent = 99))\n",
    "    plt.title('Source',fontsize=15)\n",
    "\n",
    "#Prediction\n",
    "    plt.subplot(3,3,3)\n",
    "    plt.axis('off')\n",
    "    img_Prediction = io.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                            str(checkpoints), \n",
    "                                            f\"{random_choice_shortname_no_PNG}_fake.png\"))\n",
    "    plt.imshow(img_Prediction, norm=simple_norm(img_Prediction, percent = 99))\n",
    "    plt.title('Prediction',fontsize=15)\n",
    "\n",
    "#Setting up colours\n",
    "    cmap = plt.cm.CMRmap\n",
    "\n",
    "#SSIM between GT and Source\n",
    "    plt.subplot(3,3,5)\n",
    "#plt.axis('off')\n",
    "    plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "\n",
    "    img_SSIM_GTvsSource = imageio.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                                      str(checkpoints), \n",
    "                                                      f\"SSIM_GTvsSource_{random_choice_shortname_no_PNG}.tif\"))\n",
    "    img_SSIM_GTvsSource = img_SSIM_GTvsSource / 255\n",
    "    imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "    plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n",
    "    plt.title('Target vs. Source',fontsize=15)\n",
    "    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsSource,3)),fontsize=14)\n",
    "    plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n",
    "\n",
    "#SSIM between GT and Prediction\n",
    "    plt.subplot(3,3,6)\n",
    "#plt.axis('off')\n",
    "    plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "\n",
    "\n",
    "    img_SSIM_GTvsPrediction = imageio.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                                          str(checkpoints), \n",
    "                                                          f\"SSIM_GTvsPrediction_{random_choice_shortname_no_PNG}.tif\"))\n",
    "    img_SSIM_GTvsPrediction = img_SSIM_GTvsPrediction / 255\n",
    "    imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n",
    "\n",
    "\n",
    "    plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n",
    "    plt.title('Target vs. Prediction',fontsize=15)\n",
    "    plt.xlabel('mSSIM: '+str(round(index_SSIM_GTvsPrediction,3)),fontsize=14)\n",
    "\n",
    "#Root Squared Error between GT and Source\n",
    "    plt.subplot(3,3,8)\n",
    "#plt.axis('off')\n",
    "    plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "\n",
    "    img_RSE_GTvsSource = imageio.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                                     str(checkpoints), \n",
    "                                                     f\"RSE_GTvsSource_{random_choice_shortname_no_PNG}.tif\"))\n",
    "    img_RSE_GTvsSource = img_RSE_GTvsSource / 255\n",
    "\n",
    "\n",
    "    imRSE_GTvsSource = plt.imshow(img_RSE_GTvsSource, cmap = cmap, vmin=0, vmax = 1)\n",
    "    plt.colorbar(imRSE_GTvsSource,fraction=0.046,pad=0.04)\n",
    "    plt.title('Target vs. Source',fontsize=15)\n",
    "    plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsSource,3))+', PSNR: '+str(round(PSNR_GTvsSource,3)),fontsize=14)\n",
    "#plt.title('Target vs. Source PSNR: '+str(round(PSNR_GTvsSource,3)))\n",
    "    plt.ylabel('RSE maps',fontsize=20, rotation=0, labelpad=75)\n",
    "\n",
    "#Root Squared Error between GT and Prediction\n",
    "    plt.subplot(3,3,9)\n",
    "#plt.axis('off')\n",
    "    plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "\n",
    "    img_RSE_GTvsPrediction = imageio.imread(os.path.join(QC_model_path, QC_model_name, \"Quality Control\", \n",
    "                                                         str(checkpoints), \n",
    "                                                         f\"RSE_GTvsPrediction_{random_choice_shortname_no_PNG}.tif\"))\n",
    "\n",
    "    img_RSE_GTvsPrediction = img_RSE_GTvsPrediction / 255\n",
    "\n",
    "    imRSE_GTvsPrediction = plt.imshow(img_RSE_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n",
    "    plt.colorbar(imRSE_GTvsPrediction,fraction=0.046,pad=0.04)\n",
    "    plt.title('Target vs. Prediction',fontsize=15)\n",
    "    plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsPrediction,3))+', PSNR: '+str(round(PSNR_GTvsPrediction,3)),fontsize=14)\n",
    "    plt.savefig(os.path.join(full_QC_model_path, 'Quality Control', 'QC_example_data.png'),\n",
    "                bbox_inches='tight',pad_inches=0)\n",
    "\n",
    "\n",
    "#Make a pdf summary of the QC results\n",
    "\n",
    "qc_pdf_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2y51M-I67kc"
   },
   "source": [
    "## **5.3. Export your model into the BioImage Model Zoo format**\n",
    "\n",
    "<font size = 4>This section exports the model into the [BioImage Model Zoo](https://bioimage.io/#/) format so it can be used directly with other community partners such as deepImageJ, Ilastik or BiaPy.\n",
    "\n",
    "<font size = 4>Please run the cells of previous Sections 5.1 and 5.2 before going ahead.\n",
    "\n",
    "<font size = 4>Once the cell is executed, you will find a new zip file with the name specified in `trained_model_name.bioimage.io.model` in the model folder specified at the beginning of Section 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4NFacnIC67kc"
   },
   "outputs": [],
   "source": [
    "# User input\n",
    "# ---------------------------------------\n",
    "\n",
    "#@markdown ##Which model checkpoint would you like to export?\n",
    "checkpoints = \"latest\" #@param {type:\"string\"}\n",
    "\n",
    "# Information about the model\n",
    "#@markdown ##Introduce the information to document your model:\n",
    "Trained_model_name = \"Model\" #@param {type:\"string\"}\n",
    "Trained_model_description = \"A conditional cycleGAN trained to infer XXX from XXX \" #@param {type:\"string\"}\n",
    "\n",
    "# Information about the authors, packager, mantainer\n",
    "# @markdown ## Author(s) names and affiliations, comma separated:\n",
    "# @markdown > *NOTICE:* The order and number of affiliations need to be the same as the authors\n",
    "Trained_model_authors = \"Author 1 Name, Author 2 Name, Author 3 Name\" #@param {type:\"string\"}\n",
    "Trained_model_authors_affiliation = \"Author 1 Affiliation, Author 2 Affiliation, Author 3 Affiliation\" #@param {type:\"string\"}\n",
    "\n",
    "# Check that the length of author list and affiliation list are the shame\n",
    "list_auth_names = Trained_model_authors.replace(\" \", \"\").split(\",\") # Remove spaces and split based on comas\n",
    "list_auth_names = [e for e in list_auth_names if e] # Remove empty cases (maybe due to additional final coma)\n",
    "list_auth_affs = Trained_model_authors_affiliation.replace(\" \", \"\").split(\",\") # Remove spaces and split based on comas\n",
    "list_auth_affs = [e for e in list_auth_affs if e] # Remove empty cases (maybe due to additional final coma)\n",
    "assert len(list_auth_names) == len(list_auth_affs)\n",
    "\n",
    "# @markdown ## Model Packager:\n",
    "# @markdown The packager will be taken from the list of authors above. You just need to choose the packager position in the list of authors.\n",
    "# @markdown > *E.g.* 1 for Author 1, 2 for Author 2, etc.\n",
    "Trained_model_packager = 1 #@param {type:\"number\"}\n",
    "\n",
    "# @markdown ## Model Maintainer:\n",
    "\n",
    "# @markdown ### Maintainer's name:\n",
    "# @markdown If maintainers name is **NOT** in author list, please provide here the name:\n",
    "Trained_model_maintainer_name =  \"Maintainer Name\" #@param {type:\"string\"}\n",
    "# @markdown Otherwise, provide the corresponding position number:\n",
    "# @markdown > *E.g.* 1 for Author 1, 2 for Author 2, etc.\n",
    "Trained_model_maintainer_number = 1 #@param {type:\"number\"}\n",
    "\n",
    "# @markdown ### Maintainer's GitHub user:\n",
    "Trained_model_maintainer_github = \"\" #@param {type:\"string\"}\n",
    "\n",
    "# @markdown ## License:\n",
    "Trained_model_license = \"CC-BY-NC-1.0\" #@param [\"0BSD\",\"AAL\",\"Abstyles\",\"AdaCore-doc\",\"Adobe-2006\",\"Adobe-Display-PostScript\",\"Adobe-Glyph\",\"Adobe-Utopia\",\"ADSL\",\"AFL-1.1\",\"AFL-1.2\",\"AFL-2.0\",\"AFL-2.1\",\"AFL-3.0\",\"Afmparse\",\"AGPL-1.0-only\",\"AGPL-1.0-or-later\",\"AGPL-3.0-only\",\"AGPL-3.0-or-later\",\"Aladdin\",\"AMDPLPA\",\"AML\",\"AML-glslang\",\"AMPAS\",\"ANTLR-PD\",\"ANTLR-PD-fallback\",\"Apache-1.0\",\"Apache-1.1\",\"Apache-2.0\",\"APAFML\",\"APL-1.0\",\"App-s2p\",\"APSL-1.0\",\"APSL-1.1\",\"APSL-1.2\",\"APSL-2.0\",\"Arphic-1999\",\"Artistic-1.0\",\"Artistic-1.0-cl8\",\"Artistic-1.0-Perl\",\"Artistic-2.0\",\"ASWF-Digital-Assets-1.0\",\"ASWF-Digital-Assets-1.1\",\"Baekmuk\",\"Bahyph\",\"Barr\",\"bcrypt-Solar-Designer\",\"Beerware\",\"Bitstream-Charter\",\"Bitstream-Vera\",\"BitTorrent-1.0\",\"BitTorrent-1.1\",\"blessing\",\"BlueOak-1.0.0\",\"Boehm-GC\",\"Borceux\",\"Brian-Gladman-2-Clause\",\"Brian-Gladman-3-Clause\",\"BSD-1-Clause\",\"BSD-2-Clause\",\"BSD-2-Clause-Darwin\",\"BSD-2-Clause-Patent\",\"BSD-2-Clause-Views\",\"BSD-3-Clause\",\"BSD-3-Clause-acpica\",\"BSD-3-Clause-Attribution\",\"BSD-3-Clause-Clear\",\"BSD-3-Clause-flex\",\"BSD-3-Clause-HP\",\"BSD-3-Clause-LBNL\",\"BSD-3-Clause-Modification\",\"BSD-3-Clause-No-Military-License\",\"BSD-3-Clause-No-Nuclear-License\",\"BSD-3-Clause-No-Nuclear-License-2014\",\"BSD-3-Clause-No-Nuclear-Warranty\",\"BSD-3-Clause-Open-MPI\",\"BSD-3-Clause-Sun\",\"BSD-4-Clause\",\"BSD-4-Clause-Shortened\",\"BSD-4-Clause-UC\",\"BSD-4.3RENO\",\"BSD-4.3TAHOE\",\"BSD-Advertising-Acknowledgement\",\"BSD-Attribution-HPND-disclaimer\",\"BSD-Inferno-Nettverk\",\"BSD-Protection\",\"BSD-Source-beginning-file\",\"BSD-Source-Code\",\"BSD-Systemics\",\"BSD-Systemics-W3Works\",\"BSL-1.0\",\"BUSL-1.1\",\"bzip2-1.0.6\",\"C-UDA-1.0\",\"CAL-1.0\",\"CAL-1.0-Combined-Work-Exception\",\"Caldera\",\"Caldera-no-preamble\",\"CATOSL-1.1\",\"CC-BY-1.0\",\"CC-BY-2.0\",\"CC-BY-2.5\",\"CC-BY-2.5-AU\",\"CC-BY-3.0\",\"CC-BY-3.0-AT\",\"CC-BY-3.0-AU\",\"CC-BY-3.0-DE\",\"CC-BY-3.0-IGO\",\"CC-BY-3.0-NL\",\"CC-BY-3.0-US\",\"CC-BY-4.0\",\"CC-BY-NC-1.0\",\"CC-BY-NC-2.0\",\"CC-BY-NC-2.5\",\"CC-BY-NC-3.0\",\"CC-BY-NC-3.0-DE\",\"CC-BY-NC-4.0\",\"CC-BY-NC-ND-1.0\",\"CC-BY-NC-ND-2.0\",\"CC-BY-NC-ND-2.5\",\"CC-BY-NC-ND-3.0\",\"CC-BY-NC-ND-3.0-DE\",\"CC-BY-NC-ND-3.0-IGO\",\"CC-BY-NC-ND-4.0\",\"CC-BY-NC-SA-1.0\",\"CC-BY-NC-SA-2.0\",\"CC-BY-NC-SA-2.0-DE\",\"CC-BY-NC-SA-2.0-FR\",\"CC-BY-NC-SA-2.0-UK\",\"CC-BY-NC-SA-2.5\",\"CC-BY-NC-SA-3.0\",\"CC-BY-NC-SA-3.0-DE\",\"CC-BY-NC-SA-3.0-IGO\",\"CC-BY-NC-SA-4.0\",\"CC-BY-ND-1.0\",\"CC-BY-ND-2.0\",\"CC-BY-ND-2.5\",\"CC-BY-ND-3.0\",\"CC-BY-ND-3.0-DE\",\"CC-BY-ND-4.0\",\"CC-BY-SA-1.0\",\"CC-BY-SA-2.0\",\"CC-BY-SA-2.0-UK\",\"CC-BY-SA-2.1-JP\",\"CC-BY-SA-2.5\",\"CC-BY-SA-3.0\",\"CC-BY-SA-3.0-AT\",\"CC-BY-SA-3.0-DE\",\"CC-BY-SA-3.0-IGO\",\"CC-BY-SA-4.0\",\"CC-PDDC\",\"CC0-1.0\",\"CDDL-1.0\",\"CDDL-1.1\",\"CDL-1.0\",\"CDLA-Permissive-1.0\",\"CDLA-Permissive-2.0\",\"CDLA-Sharing-1.0\",\"CECILL-1.0\",\"CECILL-1.1\",\"CECILL-2.0\",\"CECILL-2.1\",\"CECILL-B\",\"CECILL-C\",\"CERN-OHL-1.1\",\"CERN-OHL-1.2\",\"CERN-OHL-P-2.0\",\"CERN-OHL-S-2.0\",\"CERN-OHL-W-2.0\",\"CFITSIO\",\"check-cvs\",\"checkmk\",\"ClArtistic\",\"Clips\",\"CMU-Mach\",\"CMU-Mach-nodoc\",\"CNRI-Jython\",\"CNRI-Python\",\"CNRI-Python-GPL-Compatible\",\"COIL-1.0\",\"Community-Spec-1.0\",\"Condor-1.1\",\"copyleft-next-0.3.0\",\"copyleft-next-0.3.1\",\"Cornell-Lossless-JPEG\",\"CPAL-1.0\",\"CPL-1.0\",\"CPOL-1.02\",\"Cronyx\",\"Crossword\",\"CrystalStacker\",\"CUA-OPL-1.0\",\"Cube\",\"curl\",\"D-FSL-1.0\",\"DEC-3-Clause\",\"diffmark\",\"DL-DE-BY-2.0\",\"DL-DE-ZERO-2.0\",\"DOC\",\"Dotseqn\",\"DRL-1.0\",\"DRL-1.1\",\"DSDP\",\"dtoa\",\"dvipdfm\",\"ECL-1.0\",\"ECL-2.0\",\"EFL-1.0\",\"EFL-2.0\",\"eGenix\",\"Elastic-2.0\",\"Entessa\",\"EPICS\",\"EPL-1.0\",\"EPL-2.0\",\"ErlPL-1.1\",\"etalab-2.0\",\"EUDatagrid\",\"EUPL-1.0\",\"EUPL-1.1\",\"EUPL-1.2\",\"Eurosym\",\"Fair\",\"FBM\",\"FDK-AAC\",\"Ferguson-Twofish\",\"Frameworx-1.0\",\"FreeBSD-DOC\",\"FreeImage\",\"FSFAP\",\"FSFAP-no-warranty-disclaimer\",\"FSFUL\",\"FSFULLR\",\"FSFULLRWD\",\"FTL\",\"Furuseth\",\"fwlw\",\"GCR-docs\",\"GD\",\"GFDL-1.1-invariants-only\",\"GFDL-1.1-invariants-or-later\",\"GFDL-1.1-no-invariants-only\",\"GFDL-1.1-no-invariants-or-later\",\"GFDL-1.1-only\",\"GFDL-1.1-or-later\",\"GFDL-1.2-invariants-only\",\"GFDL-1.2-invariants-or-later\",\"GFDL-1.2-no-invariants-only\",\"GFDL-1.2-no-invariants-or-later\",\"GFDL-1.2-only\",\"GFDL-1.2-or-later\",\"GFDL-1.3-invariants-only\",\"GFDL-1.3-invariants-or-later\",\"GFDL-1.3-no-invariants-only\",\"GFDL-1.3-no-invariants-or-later\",\"GFDL-1.3-only\",\"GFDL-1.3-or-later\",\"Giftware\",\"GL2PS\",\"Glide\",\"Glulxe\",\"GLWTPL\",\"gnuplot\",\"GPL-1.0-only\",\"GPL-1.0-or-later\",\"GPL-2.0-only\",\"GPL-2.0-or-later\",\"GPL-3.0-only\",\"GPL-3.0-or-later\",\"Graphics-Gems\",\"gSOAP-1.3b\",\"gtkbook\",\"HaskellReport\",\"hdparm\",\"Hippocratic-2.1\",\"HP-1986\",\"HP-1989\",\"HPND\",\"HPND-DEC\",\"HPND-doc\",\"HPND-doc-sell\",\"HPND-export-US\",\"HPND-export-US-modify\",\"HPND-Fenneberg-Livingston\",\"HPND-INRIA-IMAG\",\"HPND-Kevlin-Henney\",\"HPND-Markus-Kuhn\",\"HPND-MIT-disclaimer\",\"HPND-Pbmplus\",\"HPND-sell-MIT-disclaimer-xserver\",\"HPND-sell-regexpr\",\"HPND-sell-variant\",\"HPND-sell-variant-MIT-disclaimer\",\"HPND-UC\",\"HTMLTIDY\",\"IBM-pibs\",\"ICU\",\"IEC-Code-Components-EULA\",\"IJG\",\"IJG-short\",\"ImageMagick\",\"iMatix\",\"Imlib2\",\"Info-ZIP\",\"Inner-Net-2.0\",\"Intel\",\"Intel-ACPI\",\"Interbase-1.0\",\"IPA\",\"IPL-1.0\",\"ISC\",\"ISC-Veillard\",\"Jam\",\"JasPer-2.0\",\"JPL-image\",\"JPNIC\",\"JSON\",\"Kastrup\",\"Kazlib\",\"Knuth-CTAN\",\"LAL-1.2\",\"LAL-1.3\",\"Latex2e\",\"Latex2e-translated-notice\",\"Leptonica\",\"LGPL-2.0-only\",\"LGPL-2.0-or-later\",\"LGPL-2.1-only\",\"LGPL-2.1-or-later\",\"LGPL-3.0-only\",\"LGPL-3.0-or-later\",\"LGPLLR\",\"Libpng\",\"libpng-2.0\",\"libselinux-1.0\",\"libtiff\",\"libutil-David-Nugent\",\"LiLiQ-P-1.1\",\"LiLiQ-R-1.1\",\"LiLiQ-Rplus-1.1\",\"Linux-man-pages-1-para\",\"Linux-man-pages-copyleft\",\"Linux-man-pages-copyleft-2-para\",\"Linux-man-pages-copyleft-var\",\"Linux-OpenIB\",\"LOOP\",\"LPD-document\",\"LPL-1.0\",\"LPL-1.02\",\"LPPL-1.0\",\"LPPL-1.1\",\"LPPL-1.2\",\"LPPL-1.3a\",\"LPPL-1.3c\",\"lsof\",\"Lucida-Bitmap-Fonts\",\"LZMA-SDK-9.11-to-9.20\",\"LZMA-SDK-9.22\",\"Mackerras-3-Clause\",\"Mackerras-3-Clause-acknowledgment\",\"magaz\",\"mailprio\",\"MakeIndex\",\"Martin-Birgmeier\",\"McPhee-slideshow\",\"metamail\",\"Minpack\",\"MirOS\",\"MIT\",\"MIT-0\",\"MIT-advertising\",\"MIT-CMU\",\"MIT-enna\",\"MIT-feh\",\"MIT-Festival\",\"MIT-Modern-Variant\",\"MIT-open-group\",\"MIT-testregex\",\"MIT-Wu\",\"MITNFA\",\"MMIXware\",\"Motosoto\",\"MPEG-SSG\",\"mpi-permissive\",\"mpich2\",\"MPL-1.0\",\"MPL-1.1\",\"MPL-2.0\",\"MPL-2.0-no-copyleft-exception\",\"mplus\",\"MS-LPL\",\"MS-PL\",\"MS-RL\",\"MTLL\",\"MulanPSL-1.0\",\"MulanPSL-2.0\",\"Multics\",\"Mup\",\"NAIST-2003\",\"NASA-1.3\",\"Naumen\",\"NBPL-1.0\",\"NCGL-UK-2.0\",\"NCSA\",\"Net-SNMP\",\"NetCDF\",\"Newsletr\",\"NGPL\",\"NICTA-1.0\",\"NIST-PD\",\"NIST-PD-fallback\",\"NIST-Software\",\"NLOD-1.0\",\"NLOD-2.0\",\"NLPL\",\"Nokia\",\"NOSL\",\"Noweb\",\"NPL-1.0\",\"NPL-1.1\",\"NPOSL-3.0\",\"NRL\",\"NTP\",\"NTP-0\",\"O-UDA-1.0\",\"OCCT-PL\",\"OCLC-2.0\",\"ODbL-1.0\",\"ODC-By-1.0\",\"OFFIS\",\"OFL-1.0\",\"OFL-1.0-no-RFN\",\"OFL-1.0-RFN\",\"OFL-1.1\",\"OFL-1.1-no-RFN\",\"OFL-1.1-RFN\",\"OGC-1.0\",\"OGDL-Taiwan-1.0\",\"OGL-Canada-2.0\",\"OGL-UK-1.0\",\"OGL-UK-2.0\",\"OGL-UK-3.0\",\"OGTSL\",\"OLDAP-1.1\",\"OLDAP-1.2\",\"OLDAP-1.3\",\"OLDAP-1.4\",\"OLDAP-2.0\",\"OLDAP-2.0.1\",\"OLDAP-2.1\",\"OLDAP-2.2\",\"OLDAP-2.2.1\",\"OLDAP-2.2.2\",\"OLDAP-2.3\",\"OLDAP-2.4\",\"OLDAP-2.5\",\"OLDAP-2.6\",\"OLDAP-2.7\",\"OLDAP-2.8\",\"OLFL-1.3\",\"OML\",\"OpenPBS-2.3\",\"OpenSSL\",\"OpenSSL-standalone\",\"OpenVision\",\"OPL-1.0\",\"OPL-UK-3.0\",\"OPUBL-1.0\",\"OSET-PL-2.1\",\"OSL-1.0\",\"OSL-1.1\",\"OSL-2.0\",\"OSL-2.1\",\"OSL-3.0\",\"PADL\",\"Parity-6.0.0\",\"Parity-7.0.0\",\"PDDL-1.0\",\"PHP-3.0\",\"PHP-3.01\",\"Pixar\",\"Plexus\",\"pnmstitch\",\"PolyForm-Noncommercial-1.0.0\",\"PolyForm-Small-Business-1.0.0\",\"PostgreSQL\",\"PSF-2.0\",\"psfrag\",\"psutils\",\"Python-2.0\",\"Python-2.0.1\",\"python-ldap\",\"Qhull\",\"QPL-1.0\",\"QPL-1.0-INRIA-2004\",\"radvd\",\"Rdisc\",\"RHeCos-1.1\",\"RPL-1.1\",\"RPL-1.5\",\"RPSL-1.0\",\"RSA-MD\",\"RSCPL\",\"Ruby\",\"SAX-PD\",\"SAX-PD-2.0\",\"Saxpath\",\"SCEA\",\"SchemeReport\",\"Sendmail\",\"Sendmail-8.23\",\"SGI-B-1.0\",\"SGI-B-1.1\",\"SGI-B-2.0\",\"SGI-OpenGL\",\"SGP4\",\"SHL-0.5\",\"SHL-0.51\",\"SimPL-2.0\",\"SISSL\",\"SISSL-1.2\",\"SL\",\"Sleepycat\",\"SMLNJ\",\"SMPPL\",\"SNIA\",\"snprintf\",\"softSurfer\",\"Soundex\",\"Spencer-86\",\"Spencer-94\",\"Spencer-99\",\"SPL-1.0\",\"ssh-keyscan\",\"SSH-OpenSSH\",\"SSH-short\",\"SSLeay-standalone\",\"SSPL-1.0\",\"SugarCRM-1.1.3\",\"Sun-PPP\",\"SunPro\",\"SWL\",\"swrule\",\"Symlinks\",\"TAPR-OHL-1.0\",\"TCL\",\"TCP-wrappers\",\"TermReadKey\",\"TGPPL-1.0\",\"TMate\",\"TORQUE-1.1\",\"TOSL\",\"TPDL\",\"TPL-1.0\",\"TTWL\",\"TTYP0\",\"TU-Berlin-1.0\",\"TU-Berlin-2.0\",\"UCAR\",\"UCL-1.0\",\"ulem\",\"UMich-Merit\",\"Unicode-3.0\",\"Unicode-DFS-2015\",\"Unicode-DFS-2016\",\"Unicode-TOU\",\"UnixCrypt\",\"Unlicense\",\"UPL-1.0\",\"URT-RLE\",\"Vim\",\"VOSTROM\",\"VSL-1.0\",\"W3C\",\"W3C-19980720\",\"W3C-20150513\",\"w3m\",\"Watcom-1.0\",\"Widget-Workshop\",\"Wsuipa\",\"WTFPL\",\"X11\",\"X11-distribute-modifications-variant\",\"Xdebug-1.03\",\"Xerox\",\"Xfig\",\"XFree86-1.1\",\"xinetd\",\"xkeyboard-config-Zinoviev\",\"xlock\",\"Xnet\",\"xpp\",\"XSkat\",\"YPL-1.0\",\"YPL-1.1\",\"Zed\",\"Zeeff\",\"Zend-2.0\",\"Zimbra-1.3\",\"Zimbra-1.4\",\"Zlib\",\"zlib-acknowledgement\",\"ZPL-1.1\",\"ZPL-2.0\",\"ZPL-2.1\"]\n",
    "\n",
    "# References\n",
    "# ---------------------------------------\n",
    "\n",
    "# References and their corresponding DOI's are already provided\n",
    "Trained_model_references = [\"Isola et al. arXiv in 2016\",\n",
    "                            \"Lucas von Chamier et al. biorXiv 2020\"]\n",
    "Trained_model_links = [\"https://arxiv.org/abs/1611.07004\",\n",
    "                     \"https://doi.org/10.1101/2020.03.20.000133\"]\n",
    "assert len(Trained_model_links) == len(Trained_model_references)\n",
    "\n",
    "citations = []\n",
    "for text, link in zip(Trained_model_references, Trained_model_links):\n",
    "  if 'https://doi.org/' in link:\n",
    "    citations.append(bioimageio_spec.CiteEntry(text=text, doi=link.replace('https://doi.org/', '')))\n",
    "  else:\n",
    "    citations.append(bioimageio_spec.CiteEntry(text=text, url=link))\n",
    "\n",
    "# Prepare the author/maintainer/packager\n",
    "# ---------------------------------------\n",
    "\n",
    "author_name_list = [e.strip() for e in Trained_model_authors.split(',')]\n",
    "author_affiliation_list = [e.strip() for e in Trained_model_authors_affiliation.split(',')]\n",
    "\n",
    "authors = []\n",
    "for author, affiliation in zip(author_name_list, author_affiliation_list):\n",
    "  authors.append(bioimageio_spec.Author(name=author, affiliation=affiliation))\n",
    "\n",
    "assert len(author_name_list) >= Trained_model_packager, \"Author list has less authors than the packager number inserted.\"\n",
    "packager = [authors[Trained_model_packager-1]]\n",
    "\n",
    "if Trained_model_maintainer_name != \"Maintainer Name\":\n",
    "  # The user specified a new person as a maintainer\n",
    "  maintainer = [bioimageio_spec.Maintainer(github_user=Trained_model_maintainer_github,\n",
    "                                           name=Trained_model_maintainer_name)]\n",
    "else:\n",
    "  # The user chose an author as a maintainer\n",
    "  assert len(author_name_list) >= Trained_model_maintainer_number, \"Author list has less authors than the maintainer number inserted, no maintainer name will be added.\"\n",
    "  maintainer = [bioimageio_spec.Maintainer(github_user=Trained_model_maintainer_github,\n",
    "                                           name=author_name_list[Trained_model_maintainer_number-1],\n",
    "                                           affiliation=author_affiliation_list[Trained_model_maintainer_number-1])]\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# Training data\n",
    "# ---------------------------------------\n",
    "\n",
    "#@markdown ## Include information about training data (optional):\n",
    "Include_training_data = False #@param {type: \"boolean\"}\n",
    "#@markdown ### - If it is published in the BioImage Model Zoo, please, provide the ID (e.g., `zero/dataset_fnet_3d_zerocostdl4mic`)\n",
    "Data_from_bioimage_model_zoo = False #@param {type: \"boolean\"}\n",
    "Training_data_ID = ''#@param {type:\"string\"}\n",
    "#@markdown ### - If not, please provide the URL tot he data and a short description\n",
    "Training_data_source = ''#@param {type:\"string\"}\n",
    "Training_data_description = ''#@param {type:\"string\"}\n",
    "\n",
    "# Create the training data\n",
    "if Include_training_data:\n",
    "    if Data_from_bioimage_model_zoo:\n",
    "      training_data = bioimageio.spec.LinkedDataset(id=Training_data_ID)\n",
    "    else:\n",
    "      training_data = bioimageio_spec.DatasetDescr(source=Training_data_source,\n",
    "                                                   description=Training_data_description)\n",
    "else:\n",
    "    training_data=None\n",
    "\n",
    "# Add example image information\n",
    "# ---------------------------------------\n",
    "\n",
    "#@markdown ##Include and example image to test the model:\n",
    "\n",
    "Default_example_image_from_QC = True #@param {type:\"boolean\"}\n",
    "#@markdown ###If not, please input:\n",
    "Test_input_path =  \"\" #@param {type:\"string\"}\n",
    "#@markdown ###Pixel size (in microns) of the example image:\n",
    "# information about the example image\n",
    "PixelSize = 1 #@param {type:\"number\"}\n",
    "\n",
    "if Default_example_image_from_QC:\n",
    "    Test_input_path = os.path.join(Source_QC_folder, sorted(os.listdir(Source_QC_folder))[0])\n",
    "\n",
    "# Output path\n",
    "# ---------------------------------------\n",
    "\n",
    "# Where to save the model\n",
    "output_root = os.path.join(full_QC_model_path, 'bioimage.io.model')\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "output_path = os.path.join(output_root, f\"{Trained_model_name}.zip\")\n",
    "\n",
    "# Attach the QC report to the model (if it exists)\n",
    "# ---------------------------------------\n",
    "\n",
    "attachments = []\n",
    "qc_path = os.path.join(full_QC_model_path, 'Quality Control', 'training_evaluation.csv')\n",
    "if os.path.exists(qc_path):\n",
    "  attachments.append(FileDescr(source = qc_path))\n",
    "\n",
    "# Preprocessing\n",
    "# ---------------------------------------\n",
    "\n",
    "bmz_preprocess = [bioimageio_spec.EnsureDtypeDescr(kwargs=bioimageio_spec.EnsureDtypeKwargs(dtype=\"float32\"))]\n",
    "\n",
    "# Preprocessing when there is no normalization\n",
    "bmz_preprocess.append(bioimageio_spec.ScaleRangeDescr(kwargs=bioimageio_spec.ScaleRangeKwargs(axes = ('x', 'y'),\n",
    "                                                                                              min_percentile = 0. ,\n",
    "                                                                                              max_percentile = 100.)))\n",
    "bmz_preprocess.append(bioimageio_spec.ScaleLinearDescr(kwargs=bioimageio_spec.ScaleLinearKwargs(gain = 2 ,\n",
    "                                                                                                offset= -1)))\n",
    "# Postprocessing\n",
    "# ---------------------------------------\n",
    "\n",
    "#bmz_postprocess = [[{\"name\": \"scale_linear\", \"kwargs\": {\"gain\": 0.5,\n",
    "                                                        #\"offset\": 0.5,\n",
    "                                                        #\"axes\": \"xy\"}}]]\n",
    "bmz_postprocess = [bioimageio_spec.EnsureDtypeDescr(kwargs=bioimageio_spec.EnsureDtypeKwargs(dtype=\"float32\"))]\n",
    "\n",
    "# Test input image\n",
    "# ---------------------------------------\n",
    "\n",
    "test_img = imageio.imread(Test_input_path, pilmode=\"RGB\", mode=\"F\")\n",
    "\n",
    "# We crop an image of a shape that can be processed with pix2pix without tiling\n",
    "# x may be a 3 channel image. we assume that x,y have the highest dimensions\n",
    "patch_size_QC = np.min([np.int32(np.floor(i/512) * 512) for i in test_img.shape[:2]])\n",
    "\n",
    "print(f\"The test image is cropped to a size of {patch_size_QC}x{patch_size_QC}\")\n",
    "test_img = test_img[:patch_size_QC, :patch_size_QC]\n",
    "\n",
    "# We check if the image has 3 axes, which woul mean that there is a channel axis\n",
    "if len(test_img.shape) == 3:\n",
    "  # Then, assuming that the highest dimensions are x and y, we will check if it\n",
    "  # follows the 'cxy' format and transform it otherwise into that format.\n",
    "  smaller_dim = min(test_img.shape)\n",
    "  smaller_dim_idx = test_img.shape.index(smaller_dim)\n",
    "\n",
    "  if smaller_dim_idx != 0:\n",
    "    test_img = np.moveaxis(test_img, source=smaller_dim_idx, destination=0)\n",
    "\n",
    "test_img = np.expand_dims(test_img, axis = 0) # add batch dimension\n",
    "\n",
    "test_input_numpy_path = os.path.join(output_root, \"test_input.npy\")\n",
    "test_output_numpy_path = os.path.join(base_path, \"temp_test_output.npy\")\n",
    "\n",
    "np.save(test_input_numpy_path, test_img.astype(np.float32))\n",
    "np.save(test_output_numpy_path, test_img.astype(np.float32))\n",
    "\n",
    "# Input & output specs\n",
    "# ---------------------------------------\n",
    "\n",
    "# Create the channel names for the output\n",
    "channel_names = [f'channel{idx}' for idx in range(3)]\n",
    "\n",
    "# Create the input tensor\n",
    "input_tensor_axes = [bioimageio_spec.BatchAxis(id='batch',\n",
    "                                               description='',\n",
    "                                               type='batch',\n",
    "                                               size=None),\n",
    "                     bioimageio_spec.ChannelAxis(id='channel',\n",
    "                                                 description='',\n",
    "                                                 type='channel',\n",
    "                                                 channel_names=channel_names),\n",
    "                     bioimageio_spec.SpaceInputAxis(id='y',\n",
    "                                                    description='',\n",
    "                                                    type='space',\n",
    "                                                    size=test_img.shape[-2],\n",
    "                                                    unit='micrometer',\n",
    "                                                    scale=PixelSize,\n",
    "                                                    concatenable=False),\n",
    "                     bioimageio_spec.SpaceInputAxis(id='x',\n",
    "                                                    description='',\n",
    "                                                    type='space',\n",
    "                                                    size=test_img.shape[-1],\n",
    "                                                    unit='micrometer',\n",
    "                                                    scale=PixelSize,\n",
    "                                                    concatenable=False)\n",
    "                    ]\n",
    "input_tensor = bioimageio_spec.InputTensorDescr(id=bioimageio_spec.TensorId('input0'),\n",
    "                                                description= 'This is the test input tensor created from the example image.',\n",
    "                                                axes=input_tensor_axes,\n",
    "                                                test_tensor = bioimageio_spec.FileDescr(source = test_input_numpy_path),\n",
    "                                                preprocessing = bmz_preprocess\n",
    "                                                )\n",
    "\n",
    "# Create the output tensor\n",
    "output_tensor_axes = [bioimageio_spec.BatchAxis(id='batch',\n",
    "                                                description='',\n",
    "                                                type='batch',\n",
    "                                                size=None),\n",
    "                      bioimageio_spec.ChannelAxis(id='channel',\n",
    "                                                  description='',\n",
    "                                                  type='channel',\n",
    "                                                  channel_names=channel_names),\n",
    "                      bioimageio_spec.SpaceOutputAxis(id='y',\n",
    "                                                      description='',\n",
    "                                                      type='space',\n",
    "                                                      unit='micrometer',\n",
    "                                                      scale=1.0, # consider changing it if the input has more than one channel\n",
    "                                                      size=bioimageio_spec.SizeReference(tensor_id=bioimageio_spec.TensorId('input0'),\n",
    "                                                                                          axis_id='y',\n",
    "                                                                                          offset=0)),\n",
    "                      bioimageio_spec.SpaceOutputAxis(id='x',\n",
    "                                                      description='',\n",
    "                                                      type='space',\n",
    "                                                      unit='micrometer',\n",
    "                                                      scale=1.0, # consider changing it if the input has more than one channel\n",
    "                                                      size=bioimageio_spec.SizeReference(tensor_id=bioimageio_spec.TensorId('input0'),\n",
    "                                                                                          axis_id='x',\n",
    "                                                                                          offset=0), )\n",
    "                      ]\n",
    "\n",
    "output_tensor = bioimageio_spec.OutputTensorDescr(id=bioimageio_spec.TensorId(\"output0\"),\n",
    "                                                  axes=output_tensor_axes,\n",
    "                                                  test_tensor = bioimageio_spec.FileDescr(source = test_output_numpy_path),\n",
    "                                                  postprocessing = bmz_postprocess,\n",
    "                                                  )\n",
    "\n",
    "# Create the cover\n",
    "# ---------------------------------------\n",
    "\n",
    "cover_path = Test_input_path\n",
    "\n",
    "# Create a markdown README with info\n",
    "# ---------------------------------------\n",
    "readme_path = os.path.join(output_root, \"README.md\")\n",
    "with open(readme_path, \"w\") as f:\n",
    "  f.write(\"Visit https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
    "\n",
    "# Weights\n",
    "# ---------------------------------------\n",
    "\n",
    "# Export torchscript\n",
    "path_model_checkpoint = os.path.join(output_root, f'{checkpoints}_net_G_torchscript.pt')\n",
    "export_cyclegan_torchscript_model(path_model_checkpoint)\n",
    "\n",
    "!python3 pytorch-CycleGAN-and-pix2pix/cyclegan_model_export.py \\\n",
    "          --dataroot \"$base_path\" --name \"$QC_model_name\" --model \"cycle_gan\" \\\n",
    "          --epoch \"$checkpoints\" --no_dropout --preprocess \"scale_width\" \\\n",
    "          --load_size \"$patch_size_QC\" --crop_size \"$patch_size_QC\" \\\n",
    "          --results_dir \"$QC_prediction_results\" --checkpoints_dir \"$QC_model_path\" \\\n",
    "          --num_test \"$Nb_files_Data_folder\" --input_nc 3 --output_nc 3 \\\n",
    "          --dataset_mode \"aligned\" --gpu_ids $gpu_available\n",
    "\n",
    "torchscript_weights_descriptor = bioimageio_spec.TorchscriptWeightsDescr(source=path_model_checkpoint,\n",
    "                                                                         pytorch_version=torch.__version__.split('+')[0]) # Remove +cu...\n",
    "weights_descriptor = bioimageio_spec.WeightsDescr(torchscript=torchscript_weights_descriptor)\n",
    "\n",
    "# Build the bioimage model zoo model\n",
    "# ---------------------------------------\n",
    "\n",
    "for i in range(2):\n",
    "  # We create the model, process the input image and create the model again with the correct output.\n",
    "  model_description = bioimageio_spec.ModelDescr(name=Trained_model_name,\n",
    "                                                 description=Trained_model_description,\n",
    "                                                 covers=[cover_path],\n",
    "                                                 authors=authors,\n",
    "                                                 attachments=attachments,\n",
    "                                                 cite=citations,\n",
    "                                                 license=Trained_model_license,\n",
    "                                                 maintainers=maintainer,\n",
    "                                                 packaged_by=packager,\n",
    "                                                 tags=[\"in-silico-labeling\",\"pytorch\", \"cyclegan\", \"conditional-gan\",\n",
    "                                                        \"zerocostdl4mic\", \"deepimagej\", \"actin\", \"dapi\", \"cells\", \"nuclei\",\n",
    "                                                        \"fluorescence-light-microscopy\", \"2d\"],  # the tags are used to make models more findable on the website,\n",
    "                                                 documentation=readme_path,\n",
    "                                                 inputs=[input_tensor],\n",
    "                                                 outputs=[output_tensor],\n",
    "                                                 weights=weights_descriptor,\n",
    "                                                 training_data=training_data,\n",
    "                                                 links=[]\n",
    "                                                )\n",
    "  if i == 0:\n",
    "    # Define the new input sample (taken from the new model description)\n",
    "    new_input_paths = {ipt.id: download(ipt.test_tensor).path for ipt in model_description.inputs}\n",
    "\n",
    "    # The prediction pipeline expects a Sample object from bioimageio.core\n",
    "    input_sample = create_sample_for_model(\n",
    "        model=model_description, inputs=new_input_paths, sample_id=\"my_demo_sample\"\n",
    "    )\n",
    "\n",
    "    # Create the new prediction\n",
    "    prediction = bioimageio_core.predict(model=model_description, inputs=input_sample)\n",
    "\n",
    "    # Save the new prediction on a NumPy file\n",
    "    new_output_path = os.path.join(output_root, \"test_output.npy\")\n",
    "\n",
    "    # Delete the file if exists to avoid SHA256 mismatch\n",
    "    if os.path.exists(test_output_numpy_path):\n",
    "      os.remove(test_output_numpy_path)\n",
    "\n",
    "    prediction_tensor = prediction.members[\"output0\"].data\n",
    "    np.save(new_output_path, prediction_tensor)\n",
    "\n",
    "    # Create the output tensor description\n",
    "\n",
    "    output_tensor = bioimageio_spec.OutputTensorDescr(id=bioimageio_spec.TensorId(\"output0\"),\n",
    "                                                      axes=output_tensor_axes,\n",
    "                                                      test_tensor = bioimageio_spec.FileDescr(source = new_output_path),\n",
    "                                                      postprocessing = bmz_postprocess,\n",
    "                                                     )\n",
    "\n",
    "    # Clean first model\n",
    "    del model_description\n",
    "\n",
    "# Test the model\n",
    "# ---------------------------------------\n",
    "\n",
    "# Using the validation context, it will reset the know files\n",
    "# Avoiding the SHA256 mistmach errors\n",
    "with bioimageio.spec.ValidationContext():\n",
    "  summary = bioimageio_core.test_model(model_description)\n",
    "  summary.display()\n",
    "\n",
    "  success = summary.status == \"passed\"\n",
    "\n",
    "success = True\n",
    "\n",
    "if success:\n",
    "  with bioimageio.spec.ValidationContext():\n",
    "    # In case it has passed the test, save the bioimage.io model with the correct format\n",
    "    save_bioimageio_package(model_description, output_path=Path(output_path))\n",
    "    print(\"\\nThe bioimage.io model was successfully exported to\", output_path)\n",
    "else:\n",
    "  print(\"\\nThe bioimage.io model was exported to\", output_path)\n",
    "  print(\"Some tests of the model did not work! You can still download and test the model.\")\n",
    "  print(\"You can still download and test the model, but it may not work as expected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tJeeJjLnRkP"
   },
   "source": [
    "# **6. Using the trained model**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8wuQGjoq6eN"
   },
   "source": [
    "## **6.1. Generate prediction(s) from unseen dataset**\n",
    "---\n",
    "\n",
    "<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as PNG images.\n",
    "\n",
    "<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n",
    "\n",
    "<font size = 4>**`Result_folder`:** This folder will contain the predicted output images.\n",
    "\n",
    "<font size = 4>**`checkpoint`:** Choose the checkpoint number you would like to use to perform predictions. To use the \"latest\" checkpoint, input \"latest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "y2TD5p7MZrEb"
   },
   "outputs": [],
   "source": [
    "#@markdown ### Provide the path to your dataset and to the folder where the predictions are saved, then play the cell to predict outputs from your unseen images.\n",
    "\n",
    "latest = \"latest\"\n",
    "\n",
    "Data_folder = \"\" #@param {type:\"string\"}\n",
    "Result_folder = \"\" #@param {type:\"string\"}\n",
    "\n",
    "# model name and path\n",
    "#@markdown ###Do you want to use the current trained model?\n",
    "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ###If not, please provide the path to the model folder:\n",
    "\n",
    "Prediction_model_folder = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ###What model checkpoint would you like to use?\n",
    "\n",
    "checkpoint = latest#@param {type:\"raw\"}\n",
    "\n",
    "#Here we find the loaded model name and parent path\n",
    "Prediction_model_name = os.path.basename(Prediction_model_folder)\n",
    "Prediction_model_path = os.path.dirname(Prediction_model_folder)\n",
    "\n",
    "#here we check if we use the newly trained network or not\n",
    "if (Use_the_current_trained_model):\n",
    "  print(\"Using current trained network.\")\n",
    "  Prediction_model_name = model_name\n",
    "  Prediction_model_path = model_path\n",
    "\n",
    "#here we check if the model exists\n",
    "full_Prediction_model_path = os.path.join(Prediction_model_path, Prediction_model_name)\n",
    "\n",
    "if os.path.exists(full_Prediction_model_path):\n",
    "  print(f\"The {Prediction_model_name} network will be used.\")\n",
    "else:\n",
    "  print(f\"{bcolors.WARNING}!! WARNING: The chosen model does not exist !!{bcolors.ENDC}\")\n",
    "  print('Please make sure you provide a valid model path and model name before proceeding further.')\n",
    "\n",
    "# Here we check that checkpoint exist, if not the closest one will be chosen\n",
    "\n",
    "Nb_Checkpoint = len(glob(os.path.join(full_Prediction_model_path, '*G_A.pth')))\n",
    "\n",
    "if not checkpoint == \"latest\":\n",
    "\n",
    "  if checkpoint < 10:\n",
    "    checkpoint = 5\n",
    "\n",
    "  if not checkpoint % 5 == 0:\n",
    "    checkpoint = ((int(checkpoint / 5)-1) * 5)\n",
    "    print (bcolors.WARNING + \" Your chosen checkpoints is not divisible by 5; therefore the checkpoints chosen is now:\",checkpoints)\n",
    "\n",
    "  if checkpoint > Nb_Checkpoint*5:\n",
    "    checkpoint = \"latest\"\n",
    "\n",
    "  if checkpoint == Nb_Checkpoint*5:\n",
    "    checkpoint = \"latest\"\n",
    "\n",
    "\n",
    "# Here we need to move the data to be analysed so that cycleGAN can find them\n",
    "\n",
    "Saving_path_prediction = os.path.join(base_path, Prediction_model_name)\n",
    "\n",
    "if os.path.exists(Saving_path_prediction):\n",
    "  shutil.rmtree(Saving_path_prediction)\n",
    "os.makedirs(Saving_path_prediction)\n",
    "\n",
    "Saving_path_Data_folder = os.path.join(Saving_path_prediction, \"testA\")\n",
    "\n",
    "if os.path.exists(Saving_path_Data_folder):\n",
    "  shutil.rmtree(Saving_path_Data_folder)\n",
    "os.makedirs(Saving_path_Data_folder)\n",
    "\n",
    "for files in os.listdir(Data_folder):\n",
    "    shutil.copyfile(os.path.join(Data_folder, files), os.path.join(Saving_path_Data_folder, files))\n",
    "\n",
    "Nb_files_Data_folder = len(os.listdir(Data_folder)) +10\n",
    "\n",
    "#Here we copy and rename the checkpoint to be used\n",
    "\n",
    "shutil.copyfile(os.path.join(full_Prediction_model_path, str(checkpoint)+\"_net_G_A.pth\"),\n",
    "                os.path.join(full_Prediction_model_path, str(checkpoint)+\"_net_G.pth\"))\n",
    "\n",
    "# This will find the image dimension of a randomly choosen image in Data_folder\n",
    "random_choice = random.choice(os.listdir(Data_folder))\n",
    "x = imageio.imread(os.path.join(Data_folder, random_choice))\n",
    "\n",
    "#Find image XY dimension\n",
    "Image_Y = x.shape[0]\n",
    "Image_X = x.shape[1]\n",
    "\n",
    "Image_min_dim = min(Image_Y, Image_X)\n",
    "\n",
    "#-------------------------------- Perform predictions -----------------------------\n",
    "\n",
    "#-------------------------------- Options that can be used to perform predictions -----------------------------\n",
    "\n",
    "# basic parameters\n",
    "        #('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n",
    "        #('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n",
    "        #('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
    "        #('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
    "\n",
    "# model parameters\n",
    "        #('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n",
    "        #('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n",
    "        #('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n",
    "        #('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n",
    "        #('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n",
    "        #('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n",
    "        #('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n",
    "        #('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n",
    "        #('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n",
    "        #('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n",
    "        #('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
    "        #('--no_dropout', action='store_true', help='no dropout for the generator')\n",
    "\n",
    "# dataset parameters\n",
    "        #('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n",
    "        #('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n",
    "        #('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
    "        #('--num_threads', default=4, type=int, help='# threads for loading data')\n",
    "        #('--batch_size', type=int, default=1, help='input batch size')\n",
    "        #('--load_size', type=int, default=286, help='scale images to this size')\n",
    "        #('--crop_size', type=int, default=256, help='then crop to this size')\n",
    "        #('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
    "        #('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n",
    "        #('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
    "        #('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n",
    "\n",
    "# additional parameters\n",
    "        #('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
    "        #('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n",
    "        #('--verbose', action='store_true', help='if specified, print more debugging information')\n",
    "        #('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n",
    "\n",
    "\n",
    "        #('--ntest', type=int, default=float(\"inf\"), help='# of test examples.')\n",
    "        #('--results_dir', type=str, default='./results/', help='saves results here.')\n",
    "        #('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n",
    "        #('--phase', type=str, default='test', help='train, val, test, etc')\n",
    "\n",
    "# Dropout and Batchnorm has different behavioir during training and test.\n",
    "        #('--eval', action='store_true', help='use eval mode during test time.')\n",
    "        #('--num_test', type=int, default=50, help='how many test images to run')\n",
    "        # rewrite devalue values\n",
    "\n",
    "# To avoid cropping, the load_size should be the same as crop_size\n",
    "        #parser.set_defaults(load_size=parser.get_default('crop_size'))\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Get if GPU is available\n",
    "gpu_available = !if type nvidia-smi >/dev/null 2>&1; then echo 0; else echo -1; fi;\n",
    "gpu_available = str(gpu_available[0])\n",
    "\n",
    "#---------------------------- Predictions are performed here ----------------------\n",
    "\n",
    "!python pytorch-CycleGAN-and-pix2pix/test.py --dataroot \"$Saving_path_Data_folder\" --name \"$Prediction_model_name\" --model test --no_dropout --preprocess scale_width --load_size $Image_min_dim --crop_size $Image_min_dim --results_dir \"$Result_folder\" --checkpoints_dir \"$Prediction_model_path\" --num_test $Nb_files_Data_folder --epoch $checkpoint --gpu_ids $gpu_available\n",
    "\n",
    "#-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXqS_EhByhQ7"
   },
   "source": [
    "## **6.2. Inspect the predicted output**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "64emoATwylxM"
   },
   "outputs": [],
   "source": [
    "# @markdown ##Run this cell to display a randomly chosen input and its corresponding predicted output.\n",
    "import os\n",
    "# This will display a randomly chosen dataset input and predicted output\n",
    "random_choice = random.choice(os.listdir(Data_folder))\n",
    "\n",
    "\n",
    "random_choice_no_extension = os.path.splitext(random_choice)\n",
    "\n",
    "\n",
    "x = imageio.imread(os.path.join(Result_folder, Prediction_model_name, \"test_\"+str(checkpoint), \n",
    "                                \"images\", random_choice_no_extension[0]+\"_real.png\"))\n",
    "\n",
    "\n",
    "y = imageio.imread(os.path.join(Result_folder, Prediction_model_name, \"test_\"+str(checkpoint), \n",
    "                                \"images\", random_choice_no_extension[0]+\"_fake.png\"))\n",
    "\n",
    "f=plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x, interpolation='nearest')\n",
    "plt.title('Input')\n",
    "plt.axis('off');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(y, interpolation='nearest')\n",
    "plt.title('Prediction')\n",
    "plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvkd66PldsXB"
   },
   "source": [
    "## **6.3. Download your predictions**\n",
    "---\n",
    "\n",
    "<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE8vQZ7RWY_L"
   },
   "source": [
    "# **7. Version log**\n",
    "---\n",
    "\n",
    "<font size = 4>**v1.14.1**:\n",
    "\n",
    "*   Update section 5.3. to export the model on the BioImage Model Zoo format.\n",
    "*   Fix the requirements installation version.\n",
    "*   Update some outdated functions (from imageio, scikit-image or fpdf2).\n",
    "*   Do some general code cleaning: remove unused libraries, change to os.path.join, short the code, etc.\n",
    "\n",
    "<font size = 4>**v1.13.3**:\n",
    "\n",
    "*   Change LICENSE code cell for a documentation cell.\n",
    "\n",
    "<font size = 4>**v1.13.2**:\n",
    "\n",
    "*    Replaced all absolute pathing with relative pathing.\n",
    "\n",
    "<font size = 4>**v1.13**:  \n",
    "\n",
    "*   This version now includes an automatic restart allowing to set the h5py library to v2.10.\n",
    "*   The section 1 and 2 are now swapped for better export of *requirements.txt*.\n",
    "\n",
    "*   This version also now includes built-in version check and the version log that you're reading now.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvSlTaH14s3t"
   },
   "source": [
    "# **Thank you for using CycleGAN!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
