{"cells":[{"cell_type":"markdown","metadata":{"id":"YrTo6T74i7s0"},"source":["# **MaskRCNN**\n","\n","---\n","\n","<font size = 4> This notebook is an implementation of MaskRCNN. This neural network performs instance segmentation. This means it can be used to detect objects in images, segment these objects and classify them. This notebook is based on the work of [He et al.](https://arxiv.org/abs/1703.06870)\n","\n","---\n","\n","<font size = 4>*Disclaimer*:\n","\n","<font size = 4>This notebook is part of the *Zero-Cost Deep-Learning to Enhance Microscopy* project (ZeroCostDL4Mic) (https://github.com/HenriquesLab/DeepLearning_Collab/wiki)\n","\n","<font size = 4>This notebook is based on the following paper: \n","\n","<font size = 4>**Mask R-CNN**, arxiv, 2018 by Kaiming He, Georgia Gkioxari, Piotr Doll√°r, Ross Girshick [here](https://arxiv.org/abs/1703.06870)\n","\n","<font size = 4>And source code found in: *https://github.com/matterport/Mask_RCNN*\n","\n","<font size = 4>Provide information on dataset availability and link for download if applicable.\n","\n","\n","<font size = 4>**Please also cite this original paper when using or developing this notebook.**"]},{"cell_type":"markdown","metadata":{"id":"RZL8pqcEi0KY"},"source":["# **How to use this notebook?**\n","\n","---\n","\n","<font size = 4>Video describing how to use ZeroCostDL4Mic notebooks are available on youtube:\n","  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n","  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n","\n","\n","---\n","### **Structure of a notebook**\n","\n","<font size = 4>The notebook contains two types of cell:  \n","\n","<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n","\n","<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n","\n","---\n","### **Table of contents, Code snippets** and **Files**\n","\n","<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n","\n","<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n","\n","<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n","\n","<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here. \n","\n","<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n","\n","<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n","\n","---\n","### **Making changes to the notebook**\n","\n","<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n","\n","<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n","You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."]},{"cell_type":"markdown","metadata":{"id":"3yywetML0lUX"},"source":["# **0. Before getting started**\n","---\n","<font size = 4>**We strongly recommend that you generate extra paired images. These images can be used to assess the quality of your trained model (Quality control dataset)**. The quality control assessment can be done directly in this notebook.\n","\n","<font size = 4> **Additionally, the corresponding input and output files need to have the same name**.\n","\n","<font size = 4> Please note that while the file format is flexible (.tif, .png, .jpeg should all work) but these currently **must be of RGB** type.\n","\n","<font size = 4>Here's the data structure that you should use:\n","*   Experiment A\n","    - **Training dataset**\n","      - Training\n","        - img_1.png, img_1.png.csv, img_2.png, img_2.png.csv, ...\n","      - Validation\n","        - img_a.png, img_a.png.csv, img_b.png, img_b.png.csv,...\n","    - **Quality control dataset**\n","      - Validation\n","        - img_a.png, img_a.png.csv, img_b.png, img_b.png.csv\n","    - **Data to be predicted**\n","    - **Results**\n","\n","---\n","\n","<font size = 4> **Note: This notebook is still in the beta stage.\n","Currently, the notebook works only if the annotation files are in csv format with the following columns:**\n","\n","***| filename | width | height | object_index | class_name | x | y |***\n","\n","where each row in the csv will provide the coordinates **(x,y)** of an edge point in the segmentation mask of an individual object with a dedicated **object_index** (e.g. 1, 2, 3....) and its **class_name** (e.g. 'nucleus' or 'horse' etc.) on the image of dimensions **width** x **height** (pixels). If you already have a dataset with segmentation masks we can provide a fiji macro that can convert the dataset into the correct format.\n","*We are actively working on integrating more flexibility into the annotations this notebook can be used with.*\n","\n","---\n","\n","<font size = 4>**Important note**\n","\n","<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to run predictions using the model that you trained.\n","\n","<font size = 4>- If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n","\n","<font size = 4>- If you only wish to **run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n","---"]},{"cell_type":"markdown","metadata":{"id":"ffNw8dIQjftT"},"source":["# **1. Install MaskRCNN and dependencies**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"iYBjQqd95MpG"},"source":["## **1.1. Install key dependencies**\n","---\n","<font size = 4> "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UTratIh3-Zl_"},"outputs":[],"source":["#@markdown ##Install MaskRCNN and dependencies\n","!pip install fpdf2\n","!pip install imgaug\n","!pip install h5py==2.10\n","!git clone https://github.com/matterport/Mask_RCNN\n","\n","#Force session restart\n","exit(0)"]},{"cell_type":"markdown","metadata":{"id":"c3JUL5cQ5cY-"},"source":["## **1.2. Restart your runtime**\n","---\n","<font size = 4>\n","\n","\n","**<font size = 4> Ignore the following message error message. Your Runtime has automatically restarted. This is normal.**\n","\n","<img width=\"40%\" alt =\"\" src=\"https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/session_crash.png\"><figcaption>  </figcaption>\n"]},{"cell_type":"markdown","metadata":{"id":"eLGtfVWE6lu9"},"source":["## **1.3. Load key dependencies**\n","---\n","<font size = 4> "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"laDhajuKOs9t"},"outputs":[],"source":["Notebook_version = '1.14.1'\n","Network = 'MaskRCNN'\n","\n","from builtins import any as b_any\n","\n","def get_requirements_path():\n","    # Store requirements file in 'base_path' directory\n","    current_dir = os.getcwd()\n","    dir_count = current_dir.count('/') - 1\n","    path = '../' * (dir_count) + 'requirements.txt'\n","    return path\n","\n","def filter_files(file_list, filter_list):\n","    filtered_list = []\n","    for fname in file_list:\n","        if b_any(fname.split('==')[0] in s for s in filter_list):\n","            filtered_list.append(fname)\n","    return filtered_list\n","\n","def build_requirements_file(before, after):\n","    path = get_requirements_path()\n","\n","    # Exporting requirements.txt for local run\n","    !pip freeze > $path\n","\n","    # Get minimum requirements file\n","    df = pd.read_csv(path)\n","    mod_list = [m.split('.')[0] for m in after if not m in before]\n","    req_list_temp = df.values.tolist()\n","    req_list = [x[0] for x in req_list_temp]\n","\n","    # Replace with package name and handle cases where import name is different to module name\n","    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n","    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n","    filtered_list = filter_files(req_list, mod_replace_list)\n","\n","    file=open(path,'w')\n","    for item in filtered_list:\n","        file.writelines(item)\n","\n","    file.close()\n","\n","import sys\n","before = [str(m) for m in sys.modules]\n","\n","#@markdown ##Load Key Dependencies\n","# %tensorflow_version 1.x\n","\n","import os\n","import sys\n","import json\n","import datetime\n","import time\n","import numpy as np\n","import skimage.draw\n","from skimage import io\n","import imgaug\n","import pandas as pd\n","import csv\n","import random\n","import datetime\n","import shutil\n","from matplotlib import pyplot as plt\n","import matplotlib.lines as lines\n","from matplotlib.patches import Polygon\n","import IPython.display\n","from PIL import Image, ImageDraw, ImageFont\n","from fpdf import FPDF, HTMLMixin \n","from pip._internal.operations.freeze import freeze\n","import subprocess as sp\n","\n","#Create a variable to get and store relative base path\n","base_path = os.getcwd()\n","\n","# Root directory of the project\n","ROOT_DIR = os.path.abspath(base_path)\n","# !git clone https://github.com/matterport/Mask_RCNN\n","# Import Mask RCNN\n","sys.path.append(ROOT_DIR)  # To find local version of the library\n","os.chdir(base_path + '/Mask_RCNN')\n","\n","#Here we need to replace \"self.keras_model.metrics_tensors.append(loss)\" with \"self.keras_model.add_metric(loss, name)\"\n","# in model.py line 2199, otherwise we get version issues.\n","from tempfile import mkstemp\n","from shutil import move, copymode\n","from os import fdopen, remove\n","#This function replaces the old default files with new values\n","def replace(file_path, pattern, subst):\n","    #Create temp file\n","    fh, abs_path = mkstemp()\n","    with fdopen(fh,'w') as new_file:\n","        with open(file_path) as old_file:\n","            for line in old_file:\n","                new_file.write(line.replace(pattern, subst))\n","    #Copy the file permissions from the old file to the new file\n","    copymode(file_path, abs_path)\n","    #Remove original file\n","    remove(file_path)\n","    #Move new file\n","    move(abs_path, file_path)\n","\n","replace(base_path + \"/Mask_RCNN/mrcnn/model.py\",'self.keras_model.metrics_tensors.append(loss)','self.keras_model.add_metric(loss, name)')\n","#replace(base_path + \"/Mask_RCNN/mrcnn/model.py\", \"save_weights_only=True),\", \"save_weights_only=True),\\n            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience = 30, min_lr = 0, verbose = 1)\")\n","#replace(base_path + \"/Mask_RCNN/mrcnn/model.py\", \"save_weights_only=True),\", \"save_weights_only=True),\\n            keras.callbacks.CSVLogger(base_path + '/results.csv'),\")\n","replace(base_path + \"/Mask_RCNN/mrcnn/model.py\",'workers = 0','workers = 1')\n","replace(base_path + \"/Mask_RCNN/mrcnn/model.py\",'workers = multiprocessing.cpu_count()','workers = 1')\n","replace(base_path + \"/Mask_RCNN/mrcnn/model.py\",'use_multiprocessing=True','use_multiprocessing=False')\n","replace(base_path + \"/Mask_RCNN/mrcnn/utils.py\",\"shift = np.array([0, 0, 1, 1])\",\"shift = np.array([0., 0., 1., 1.])\")\n","replace(base_path + \"/Mask_RCNN/mrcnn/visualize.py\", \"i += 1\",\"i += 1\\n    plt.savefig(base_path + '/TrainingDataExample_MaskRCNN.png',bbox_inches='tight',pad_inches=0)\")\n","#replace(base_path + \"/Mask_RCNN/mrcnn/model.py\",\"   class_ids\",\"    if config.NUM_CLASSES == 2:\\n     class_ids = tf.ones_like(probs[:, 0], dtype=tf.int32)\\n   else:\\n     class_ids\")\n","\n","#Using this command will allow display of detections below the 0.5 score threshold, if only 1 class beyond background is in the dataset\n","replace(base_path + \"/Mask_RCNN/mrcnn/model.py\",\"class_ids = tf.argmax(probs\",\"if config.NUM_CLASSES >= 2:\\n     class_ids = tf.ones_like(probs[:, 0], dtype=tf.int32)\\n    else:\\n     class_ids = tf.argmax(probs\")\n","\n","\n","from mrcnn.config import Config\n","from mrcnn import model as modellib, utils\n","from mrcnn import visualize\n","from mrcnn.model import log\n","from mrcnn import utils\n","\n","def get_ax(rows=1, cols=1, size=8):\n","    \"\"\"Return a Matplotlib Axes array to be used in\n","    all visualizations in the notebook. Provide a\n","    central point to control graph sizes.\n","    \n","    Change the default size attribute to control the size\n","    of rendered images\n","    \"\"\"\n","    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n","    return ax\n","\n","############################################################\n","#  Dataset\n","############################################################\n","\n","class ClassDataset(utils.Dataset):\n","    def load_coco(annotation_file):\n","        dataset = json.load(open(annotation_file, 'r'))\n","        assert type(dataset)==dict, 'annotation file format {} not supported'.format(type(dataset))\n","        self.dataset = dataset\n","        self.createIndex()\n","\n","    def createIndex(self):\n","        # create index\n","        print('creating index...')\n","        anns, cats, imgs = {}, {}, {}\n","        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n","        if 'annotations' in self.dataset:\n","            for ann in self.dataset['annotations']:\n","                imgToAnns[ann['image_id']].append(ann)\n","                anns[ann['id']] = ann\n","\n","        if 'images' in self.dataset:\n","            for img in self.dataset['images']:\n","                imgs[img['id']] = img\n","\n","        if 'categories' in self.dataset:\n","            for cat in self.dataset['categories']:\n","                cats[cat['id']] = cat\n","\n","        if 'annotations' in self.dataset and 'categories' in self.dataset:\n","            for ann in self.dataset['annotations']:\n","                catToImgs[ann['category_id']].append(ann['image_id'])\n","\n","        print('index created!')\n","\n","        # create class members\n","        self.anns = anns\n","        self.imgToAnns = imgToAnns\n","        self.catToImgs = catToImgs\n","        self.imgs = imgs\n","        self.cats = cats\n","\n","    def load_class(self, dataset_dir, subset):\n","        \"\"\"Load a subset of the dataset.\n","        dataset_dir: Root directory of the dataset.\n","        subset: Subset to load: train or val\n","        \"\"\"\n","\n","        # Add classes. We have only one class to add.\n","        self.add_class(\"Training_Datasets\", 1, \"nucleus\")\n","                \n","        # Train or validation dataset?\n","        assert subset in [\"Training\", \"Validation\"]\n","        dataset_dir = os.path.join(dataset_dir, subset)\n","\n","        # Load annotations\n","        # VGG Image Annotator (up to version 1.6) saves each image in the form:\n","        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n","        #   'regions': {\n","        #       '0': {\n","        #           'region_attributes': {},\n","        #           'shape_attributes': {\n","        #               'all_points_x': [...],\n","        #               'all_points_y': [...],\n","        #               'name': 'polygon'}},\n","        #       ... more regions ...\n","        #   },\n","        #   'size': 100202\n","        # }\n","        # We mostly care about the x and y coordinates of each region\n","        # Note: In VIA 2.0, regions was changed from a dict to a list.\n","        annotations = json.load(open(os.path.join(dataset_dir, \"birds071220220_json.json\")))\n","        annotations = list(annotations.values())  # don't need the dict keys\n","   \n","        # The VIA tool saves images in the JSON even if they don't have any\n","        # annotations. Skip unannotated images.\n","        annotations = [a for a in annotations if a['regions']]\n","        \n","        # Add images\n","        for a in annotations:\n","            # Get the x, y coordinaets of points of the polygons that make up\n","            # the outline of each object instance. These are stores in the\n","            # shape_attributes (see json format above)\n","            # The if condition is needed to support VIA versions 1.x and 2.x.\n","            if type(a['regions']) is dict:\n","                polygons = [r['shape_attributes'] for r in a['regions'].values()]\n","            else:\n","                polygons = [r['shape_attributes'] for r in a['regions']] \n","\n","            #Get the class of the object\n","            obj_class = [c['region_attributes']['species'] for c in a['regions']]\n","\n","            # load_mask() needs the image size to convert polygons to masks.\n","            # Unfortunately, VIA doesn't include it in JSON, so we must read\n","            # the image. This is only managable since the dataset is tiny.\n","            image_path = os.path.join(dataset_dir, a['filename'])\n","            image = skimage.io.imread(image_path)\n","            height, width = image.shape[:2]\n","\n","            self.add_image(\n","                \"Training_Datasets\",\n","                image_id=a['filename'],  # use file name as a unique image id\n","                path=image_path,\n","                width=width, height=height,\n","                polygons=polygons,\n","                obj_class=obj_class)\n","            \n","    def load_image_csv(self, dataset_dir, subset):\n","        # Add classes. We have only one class to add.\n","        # self.add_class(\"Training_Datasets\", 1, \"nucleus\")\n","        #self.add_class(\"Training_Datasets\", 2, \"Great tit\")\n","        \n","        # Train or validation dataset?\n","        assert subset in [\"Training\", \"Validation\"]\n","        dataset_dir = os.path.join(dataset_dir, subset)\n","        #Data Format\n","        #csv file:\n","        #filename,width,height,object_index, class_name, x, y\n","        #file_1,256,256,1,nucleus, 1, 1\n","        #file_1,256,256,1,nucleus, 3, 10\n","        #file_1,256,256,1,nucleus, 1, 3\n","        #file_1,256,256,1,nucleus, 3, 7\n","        #file_1,256,256,2,nucleus, 17, 20\n","        #...\n","        class_index = 0\n","        obj_class_old = \"\"\n","        #class_names will hold all the classes we find in the dataset \n","        class_names = {obj_class_old:class_index}\n","        for csv_file_name in os.listdir(dataset_dir):\n","          if csv_file_name.endswith('.csv'):\n","            with open(os.path.join(dataset_dir,csv_file_name)) as csvfile_count:\n","              row_count = sum(1 for _ in csvfile_count)\n","            with open(os.path.join(dataset_dir,csv_file_name)) as csvfile:\n","              annotations = csv.reader(csvfile)\n","              next(annotations)\n","              polygons = []\n","              x_values = []\n","              y_values = []\n","              index_old = 1\n","              for line in annotations:\n","                img_file_name = line[0]\n","                index_new = int(line[4])\n","                obj_class = line[3]\n","                \n","                if not obj_class in class_names:\n","                  class_index+=1\n","                  class_names[obj_class] = class_index\n","                  self.add_class(\"Training_Datasets\", class_index, obj_class)\n","                \n","                if index_new == index_old:\n","                  x_values.append(int(line[5]))\n","                  y_values.append(int(line[6]))\n","                 \n","                  if row_count == annotations.line_num:\n","                    polygon = {\"class_name\":class_names[obj_class],\"all_points_x\":x_values,\"all_points_y\":y_values}\n","                    polygons.append(polygon)\n","                    \n","                elif index_new != index_old:\n","                  polygon = {\"class_name\":class_names[obj_class_old],\"all_points_x\":x_values,\"all_points_y\":y_values}\n","                  polygons.append(polygon)\n","                  x_values = []\n","                  x_values.append(int(line[5]))\n","                  y_values = []\n","                  y_values.append(int(line[6]))\n","                \n","                index_old = int(line[4])\n","                obj_class_old = line[3]\n","                image_path = os.path.join(dataset_dir,img_file_name)\n","                \n","            self.add_image(\n","                \"Training_Datasets\",\n","                image_id=img_file_name,  # use file name as a unique image id\n","                path=image_path,\n","                width=int(line[1]), height=int(line[2]),\n","                polygons=polygons)\n","              #print(csv_file_name, class_index, polygons)\n","        return class_index\n","\n","    def load_mask(self, image_id):\n","        info = self.image_info[image_id]\n","        #print(info)\n","        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n","                        dtype=np.uint8)\n","        class_ids = []\n","        #class_index = 0\n","        for i, p in enumerate(info[\"polygons\"]):\n","            \n","            class_name = p['class_name']\n","            # class_names = {class_name:class_index}\n","            # if class_name != class_name_old:\n","            #   class_index+=1\n","            #   class_names[class_name] = class_index\n","            \n","            # Get indexes of pixels inside the polygon and set them to 1\n","            # print(p['y_values'])\n","            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n","            mask[rr, cc, i] = 1\n","            \n","            #class_name_old = p['class_name']\n","            class_ids.append(class_name)\n","        \n","        class_ids = np.array(class_ids)\n","\n","        return mask.astype(np.bool), class_ids.astype(np.int32)\n","\n","    # def load_mask(self, image_id):\n","    #     \"\"\"Generate instance masks for an image.\n","    #    Returns:\n","    #     masks: A bool array of shape [height, width, instance count] with\n","    #         one mask per instance.\n","    #     class_ids: a 1D array of class IDs of the instance masks.\n","    #     \"\"\"\n","    #     def clean_name(name):\n","    #       \"\"\"Returns a shorter version of object names for cleaner display.\"\"\"\n","    #       return \",\".join(name.split(\",\")[:1])\n","\n","    #     # If not a balloon dataset image, delegate to parent class.\n","    #     image_info = self.image_info[image_id]\n","    #     if image_info[\"source\"] != \"Training_Datasets\":\n","    #         return super(self.__class__, self).load_mask(image_id)\n","\n","    #     # Convert polygons to a bitmap mask of shape\n","    #     # [height, width, instance_count]\n","    #     info = self.image_info[image_id]\n","\n","    #     mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n","    #                     dtype=np.uint8)\n","    #     for i, p in enumerate(info[\"polygons\"]):\n","    #         # Get indexes of pixels inside the polygon and set them to 1\n","    #         rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n","    #         mask[rr, cc, i] = 1\n","\n","    #     classes = info[\"obj_class\"]\n","    #     class_list = [clean_name(c[\"name\"]) for c in self.class_info]\n","    #     class_ids = np.array([class_list.index(s) for s in classes])\n","\n","    #     # Return mask, and array of class IDs of each instance. Since we have\n","    #     # one class ID only, we return an array of 1s\n","    #     return mask.astype(np.bool), class_ids.astype(np.int32)#np.ones([mask.shape[-1]], dtype=np.int32)\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return the path of the image.\"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"Training_Datasets\":\n","            return info[\"path\"]\n","        else:\n","            super(self.__class__, self).image_reference(image_id)\n","\n","\n","def train(model, augmentation=True):\n","    \"\"\"Train the model.\"\"\"\n","    # Training dataset.\n","    dataset_train = ClassDataset()\n","    dataset_train.load_class(base_path + '/gdrive/MyDrive/MaskRCNN/Training_Datasets', \"Training\")\n","    dataset_train.prepare()\n","\n","    # Validation dataset\n","    dataset_val = ClassDataset()\n","    dataset_val.load_class(base_path + '/gdrive/MyDrive/MaskRCNN/Training_Datasets', \"Validation\")\n","    dataset_val.prepare()\n","\n","    if augmentation == True:\n","      augment = imgaug.augmenters.Sometimes(0.5, imgaug.augmenters.OneOf([imgaug.augmenters.Fliplr(0.5),\n","                                                                         imgaug.augmenters.Flipud(0.5),\n","                                                                         imgaug.augmenters.Affine(rotate=45)]))\n","    else:\n","      augment = None\n","    # *** This training schedule is an example. Update to your needs ***\n","    # Since we're using a very small dataset, and starting from\n","    # COCO trained weights, we don't need to train too long. Also,\n","    # no need to train all layers, just the heads should do it.\n","    print(\"Training network heads\")\n","    model.train(dataset_train, dataset_val,\n","                learning_rate=config.LEARNING_RATE,\n","                epochs=80,\n","                augmentation = augment,\n","                layers='heads')\n","\n","\n","def train_csv(model, training_folder, augmentation=True, epochs = 20, layers = 'heads'):\n","    \"\"\"Train the model.\"\"\"\n","    # Training dataset.\n","    dataset_train = ClassDataset()\n","    dataset_train.load_image_csv(training_folder, \"Training\")\n","    dataset_train.prepare()\n","\n","    # Validation dataset\n","    dataset_val = ClassDataset()\n","    dataset_val.load_image_csv(training_folder, \"Validation\")\n","    dataset_val.prepare()\n","\n","    if augmentation == True:\n","      augment = imgaug.augmenters.SomeOf((1,2),[imgaug.augmenters.OneOf([imgaug.augmenters.Affine(rotate=90),\n","                                                                        imgaug.augmenters.Affine(rotate=180),\n","                                                                        imgaug.augmenters.Affine(rotate=270)]),\n","                                                imgaug.augmenters.Fliplr(0.5),\n","                                                imgaug.augmenters.Flipud(0.5),\n","                                                imgaug.augmenters.Multiply((0.8, 1.5)),\n","                                                imgaug.augmenters.GaussianBlur(sigma=(0.0, 5.0))])\n","    else:\n","      augment = None\n","    # *** This training schedule is an example. Update to your needs ***\n","    # Since we're using a very small dataset, and starting from\n","    # COCO trained weights, we don't need to train too long. Also,\n","    # no need to train all layers, just the heads should do it.\n","    print(\"Training network heads\")\n","    model.train(dataset_train, dataset_val,\n","                learning_rate=config.LEARNING_RATE,\n","                epochs=epochs,\n","                augmentation = augment,\n","                layers=layers)\n","\n","def color_splash(image, mask):\n","    \"\"\"Apply color splash effect.\n","    image: RGB image [height, width, 3]\n","    mask: instance segmentation mask [height, width, instance count]\n","    Returns result image.\n","    \"\"\"\n","    # Make a grayscale copy of the image. The grayscale copy still\n","    # has 3 RGB channels, though.\n","    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n","    # Copy color pixels from the original color image where mask is set\n","    if mask.shape[-1] > 0:\n","        # We're treating all instances as one, so collapse the mask into one layer\n","        mask = (np.sum(mask, -1, keepdims=True) >= 1)\n","        splash = np.where(mask, image, gray).astype(np.uint8)\n","    else:\n","        splash = gray.astype(np.uint8)\n","    return splash\n","\n","\n","def detect_and_color_splash(model, image_path=None, video_path=None):\n","    assert image_path or video_path\n","\n","    # Image or video?\n","    if image_path:\n","        # Run model detection and generate the color splash effect\n","        print(\"Running on {}\".format(args.image))\n","        # Read image\n","        image = skimage.io.imread(args.image)\n","        # Detect objects\n","        r = model.detect([image], verbose=1)[0]\n","        # Color splash\n","        splash = color_splash(image, r['masks'])\n","        # Save output\n","        file_name = \"splash_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\n","        skimage.io.imsave(file_name, splash)\n","    elif video_path:\n","        import cv2\n","        # Video capture\n","        vcapture = cv2.VideoCapture(video_path)\n","        width = int(vcapture.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        height = int(vcapture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        fps = vcapture.get(cv2.CAP_PROP_FPS)\n","\n","        # Define codec and create video writer\n","        file_name = \"splash_{:%Y%m%dT%H%M%S}.avi\".format(datetime.datetime.now())\n","        vwriter = cv2.VideoWriter(file_name,\n","                                  cv2.VideoWriter_fourcc(*'MJPG'),\n","                                  fps, (width, height))\n","\n","        count = 0\n","        success = True\n","        while success:\n","            print(\"frame: \", count)\n","            # Read next image\n","            success, image = vcapture.read()\n","            if success:\n","                # OpenCV returns images as BGR, convert to RGB\n","                image = image[..., ::-1]\n","                # Detect objects\n","                r = model.detect([image], verbose=0)[0]\n","                # Color splash\n","                splash = color_splash(image, r['masks'])\n","                # RGB -> BGR to save image to video\n","                splash = splash[..., ::-1]\n","                # Add image to video writer\n","                vwriter.write(splash)\n","                count += 1\n","        vwriter.release()\n","    print(\"Saved to \", file_name)\n","\n","# Colors for the warning messages\n","class bcolors:\n","  WARNING = '\\033[31m'\n","  NORMAL = '\\033[0m'\n","\n","class ClassConfig(Config):\n","    \"\"\"Configuration for training on the toy  dataset.\n","    Derives from the base Config class and overrides some values.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    # We use a GPU with 12GB memory, which can fit two images.\n","    # Adjust down if you use a smaller GPU.\n","    IMAGES_PER_GPU = 1\n","    DETECTION_MIN_CONFIDENCE = 0\n","    NAME = \"nucleus\"\n","    # Backbone network architecture\n","    # Supported values are: resnet50, resnet101\n","    BACKBONE = \"resnet50\"\n","    # Input image resizing\n","    # Random crops of size 64x64\n","    IMAGE_RESIZE_MODE = \"crop\"\n","    IMAGE_MIN_DIM = 256\n","    IMAGE_MAX_DIM = 256\n","    IMAGE_MIN_SCALE = 2.0\n","    # Length of square anchor side in pixels\n","    RPN_ANCHOR_SCALES = (4, 8, 16, 32, 64)\n","    # ROIs kept after non-maximum supression (training and inference)\n","    POST_NMS_ROIS_TRAINING = 200\n","    POST_NMS_ROIS_INFERENCE = 400\n","    # Non-max suppression threshold to filter RPN proposals.\n","    # You can increase this during training to generate more propsals.\n","    RPN_NMS_THRESHOLD = 0.9\n","    # How many anchors per image to use for RPN training\n","    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n","    # Image mean (RGB)\n","    MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\n","    # If enabled, resizes instance masks to a smaller size to reduce\n","    # memory load. Recommended when using high-resolution images.\n","    USE_MINI_MASK = True\n","    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n","    TRAIN_ROIS_PER_IMAGE = 128\n","    # Maximum number of ground truth instances to use in one image\n","    MAX_GT_INSTANCES = 100\n","    # Max number of final detections per image\n","    DETECTION_MAX_INSTANCES = 200\n","\n","# Below we define a function which saves the predictions.\n","# It is from this branch:\n","# https://github.com/matterport/Mask_RCNN/commit/bc8f148b820ebd45246ed358a120c99b09798d71\n","\n","def save_image(image, image_name, boxes, masks, class_ids, scores, class_names, filter_classs_names=None,\n","               scores_thresh=0.1, save_dir=None, mode=0):\n","    \"\"\"\n","        image: image array\n","        image_name: image name\n","        boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n","        masks: [num_instances, height, width]\n","        class_ids: [num_instances]\n","        scores: confidence scores for each box\n","        class_names: list of class names of the dataset\n","        filter_classs_names: (optional) list of class names we want to draw\n","        scores_thresh: (optional) threshold of confidence scores\n","        save_dir: (optional) the path to store image\n","        mode: (optional) select the result which you want\n","                mode = 0 , save image with bbox,class_name,score and mask;\n","                mode = 1 , save image with bbox,class_name and score;\n","                mode = 2 , save image with class_name,score and mask;\n","                mode = 3 , save mask with black background;\n","    \"\"\"\n","    mode_list = [0, 1, 2, 3]\n","    assert mode in mode_list, \"mode's value should in mode_list %s\" % str(mode_list)\n","\n","    if save_dir is None:\n","        save_dir = os.path.join(os.getcwd(), \"output\")\n","        if not os.path.exists(save_dir):\n","            os.makedirs(save_dir)\n","\n","    useful_mask_indices = []\n","\n","    N = boxes.shape[0]\n","    if not N:\n","        print(\"\\n*** No instances in image %s to draw *** \\n\" % (image_name))\n","        return\n","    else:\n","        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n","\n","    for i in range(N):\n","        # filter\n","        class_id = class_ids[i]\n","        score = scores[i] if scores is not None else None\n","        if score is None or score < scores_thresh:\n","            continue\n","\n","        label = class_names[class_id]\n","        if (filter_classs_names is not None) and (label not in filter_classs_names):\n","            continue\n","\n","        if not np.any(boxes[i]):\n","            # Skip this instance. Has no bbox. Likely lost in image cropping.\n","            continue\n","\n","        useful_mask_indices.append(i)\n","\n","    if len(useful_mask_indices) == 0:\n","        print(\"\\n*** No instances in image %s to draw *** \\n\" % (image_name))\n","        return\n","\n","    colors = visualize.random_colors(len(useful_mask_indices))\n","\n","    if mode != 3:\n","        masked_image = image.astype(np.uint8).copy()\n","    else:\n","        masked_image = np.zeros(image.shape).astype(np.uint8)\n","\n","    if mode != 1:\n","        for index, value in enumerate(useful_mask_indices):\n","            masked_image = visualize.apply_mask(masked_image, masks[:, :, value], colors[index])\n","\n","    masked_image = Image.fromarray(masked_image)\n","\n","    if mode == 3:\n","        masked_image.save(os.path.join(save_dir, '%s' % (image_name)))\n","        return\n","\n","    draw = ImageDraw.Draw(masked_image)\n","    colors = np.array(colors).astype(int) * 255\n","\n","    for index, value in enumerate(useful_mask_indices):\n","        class_id = class_ids[value]\n","        score = scores[value]\n","        label = class_names[class_id]\n","\n","        y1, x1, y2, x2 = boxes[value]\n","        if mode != 2:\n","            color = tuple(colors[index])\n","            draw.rectangle((x1, y1, x2, y2), outline=color)\n","\n","        # Label\n","        font = ImageFont.load_default()\n","        draw.text((x1, y1), \"%s %f\" % (label, score), (255, 255, 255), font)\n","\n","    masked_image.save(os.path.join(save_dir, '%s' % (image_name)))\n","\n","def pdf_export(config, trained = False, augmentation = False, pretrained_model = False):\n","  class MyFPDF(FPDF, HTMLMixin):\n","    pass\n","\n","  config_list = \"\"\n","  for a in dir(config):\n","    if not a.startswith(\"__\") and not callable(getattr(config, a)):\n","      config_list += \"{}:     {}\\n\".format(a, getattr(config, a))\n","  \n","  pdf = MyFPDF()\n","  pdf.add_page()\n","  pdf.set_right_margin(-1)\n","  pdf.set_font(\"Arial\", size = 11, style='B') \n","\n","  Network = 'MaskRCNN'\n","  day = datetime.datetime.now()\n","  datetime_str = str(day)[0:10]\n","\n","  Header = 'Training report for '+Network+' model ('+model_name+'):\\nDate: '+datetime_str\n","  pdf.multi_cell(180, 5, txt = Header, align = 'L') \n","\n","  # add another cell\n","  if trained:\n","    training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n","    pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n","  pdf.ln(1)\n","\n","  Header_2 = 'Information for your materials and methods:'\n","  pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n","\n","  all_packages = ''\n","  for requirement in freeze(local_only=True):\n","    all_packages = all_packages+requirement+', '\n","  #print(all_packages)\n","\n","  #Main Packages\n","  main_packages = ''\n","  version_numbers = []\n","  for name in ['tensorflow','numpy','Keras']:\n","    find_name=all_packages.find(name)\n","    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n","    #Version numbers only here:\n","    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n","\n","  try:\n","    cuda_version = subprocess.run([\"nvcc\",\"--version\"],stdout=subprocess.PIPE)\n","    cuda_version = cuda_version.stdout.decode('utf-8')\n","    cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n","  except:\n","    cuda_version = ' - No cuda found - '\n","  try:\n","    gpu_name = subprocess.run([\"nvidia-smi\"],stdout=subprocess.PIPE)\n","    gpu_name = gpu_name.stdout.decode('utf-8')\n","    gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n","  except:\n","    gpu_name = ' - No GPU found - '\n","  #print(cuda_version[cuda_version.find(', V')+3:-1])\n","  #print(gpu_name)\n","  try:\n","    shape = io.imread(Training_source+'/Training/'+os.listdir(Training_source+'/Training')[0]).shape\n","  except:\n","    shape = io.imread(Training_source+'/Training/'+os.listdir(Training_source+'/Training')[0][:-4]).shape\n","  dataset_size = len(os.listdir(Training_source))/2\n","\n","  text = 'The '+Network+' model was trained using weights initialised on the coco dataset for '+str(number_of_epochs)+' epochs on '+str(dataset_size)+' labelled images (image dimensions: '+str(shape)+') with a batch size of '+str(config.BATCH_SIZE)+' and custom loss functions for region proposal and classification, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n","\n","  if pretrained_model:\n","    text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(dataset_size)+' labelled images (image dimensions: '+str(shape)+') with a batch size of '+str(config.BATCH_SIZE)+' and custom loss functions for region proposal and classification, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was retrained from a previous model checkpoint (model: '+os.path.basename(pretrained_model_path)[:-8]+', checkpoint: '+str(int(pretrained_model_path[-7:-3]))+'). Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n","\n","  pdf.set_font('')\n","  pdf.set_font_size(10.)\n","  pdf.multi_cell(190, 5, txt = text, align='L')\n","  pdf.ln(1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.ln(1)\n","  pdf.cell(28, 5, txt='Augmentation: ', ln=0)\n","  pdf.set_font('')\n","  if augmentation:\n","    aug_text = 'The dataset was augmented by vertical and horizontal flipping'\n","    # if multiply_dataset_by >= 2:\n","    #   aug_text = aug_text+'\\n- flipping'\n","    # if multiply_dataset_by > 2:\n","    #   aug_text = aug_text+'\\n- rotation'\n","  else:\n","    aug_text = 'No augmentation was used for training.'\n","  pdf.multi_cell(190, 5, txt=aug_text, align='L')\n","  pdf.ln(1)\n","  pdf.set_font('Arial', size = 11, style = 'B')\n","  pdf.ln(1)\n","  pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n","  pdf.set_font('')\n","  pdf.set_font_size(10.)\n","  # if Use_Default_Advanced_Parameters:\n","  #   pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n","  pdf.cell(200, 5, txt='The following parameters were used for training:')\n","  pdf.ln(4)\n","  pdf.multi_cell(200, 5, txt=config_list)\n","  pdf.ln(1)\n","\n","  pdf.set_font(\"Arial\", size = 11, style='B')\n","  pdf.ln(1)\n","  pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(30, 5, txt= 'Training_source:', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = Training_source+'/Training', align = 'L')\n","  pdf.ln(1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(29, 5, txt= 'Validation:', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = Training_source+'/Validation', align = 'L')\n","  #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n","  pdf.ln(1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(22, 5, txt= 'Model Path:', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n","  pdf.ln(1)\n","  pdf.cell(60, 5, txt = 'Example ground-truth annotation', ln=1)\n","  pdf.ln(1)\n","  exp_size = io.imread(base_path + '/TrainingDataExample_MaskRCNN.png').shape\n","  pdf.image(base_path + '/TrainingDataExample_MaskRCNN.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n","  pdf.ln(1)\n","  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n","  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n","  pdf.ln(1)\n","  ref_2 = '- MaskRCNN: Kaiming He, Georgia Gkioxari, Piotr Doll√°r, Ross Girshick. \"Mask R - CNN\" arxiv. 2018.'\n","  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","  pdf.ln(1)\n","  if augmentation:\n","    ref_3 = '- imgaug: Jung, Alexander et al., https://github.com/aleju/imgaug, (2020)'\n","    pdf.multi_cell(190, 5, txt = ref_3, align='L')\n","    pdf.ln(1)\n","  pdf.ln(3)\n","  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n","  pdf.set_font('Arial', size = 11, style='B')\n","  pdf.multi_cell(190, 5, txt=reminder, align='C')\n","  pdf.ln(1)\n","\n","  pdf.output(os.path.dirname(model.log_dir)+'/'+model_name+'_training_report.pdf')\n","\n","  print('------------------------------')\n","  print('PDF report exported in '+model_path+'/'+model_name+'/')\n","\n","def qc_pdf_export():\n","  class MyFPDF(FPDF, HTMLMixin):\n","    pass\n","\n","  pdf = MyFPDF()\n","  pdf.add_page()\n","  pdf.set_right_margin(-1)\n","  pdf.set_font(\"Arial\", size = 11, style='B') \n","\n","  Network = 'MaskRCNN'\n","\n","  day = datetime.datetime.now()\n","  datetime_str = str(day)[0:16]\n","\n","  Header = 'Quality Control report for '+Network+' model ('+QC_model_name+', checkpoint:'+str(Checkpoint)+')\\nDate and Time: '+datetime_str\n","  pdf.multi_cell(180, 5, txt = Header, align = 'L') \n","  pdf.ln(1)\n","\n","  all_packages = ''\n","  for requirement in freeze(local_only=True):\n","    all_packages = all_packages+requirement+', '\n","\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 11, style = 'B')\n","  pdf.ln(2)\n","  pdf.cell(190, 5, txt = 'Development of Training Losses', ln=1, align='L')\n","  pdf.ln(1)\n","  if os.path.exists(QC_model_folder+'/Quality Control/lossCurveAndmAPPlots.png'):\n","    exp_size = io.imread(QC_model_folder+'/Quality Control/lossCurveAndmAPPlots.png').shape\n","    pdf.image(QC_model_folder+'/Quality Control/lossCurveAndmAPPlots.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n","  else:\n","    pdf.set_font('')\n","    pdf.set_font('Arial', size=10)\n","    pdf.multi_cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.')\n","    pdf.ln(1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(80, 5, txt = 'P-R curves for test dataset', ln=1, align='L')\n","  pdf.ln(2)\n","  #for i in range(len(AP)):\n","  # os.path.exists(QC_model_folder+'/Quality Control/P-R_curve_'+config['model']['labels'][i]+'.png'):\n","  exp_size = io.imread(QC_model_folder+'/Quality Control/P-R_curve_'+QC_model_name+'.png').shape\n","  pdf.ln(1)\n","  pdf.image(QC_model_folder+'/Quality Control/P-R_curve_'+QC_model_name+'.png', x=16, y=None, w=round(exp_size[1]/4), h=round(exp_size[0]/4))\n","  #  else:\n","  #    pdf.cell(100, 5, txt='For the class '+config['model']['labels'][i]+' the model did not predict any objects.', ln=1, align='L')\n","  pdf.ln(3)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 11, style = 'B')\n","  pdf.ln(1)\n","  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n","  pdf.set_font('')\n","  pdf.set_font_size(10.)\n","\n","  pdf.ln(1)\n","  html = \"\"\"\n","  <body>\n","  <font size=\"10\" face=\"Courier\" >\n","  <table width=95% style=\"margin-left:0px;\">\"\"\"\n","  with open(QC_model_folder+'/Quality Control/QC_results.csv', 'r') as csvfile:\n","    metrics = csv.reader(csvfile)\n","    header = next(metrics)\n","    class_name = header[0]\n","    gt = header[1]\n","    tp = header[2]\n","    fn = header[3]\n","    iou = header[4]\n","    mAP = header[5]\n","    header = \"\"\"\n","    <tr>\n","    <th width = 15% align=\"left\">{0}</th>\n","    <th width = 15% align=\"left\">{1}</th>\n","    <th width = 15% align=\"left\">{2}</th>\n","    <th width = 15% align=\"left\">{3}</th>\n","    <th width = 15% align=\"left\">{4}</th>\n","    <th width = 15% align=\"left\">{5}</th>\n","    </tr>\"\"\".format(class_name,gt,tp,fn,iou,mAP)\n","    html = html+header\n","    i=0\n","    for row in metrics:\n","      i+=1\n","      class_name = row[0]\n","      gt = row[1]\n","      tp = row[2]\n","      fn = row[3]\n","      iou = row[4]\n","      mAP = row[5]\n","      cells = \"\"\"\n","        <tr>\n","          <td width = 15% align=\"left\">{0}</td>\n","          <td width = 15% align=\"left\">{1}</td>\n","          <td width = 15% align=\"left\">{2}</td>\n","          <td width = 15% align=\"left\">{3}</td>\n","          <td width = 15% align=\"left\">{4}</td>\n","          <td width = 15% align=\"left\">{5}</td>\n","        </tr>\"\"\".format(class_name,str(gt),str(tp),str(fn),str(iou),str(mAP))\n","      html = html+cells\n","    html = html+\"\"\"</body></table>\"\"\"\n","\n","  pdf.write_html(html)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 11, style = 'B')\n","  pdf.ln(3)\n","  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n","  pdf.ln(3)\n","  exp_size = io.imread(QC_model_folder+'/Quality Control/QC_example_data.png').shape\n","  pdf.image(QC_model_folder+'/Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/10), h = round(exp_size[0]/10))\n","\n","  pdf.set_font('')\n","  pdf.set_font_size(10.)\n","  pdf.ln(3)\n","  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n","  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n","  pdf.ln(1)\n","  ref_2 = '- MaskRCNN: Kaiming He, Georgia Gkioxari, Piotr Doll√°r, Ross Girshick. \"Mask R - CNN\" arxiv. 2018.'\n","  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","  pdf.ln(1)\n","\n","  pdf.ln(3)\n","  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n","\n","  pdf.set_font('Arial', size = 11, style='B')\n","  pdf.multi_cell(190, 5, txt=reminder, align='C')\n","  pdf.ln(1)\n","\n","  pdf.output(QC_model_folder+'/Quality Control/'+QC_model_name+'_QC_report.pdf')\n","\n","\n","  print('------------------------------')\n","  print('PDF report exported in '+QC_model_folder+'/Quality Control/')\n","\n","\n","# Check if this is the latest version of the notebook\n","All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n","print('Notebook version: '+Notebook_version)\n","Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n","print('Latest notebook version: '+Latest_Notebook_version)\n","if Notebook_version == Latest_Notebook_version:\n","  print(\"This notebook is up-to-date.\")\n","else:\n","  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n","\n","\n","# Build requirements file for local run\n","after = [str(m) for m in sys.modules]\n","build_requirements_file(before, after)"]},{"cell_type":"markdown","metadata":{"id":"s7_nokQv7M4-"},"source":["# **2. Initialise the Colab session**\n","\n","\n","\n","\n","---\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5-hsYVdkjKuI"},"source":["\n","## **2.1. Check for GPU access**\n","---\n","\n","By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n","\n","<font size = 4>Go to **Runtime -> Change the Runtime type**\n","\n","<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n","\n","<font size = 4>**Accelator: GPU** *(Graphics processing unit)*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-goWypUVEvnp"},"outputs":[],"source":["#@markdown ##Run this cell to check if you have GPU access\n","# %tensorflow_version 1.x\n","import tensorflow as tf\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"L_pjmwONjTvb"},"source":["## **2.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. \n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"QK-DDu1ljVna"},"outputs":[],"source":["#@markdown ##Run this cell to connect your Google Drive to Colab\n","\n","#@markdown * Click on the URL. \n","\n","#@markdown * Sign in your Google Account. \n","\n","#@markdown * Copy the authorization code. \n","\n","#@markdown * Enter the authorization code. \n","\n","#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n","\n","#mounts user's Google Drive to Google Colab.\n","\n","from google.colab import drive\n","drive.mount(base_path + '/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"P-YFjdLR-5hv"},"source":["**<font size = 4> If you cannot see your files, reactivate your session by connecting to your hosted runtime.** \n","\n","\n","<img width=\"40%\" alt =\"Example of image detection with retinanet.\" src=\"https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/connect_to_hosted.png\"><figcaption> Connect to a hosted runtime. </figcaption>"]},{"cell_type":"markdown","metadata":{"id":"Do_LZbDmpJiZ"},"source":["# **3. Select your paths and parameters**\n","\n","---\n","\n","<font size = 4>The code below allows the user to enter the paths to where the training data is and to define the training parameters.\n","\n","<font size = 4>If your dataset is large, this step can take a while. \n","\n","<font size = 4>**Note:** The BG class reported by MaskRCNN stands for 'background'. By default BG is the default class in MaskRCNN, so even if your dataset contains only one class, MaskRCNN will treat the dataset as a two-class set.\n"]},{"cell_type":"markdown","metadata":{"id":"M5QFEW-HpRdQ"},"source":["## **3.1. Setting the main training parameters**\n","---\n","<font size = 4>"]},{"cell_type":"markdown","metadata":{"id":"vdLRX63upWcB"},"source":["<font size = 5> **Paths for training, predictions and results**\n","\n","<font size = 4>**`Training_source:`:** This is the path to your folder containing the subfolders *Training* and *Validation*, each containing images with their respective annotations. **If your files are not organised in this way, the notebook will NOT work. So make sure everything looks right!** To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n","\n","<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten. **Note that MaskRCNN will add a timestamp to your model_name in the form: model_name*YearMonthDayTHourMinute***\n","\n","<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n","\n","<font size = 5>**Training parameters**\n","\n","<font size = 4>**`Training Depth`:** Here, you can choose how much you want to train the network. MaskRCNN is already pretrained on a large dataset which means its weights are already initialised. This means it may not be necessary to train the full model to reach satisfactory results on your dataset. To get the most out of the model, we recommend training the headlayers first for ca. 30 epochs, and then retraining the same model with an increasing depth for further 10s of epochs. To do this, use the same model_name in this section, with any other needed parameters and then load the desired weights file in section 3.3. **Default value: Head layers only**\n","\n","<font size = 4>**`number_of_epochs`:** Enter the number of epochs the networks will be trained for. Note that if you want to continue training a previously trained model, enter the final number of epochs you want to use, i.e. if your previous model was trained for 50 epochs and you want to train it to 80, enter 80 epochs here, not 30.\n","**Default value: 50**\n","\n","<font size = 4>**`detection_confidence`:** The network will assign scores of confidence to any predictions of ROIs it makes on the dataset during training. The detection confidence here indicates what threshold score you want to apply for the network to use accept any predicted ROIs. We recommend starting low here. If you notice your network is giving you too many ROIs, then increase this value gradually. **Default value: 0**\n","\n","<font size = 4>**`learning_rate:`** Input the initial value to be used as learning rate. The learning rate will decrease after 7 epochs if the validation loss does not improve. **Default value: 0.003**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kajoWCX8ps4O"},"outputs":[],"source":["#@markdown ###Path to training images:\n","\n","Training_source = \"\" #@param {type:\"string\"}\n","\n","# Ground truth images\n","#Training_validation = \"\" #@param {type:\"string\"}\n","\n","# model name and path\n","##@markdown ###Name of the model and path to model folder:\n","model_name = \"\" #@param {type:\"string\"}\n","model_path = \"\" #@param {type:\"string\"}\n","\n","full_model_path = os.path.join(model_path,model_name)\n","# if os.path.exists(full_model_path):\n","#   print(bcolors.WARNING+'Model folder already exists and will be overwritten.'+bcolors.NORMAL)\n","\n","# other parameters for training.\n","#@markdown ###Training Parameters\n","\n","Training_depth = \"3+ resnet layers\" #@param [\"Head_layers_only\", \"3+ resnet layers\", \"4+ resnet layers\", \"5+ resnet layers\", \"all layers\"]\n","##@markdown ###Advanced Parameters\n","\n","#Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n","##@markdown ###If not, please input:\n","\n","number_of_epochs =  10#@param {type:\"integer\"}\n","\n","batch_size =  4#@param{type:\"integer\"}\n","\n","image_resize_mode = \"none\"\n","\n","detection_confidence = 0 #@param {type:\"number\"}\n","\n","region_proposal_nms_threshold = 0.9 #@param{type:\"number\"}\n","\n","learning_rate = 0.003 #@param {type:\"number\"}\n","\n","#@markdown ###Loss weights\n","\n","region_proposal_class_loss =  1#@param {type:\"number\"}\n","region_proposal_class_loss = float(region_proposal_class_loss)\n","\n","region_proposal_bbox_loss =  1#@param {type:\"number\"}\n","region_proposal_bbox_loss = float(region_proposal_bbox_loss)\n","\n","mrcnn_class_loss =  1#@param {type:\"number\"}\n","mrcnn_class_loss = float(mrcnn_class_loss)\n","\n","mrcnn_bbox_loss =  1#@param {type:\"number\"}\n","mrcnn_bbox_loss = float(mrcnn_bbox_loss)\n","\n","mrcnn_mask_loss =  1#@param {type:\"number\"}\n","mrcnn_mask_loss = float(mrcnn_mask_loss)\n","\n","# Path to trained weights file\n","COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","\n","# Directory to save logs and model checkpoints, if not provided\n","# through the command line argument --logs\n","DEFAULT_LOGS_DIR = model_path\n","\n","dataset_train = ClassDataset()\n","dataset_train.load_image_csv(Training_source, \"Training\")\n","dataset_train.prepare()\n","\n","print(\"Class Count: {}\".format(dataset_train.num_classes))\n","for i, info in enumerate(dataset_train.class_info):\n","    print(\"{:3}. {:50}\".format(i, info['name']))\n","\n","############################################################\n","#  Configurations\n","############################################################\n","\n","\n","class ClassConfig(Config):\n","    \"\"\"Configuration for training on the toy  dataset.\n","    Derives from the base Config class and overrides some values.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = model_name\n","\n","    # We use a GPU with 12GB memory, which can fit two images.\n","    # Adjust down if you use a smaller GPU.\n","    IMAGES_PER_GPU = batch_size\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = len(dataset_train.class_names) # Background + nucleus\n","\n","    # Number of training steps per epoch\n","    STEPS_PER_EPOCH = (len(os.listdir(Training_source+\"/Training\"))/2)  // IMAGES_PER_GPU\n","    VALIDATION_STEPS = (len(os.listdir(Training_source+\"/Validation\"))/2) // IMAGES_PER_GPU\n","\n","    # Skip detections with < 90% confidence\n","    # DETECTION_MIN_CONFIDENCE = detection_confidence\n","\n","    LEARNING_RATE = learning_rate\n","\n","    DETECTION_MIN_CONFIDENCE = 0\n","\n","    # Backbone network architecture\n","    # Supported values are: resnet50, resnet101\n","    BACKBONE = \"resnet101\"\n","\n","    # Input image resizing\n","    # Random crops of size 64x64\n","    IMAGE_RESIZE_MODE = image_resize_mode #\"crop\"\n","    IMAGE_MIN_DIM = 128\n","    IMAGE_MAX_DIM = 128\n","    IMAGE_MIN_SCALE = 2.0\n","\n","    # Length of square anchor side in pixels\n","    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n","\n","    # ROIs kept after non-maximum supression (training and inference)\n","    POST_NMS_ROIS_TRAINING = 2000\n","    POST_NMS_ROIS_INFERENCE = 4000\n","\n","    # Non-max suppression threshold to filter RPN proposals.\n","    # You can increase this during training to generate more propsals.\n","    RPN_NMS_THRESHOLD = region_proposal_nms_threshold\n","\n","    # How many anchors per image to use for RPN training\n","    RPN_TRAIN_ANCHORS_PER_IMAGE = 128\n","\n","    # Image mean (RGB)\n","    MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\n","\n","    # If enabled, resizes instance masks to a smaller size to reduce\n","    # memory load. Recommended when using high-resolution images.\n","    USE_MINI_MASK = False\n","    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n","\n","    # Number of ROIs per image to feed to classifier/mask heads\n","    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n","    # enough positive proposals to fill this and keep a positive:negative\n","    # ratio of 1:3. You can increase the number of proposals by adjusting\n","    # the RPN NMS threshold.\n","    TRAIN_ROIS_PER_IMAGE = 128\n","\n","    # Maximum number of ground truth instances to use in one image\n","    MAX_GT_INSTANCES = 100\n","\n","    # Max number of final detections per image\n","    DETECTION_MAX_INSTANCES = 200\n","\n","    LOSS_WEIGHTS = {\n","        \"rpn_class_loss\": region_proposal_class_loss,\n","        \"rpn_bbox_loss\": region_proposal_bbox_loss,\n","        \"mrcnn_class_loss\": mrcnn_class_loss,\n","        \"mrcnn_bbox_loss\": mrcnn_bbox_loss,\n","        \"mrcnn_mask_loss\": mrcnn_mask_loss\n","    }\n","\n","if Training_depth == \"Head_layers_only\":\n","  layers = \"heads\"\n","elif Training_depth == \"3+ resnet layers\":\n","  layers = \"3+\"\n","elif Training_depth == \"4+ resnet layers\":\n","  layers = \"4+\"\n","elif Training_depth == \"5+ resnet layers\":\n","  layers = \"5+\"\n","else:\n","  layers = \"all\"\n","\n","config = ClassConfig()\n","# Training dataset\n","# dataset_train = ClassDataset()\n","# num_classes = dataset_train.load_image_csv(Training_source, \"Training\")\n","# dataset_train.prepare()\n","# print(\"Class Count: {}\".format(dataset_train.num_classes))\n","# for i, info in enumerate(dataset_train.class_info):\n","#     print(\"{:3}. {:50}\".format(i, info['name']))\n","\n","# Load and display random samples\n","image_ids = np.random.choice(dataset_train.image_ids, 1)\n","for image_id in image_ids:\n","    image = dataset_train.load_image(image_id)\n","    mask, class_ids = dataset_train.load_mask(image_id)\n","    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names, limit=dataset_train.num_classes-1)\n","\n","# plt.savefig(base_path + '/TrainingDataExample_MaskRCNN.png',bbox_inches='tight',pad_inches=0)\n","\n","# image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n","#         dataset_train, config, image_id, use_mini_mask=False)\n","\n","# visualize.display_instances(image, bbox, mask, class_ids, dataset_train.class_names,\n","#                             show_bbox=False)\n","model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=DEFAULT_LOGS_DIR)\n","config.display()\n","Use_pretrained_model = False\n","Use_Data_augmentation = False"]},{"cell_type":"markdown","metadata":{"id":"PzWJwWFGlYZi"},"source":["## **3.2. Data augmentation**\n","\n","---\n","\n","<font size = 4> Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation is not necessary for training and if the dataset the `Use_Data_Augmentation` box can be unticked.\n","\n","<font size = 4> If the box is ticked a simple augmentation of horizontal and vertical flipping will be applied to the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"d0BwRHRElaSD"},"outputs":[],"source":["#@markdown ##**Augmentation Options**\n","\n","Use_Data_augmentation = True #@param {type:\"boolean\"}\n","\n","if Use_Data_augmentation == True:\n","  # Number of training steps per epoch\n","  class AugClassConfig(ClassConfig):\n","    STEPS_PER_EPOCH = 10*((len(os.listdir(Training_source+\"/Training\"))/2)  // batch_size)\n","    VALIDATION_STEPS = 10*((len(os.listdir(Training_source+\"/Validation\"))/2) // batch_size)\n","  \n","if Use_Data_augmentation:\n","  config = AugClassConfig()"]},{"cell_type":"markdown","metadata":{"id":"uJjmzKGHk_p9"},"source":["\n","## **3.3. Using weights from a pre-trained model as initial weights**\n","---\n","<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a MaskRCNN model**. \n","\n","<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"3JsrRmNbgNeL"},"outputs":[],"source":["# @markdown ##Loading weights from a pre-trained network\n","\n","Use_pretrained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If yes, please provide the path to the model (this path should end with the file extension .h5):\n","pretrained_model_path = \"\" #@param {type:\"string\"}\n","\n","if Use_Data_augmentation == True:\n","  config = AugClassConfig()\n","else:\n","  config = ClassConfig()\n","\n","model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=DEFAULT_LOGS_DIR)\n","model.load_weights(pretrained_model_path, by_name=True)"]},{"cell_type":"markdown","metadata":{"id":"rTWfoQEPuPad"},"source":["# **4. Train the network**\n","---"]},{"cell_type":"markdown","metadata":{"id":"CRPOHMNSo0Sj"},"source":["## **4.1. Train the network**\n","---\n","<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n","\n","<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Li__jcfsTzs6"},"outputs":[],"source":["#@markdown ##Start training\n","\n","pdf_export(config, augmentation = Use_Data_augmentation, pretrained_model=Use_pretrained_model)\n","\n","if os.path.exists(model.log_dir+\"/Quality Control\"):\n","  shutil.rmtree(model.log_dir+\"/Quality Control\")\n","os.makedirs(model.log_dir+\"/Quality Control\")\n","\n","start = time.time()\n","#Here, we start the model training\n","train_csv(model, Training_source, augmentation=Use_Data_augmentation, epochs = number_of_epochs, layers = layers)\n","dt = time.time() - start\n","mins, sec = divmod(dt, 60) \n","hour, mins = divmod(mins, 60) \n","print(\"Time elapsed:\",hour, \"hour(s)\",mins,\"min(s)\",round(sec),\"sec(s)\")\n","\n","new_model_name = os.path.basename(model.log_dir)\n","#Here, we just save some interesting parameters from training as a csv file\n","if not os.path.exists(model_path+'/'+new_model_name+'/Quality Control/class_names.csv'):\n","  with open(model_path+'/'+new_model_name+'/Quality Control/class_names.csv','w') as class_count_csv:\n","    class_writer = csv.writer(class_count_csv)\n","    for class_name in dataset_train.class_names:\n","      class_writer.writerow([class_name])\n","\n","if os.path.exists(model_path+'/'+new_model_name+'/Quality Control/training_evaluation.csv'):\n","  with open(model_path+'/'+new_model_name+'/Quality Control/training_evaluation.csv','a') as csvfile:\n","    writer = csv.writer(csvfile)\n","    #print('hello')\n","    #writer.writerow(['epoch','loss','val_loss','learning rate'])\n","    model_starting_checkpoint = int(pretrained_model_path[-7:-3])\n","    for i in range(len(model.keras_model.history.history['loss'])):\n","      writer.writerow([str(model_starting_checkpoint+i),model.keras_model.history.history['loss'][i], str(learning_rate)])\n","else:\n","  with open(model_path+'/'+new_model_name+'/Quality Control/training_evaluation.csv','w') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow(['epoch','loss','val_loss','learning rate'])\n","    for i in range(len(model.keras_model.history.history['loss'])):\n","      writer.writerow([str(i+1),model.keras_model.history.history['loss'][i], model.keras_model.history.history['val_loss'][i], str(learning_rate)])\n","\n","pdf_export(config, trained=True, augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model)"]},{"cell_type":"markdown","metadata":{"id":"n0-RUNbruHa6"},"source":["# **5. Evaluate your model**\n","---\n","\n","<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model. \n","\n","<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"b10mT10YtngQ"},"outputs":[],"source":["#@markdown ###Do you want to assess the model you just trained ?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, please provide the name of the model folder:\n","\n","QC_model_folder = \"\" #@param {type:\"string\"}\n","\n","if (Use_the_current_trained_model): \n","  QC_model_folder = model_path+'/'+new_model_name\n","\n","QC_model_name = os.path.basename(QC_model_folder)\n","\n","if os.path.exists(QC_model_folder):\n","  print(\"The \"+QC_model_name+\" model will be evaluated\")\n","else:\n","  W  = '\\033[0m'  # white (normal)\n","  R  = '\\033[31m' # red\n","  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","  print('Please make sure you provide a valid model path before proceeding further.')"]},{"cell_type":"markdown","metadata":{"id":"xOOXTMHkLqYq"},"source":["## **5.1. Inspection of the loss function**\n","---\n","\n","<font size = 4>First, it is good practice to evaluate the training progress by comparing the training loss with the validation loss. The latter is a metric which shows how well the network performs on a subset of unseen data which is set aside from the training dataset. For more information on this, see for example [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols *et al.*\n","\n","<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target.\n","\n","<font size = 4>**Validation loss** describes the same error value between the model's prediction on a validation image and compared to it's target.\n","\n","<font size = 4>During training both values should decrease before reaching a minimal value which does not decrease further even after more training. Comparing the development of the validation loss with the training loss can give insights into the model's performance.\n","\n","<font size = 4>Decreasing **Training loss** and **Validation loss** indicates that training is still necessary and increasing the `number_of_epochs` is recommended. Note that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required. If the **Validation loss** suddenly increases again an the **Training loss** simultaneously goes towards zero, it means that the network is overfitting to the training data. In other words the network is remembering the exact patterns from the training data and no longer generalizes well to unseen data. In this case the training dataset has to be increased.\n","\n","<font size = 4>In this notebook, the training loss curves are plotted using **tensorboard**. However, all the training results are also logged in a csv file in your model folder."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-BpIBHDiOTqK"},"outputs":[],"source":["#@markdown ##Play the cell to show a plot of training errors vs. epoch number\n","\n","if os.path.exists(QC_model_folder):\n","  os.chdir(QC_model_folder)\n","  %load_ext tensorboard\n","  %tensorboard --logdir \"$QC_model_folder\"\n","else:\n","  print(\"The chosen model or path does not exist. Check if your model_name was saved with a timestamp.\")"]},{"cell_type":"markdown","metadata":{"id":"PdJFjEXRKApD"},"source":["## **5.2. Error mapping and quality metrics estimation**\n","---\n","\n","<font size = 4>This section will display an overlay of the input images ground-truth (solid lines) and predicted boxes (dashed lines). Additionally, the below cell will show the mAP value of the model on the QC data together with plots of the Precision-Recall curves for all the classes in the dataset. If you want to read in more detail about these scores, we recommend [this brief explanation](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173).\n","\n","<font size = 4> In a nutshell:\n","\n","<font size = 4>**Precision:** This is the proportion of the correct classifications (true positives) in all the predictions made by the model.\n","\n","<font size = 4>**Recall:** This is the proportion of the detected true positives in all the detectable data.\n","\n","<font size = 4> The files provided in the \"QC_data_folder\" should be under a subfolder called validation which contains the images (e.g. as .jpg) and annotations (.csv files)!"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"8yhm7a3gAFdK"},"outputs":[],"source":["#@markdown ### Provide the path to your quality control dataset.\n","DEFAULT_LOGS_DIR = base_path + \"/gdrive/MyDrive\"\n","QC_data_folder = \"\" #@param {type:\"string\"}\n","#Result_folder = \"\" #@param {type:\"string\"}\n","\n","# model name and path\n","\n","#Use_the_current_trained_model = False #@param {type:\"boolean\"}\n","\n","#@markdown #####During training, the model files are automatically saved inside a folder named after model_name in section 3. Provide the path to this folder below.\n","#QC_model_folder = base_path + \"/gdrive/MyDrive/maskrcnn_nucleus20210202T1206\" #@param {type:\"string\"}\n","\n","#@markdown ###Choose the checkpoint you want to evauluate:\n","Checkpoint =  8#@param {type:\"integer\"}\n","\n","#Load the dataset\n","dataset_val = ClassDataset()\n","dataset_val.load_image_csv(QC_data_folder, \"Validation\")\n","dataset_val.prepare()\n","\n","# Activate the (pre-)trained model\n","\n","detection_min_confidence = 0.35 #@param{type:\"number\"}\n","region_proposal_nms_threshold = 0.99 #@param{type:\"number\"}\n","resize_mode = \"none\" #@param[\"none\",\"square\",\"crop\",\"pad64\"]\n","\n","class InferenceConfig(ClassConfig):\n","    IMAGE_RESIZE_MODE = resize_mode\n","    RPN_NMS_THRESHOLD = region_proposal_nms_threshold\n","    NAME = \"nucleus\"\n","    IMAGES_PER_GPU = 1\n","    # Number of classes (including background)\n","    DETECTION_MIN_CONFIDENCE = detection_min_confidence\n","    NUM_CLASSES = len(dataset_val.class_names)  # Background + nucleus\n","    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n","    POST_NMS_ROIS_INFERENCE = 15000\n","inference_config = InferenceConfig()\n","\n","# Recreate the model in inference mode\n","#if Use_the_current_trained_model:\n","model = modellib.MaskRCNN(mode=\"inference\", \n","                          config=inference_config,\n","                          model_dir=QC_model_folder)\n","# else:\n","#   model = modellib.MaskRCNN(mode=\"inference\", \n","#                             config=inference_config,\n","#                             model_dir=QC_model_folder)\n","\n","# Get path to saved weights\n","if Checkpoint < 10:\n","  qc_model_path = QC_model_folder+\"/mask_rcnn_\"+QC_model_name[:-13]+\"_000\"+str(Checkpoint)+\".h5\"\n","elif Checkpoint < 100:\n","  qc_model_path = QC_model_folder+\"/mask_rcnn_\"+QC_model_name[:-13]+\"_00\"+str(Checkpoint)+\".h5\"\n","elif Checkpoint < 1000:\n","  qc_model_path = QC_model_folder+\"/mask_rcnn_\"+QC_model_name[:-13]+\"_0\"+str(Checkpoint)+\".h5\"\n","\n","# Load trained weights\n","print(\"Loading weights from \", qc_model_path)\n","model.load_weights(qc_model_path, by_name=True)\n","\n","# dataset_val = ClassDataset()\n","# num_classes = dataset_val.load_image_csv(QC_data_folder, \"Validation\")\n","# dataset_val.prepare()\n","\n","image_id = random.choice(dataset_val.image_ids)\n","original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","    modellib.load_image_gt(dataset_val, inference_config, \n","                           image_id, use_mini_mask=False)\n","\n","results = model.detect([original_image], verbose=1)\n","r = results[0]\n","visualize.display_differences(original_image, gt_bbox, gt_class_id, gt_mask, r['rois'], r['class_ids'], r['scores'], r['masks'], dataset_val.class_names, iou_threshold = 0.8, score_threshold= 0.8)\n","# visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n","#                             dataset_val.class_names, figsize=(8, 8))\n","\n","save_image(original_image, \"QC_example_data.png\", r['rois'], r['masks'],\n","        r['class_ids'],r['scores'],dataset_val.class_names,\n","      scores_thresh=0,mode=0,save_dir=QC_model_folder+'/Quality Control')"]},{"cell_type":"markdown","metadata":{"id":"IuXEDjWAK6pO"},"source":["## **5.3. Precision-Recall Curve**\n","\n","<font size = 4> The p-r curve can give a quantification how well the model\n","<font size = 4>Since the training saves model checkpoints for each epoch, you should choose which one you want to use for quality control in the `Checkpoint` box."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lzoGZUoCxpSc"},"outputs":[],"source":["#@markdown ###Show the precision-recall curve of the QC data\n","#@markdown Choose an IoU threshold for the p-r plot (between 0 and 1), ignore that the plot title says AP@50:\n","\n","iou_threshold = 0.3 #@param{type:\"number\"}\n","mAP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n","               r['rois'], r['class_ids'], r['scores'], r['masks'],\n","               iou_threshold=iou_threshold)\n","visualize.plot_precision_recall(mAP, precisions, recalls)\n","plt.savefig(QC_model_folder+'/Quality Control/P-R_curve_'+QC_model_name+'.png',bbox_inches='tight',pad_inches=0)\n","\n","gt_match, pred_match, overlaps = utils.compute_matches(gt_bbox, gt_class_id, gt_mask, r['rois'], r['class_ids'], r['scores'], r['masks'])\n","\n","#TO DO: Implement for multiclasses\n","if len(dataset_val.class_names) == 2:\n","  with open (QC_model_folder+'/Quality Control/QC_results.csv','w') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow(['class','gt instances','True positives','False Negatives', 'IoU threshold', 'mAP'])\n","    for index in dataset_val.class_names:\n","      if index != 'BG':\n","        writer.writerow([index, str(len(gt_match)), str(len(pred_match)), str(len(gt_match)-len(pred_match)), str(iou_threshold), str(mAP)])\n","        qc_pdf_export()\n","else:\n","  print('Your dataset has more than one class. This means certain features may not be enabled. We are working on implementing this section fully for multiple classes.')"]},{"cell_type":"markdown","metadata":{"id":"MGBi1lB2vSOr"},"source":["# **6. Using the trained model**\n","\n","---\n","\n","<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"HrQPXU0DvWIT"},"source":["## **6.1. Generate prediction(s) from unseen dataset**\n","---\n","\n","<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as restored image stacks (ImageJ-compatible TIFF images).\n","\n","<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n","\n","<font size = 4>**`Result_folder`:** This folder will contain the predicted output images."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7FttSetXvdTB"},"outputs":[],"source":["#@markdown ### Provide the path to your dataset and to the folder where the predictions are saved, then play the cell to predict outputs from your unseen images.\n","DEFAULT_LOGS_DIR = base_path + \"/gdrive/MyDrive\"\n","Data_folder = \"\" #@param {type:\"string\"}\n","Result_folder = \"\" #@param {type:\"string\"}\n","\n","# model name and path\n","#@markdown ###Do you want to use the current trained model?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, provide the name of the model and path to model folder:\n","Prediction_model_folder = \"\" #@param {type:\"string\"}\n","\n","if Use_the_current_trained_model:\n","  Prediction_model_folder = model_path+'/'+new_model_name\n","\n","#@markdown ###Choose the checkpoint you want to evaluate:\n","Checkpoint = 8#@param {type:\"integer\"}\n","\n","if os.path.exists(Prediction_model_folder+'/Quality Control/class_names.csv'):\n","  print('Prediction classes detected! The model will predict the following classes:')\n","  class_names = []\n","  with open(Prediction_model_folder+'/Quality Control/class_names.csv', 'r') as class_names_csv:\n","    csvreader = csv.reader(class_names_csv)\n","    for row in csvreader:\n","      print(row[0])\n","      class_names.append(row[0])\n","\n","\n","detection_min_confidence = 0.1 #@param{type:\"number\"}\n","region_proposal_nms_threshold = 0.99 #@param{type:\"number\"}\n","resize_mode = \"none\" #@param[\"none\",\"square\",\"crop\",\"pad64\"]\n","post_nms_rois = 10000 #@param{type:\"integer\"}\n","\n","\n","#Load the dataset\n","dataset_val = ClassDataset()\n","dataset_val.load_image_csv(Data_folder, \"Validation\")\n","dataset_val.prepare()\n","\n","  # Activate the (pre-)trained model\n","class InferenceConfig(ClassConfig):\n","    IMAGE_RESIZE_MODE = resize_mode\n","    IMAGE_MIN_DIM = 128\n","    IMAGE_MAX_DIM = 128\n","    IMAGE_MIN_SCALE = 2.0\n","    RPN_NMS_THRESHOLD = region_proposal_nms_threshold\n","    #DETECTION_NMS_THRESHOLD = 0.0\n","    NAME = \"nucleus\"\n","    IMAGES_PER_GPU = 1\n","    # Number of classes (including background)\n","    DETECTION_MIN_CONFIDENCE = detection_min_confidence\n","    NUM_CLASSES = len(dataset_val.class_names)  # Background + nucleus\n","    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n","    POST_NMS_ROIS_INFERENCE = post_nms_rois\n","\n","inference_config = InferenceConfig()\n","\n","# Recreate the model in inference mode\n","model = modellib.MaskRCNN(mode=\"inference\", \n","                          config=inference_config,\n","                          model_dir=Prediction_model_folder)\n","\n","# Get path to saved weights\n","if Checkpoint < 10:\n","  pred_model_path = Prediction_model_folder+\"/mask_rcnn_\"+os.path.basename(Prediction_model_folder[:-13])+\"_000\"+str(Checkpoint)+\".h5\"\n","elif Checkpoint < 100:\n","  pred_model_path = Prediction_model_folder+\"/mask_rcnn_\"+os.path.basename(Prediction_model_folder[:-13])+\"_00\"+str(Checkpoint)+\".h5\"\n","elif Checkpoint < 1000:\n","  pred_model_path = Prediction_model_folder+\"/mask_rcnn_\"+os.path.basename(Prediction_model_folder[:-13])+\"_0\"+str(Checkpoint)+\".h5\"\n","\n","# Load trained weights\n","print(\"Loading weights from \", pred_model_path)\n","model.load_weights(pred_model_path, by_name=True)\n","\n","#@markdown ###Choose how you would like to export the predictions:\n","Export_mode = \"image with class_name,score and mask\" #@param[\"image with bbox, class_name, scores, masks\",\"image with bbox,class_name and score\",\"image with class_name,score and mask\",\"mask with black background\"]\n","if Export_mode == \"image with bbox, class_name, scores, masks\":\n","  export_mode = 0\n","elif Export_mode == \"image with bbox,class_name and score\":\n","  export_mode = 1\n","elif Export_mode == \"image with class_name,score and mask\":\n","  export_mode = 2\n","elif Export_mode == \"mask with black background\":\n","  export_mode = 3\n","\n","\n","file_path = os.path.join(Data_folder, 'Validation')\n","for input in os.listdir(file_path):\n","  if input.endswith('.png'):\n","    image = io.imread(os.path.join(file_path,input))\n","    results = model.detect([image], verbose=0)\n","    r = results[0]\n","    save_image(image, \"predicted_\"+input, r['rois'], r['masks'],\n","      r['class_ids'],r['scores'],class_names,\n","    scores_thresh=0,mode=export_mode,save_dir=Result_folder)\n"]},{"cell_type":"markdown","metadata":{"id":"Yu4OGubv59qa"},"source":["## **6.2. Inspect the predicted output**\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YnWgQZmlIuv9"},"outputs":[],"source":["#@markdown ##Run this cell to display a randomly chosen input with predicted mask.\n","\n","detection_min_confidence = 0.1 #@param{type:\"number\"}\n","region_proposal_nms_threshold = 0.99 #@param{type:\"number\"}\n","resize_mode = \"none\" #@param[\"none\",\"square\",\"crop\",\"pad64\"]\n","post_nms_rois = 10000 #@param{type:\"integer\"}\n","\n","  # Activate the (pre-)trained model\n","class InferenceConfig(ClassConfig):\n","    IMAGE_RESIZE_MODE = resize_mode\n","    IMAGE_MIN_DIM = 128\n","    IMAGE_MAX_DIM = 128\n","    IMAGE_MIN_SCALE = 2.0\n","    RPN_NMS_THRESHOLD = region_proposal_nms_threshold\n","    #DETECTION_NMS_THRESHOLD = 0.0\n","    NAME = \"nucleus\"\n","    IMAGES_PER_GPU = 1\n","    # Number of classes (including background)\n","    DETECTION_MIN_CONFIDENCE = detection_min_confidence\n","    NUM_CLASSES = len(dataset_val.class_names)  # Background + nucleus\n","    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n","    POST_NMS_ROIS_INFERENCE = post_nms_rois\n","\n","inference_config = InferenceConfig()\n","\n","\n","model = modellib.MaskRCNN(mode=\"inference\", \n","                          config=inference_config,\n","                          model_dir=Prediction_model_folder)\n","\n","model.load_weights(pred_model_path, by_name=True)\n","example_image = random.choice(os.listdir(os.path.join(Data_folder,'Validation')))\n","\n","if example_image.endswith('.csv'):\n","  example_image = example_image[:-4]\n","\n","display_image = io.imread(file_path+'/'+example_image)\n","results = model.detect([display_image], verbose=0)\n","\n","r = results[0]\n","\n","visualize.display_instances(display_image, r['rois'], r['masks'], r['class_ids'], \n","                            class_names, r['scores'], ax=get_ax())"]},{"cell_type":"markdown","metadata":{"id":"BrosGM4Z50gX"},"source":["## **6.3. Download your predictions**\n","---\n","\n","<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."]},{"cell_type":"markdown","metadata":{"id":"JYfEsBazHhkW"},"source":["# **7. Version log**\n","---\n","\n","<font size = 4>**v1.14.1**:  \n","*  Fixed the PDF generation (fpdf2, delimiter, Cournier)\n","*  Removed the paths to /content\n","\n","<font size = 4>**v1.13**:  \n","\n","\n","*   This notebook is new as ZeroCostDL4Mic version 1.13. and is currently a beta version. \n","*  Further edits to this notebook in future versions will be updated in this cell."]},{"cell_type":"markdown","metadata":{"id":"F3zreN5K5S2S"},"source":["# **Thank you for using MaskRCNN**!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["YrTo6T74i7s0","RZL8pqcEi0KY","3yywetML0lUX","F3zreN5K5S2S"],"name":"MaskRCNN_ZeroCostDL4Mic.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}, "language_info": {  "codemirror_mode": {"name": "ipython","version": 3},"file_extension": ".py","mimetype": "text/x-python","name": "python","nbconvert_exporter": "python","pygments_lexer": "ipython3","version": "3.11.0"}},"nbformat":4,"nbformat_minor":0}
