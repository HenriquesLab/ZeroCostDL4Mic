{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"U-net_3D_ZeroCostDL4Mic.ipynb","provenance":[{"file_id":"1owWtQQucUxUOZMaPh2x_mxe_qXKHCZhp","timestamp":1588074588514},{"file_id":"159ARwlQE7-zi0EHxunOF_YPFLt-ZVU5x","timestamp":1587562499898},{"file_id":"1W-7NHehG5MRFILvZZzhPWWnOdJMkadb2","timestamp":1586332290412},{"file_id":"1pUetEQICxYWkYVaQIgdRH1EZBTl7oc2A","timestamp":1586292199692},{"file_id":"1MD36ZkM6XR9EuV12zimJmfCjzyeYZFWq","timestamp":1586269469061},{"file_id":"16A2mbaHzlEElntS8qkFBOsBvZG-mUeY6","timestamp":1586253795726},{"file_id":"1gJlcjOiSxr2buDOxmcFbT_d-GqwLjXtK","timestamp":1583343225796},{"file_id":"10yGI51WzHfgWgZAyE-EbkZFEvIOd6CP6","timestamp":1583171396283}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"V9zNGvape2-I","colab_type":"text"},"source":["# **3D U-Net**\n","\n","<font size = 4> The 3D U-Net was first introduced by [Çiçek et al](https://arxiv.org/abs/1606.06650) for learning dense volumetric segmentations from sparsely annotated ground-truth data building upon the original U-Net architecture by [Ronneberger et al](https://arxiv.org/abs/1505.04597).\n","\n","---\n","\n","<font size = 4>*Disclaimer*:\n","\n","<font size = 4>This notebook is inspired by the *Zero-Cost Deep-Learning to Enhance Microscopy* project ([ZeroCostDL4Mic](https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki)) and was created by Daniel Krentzel. The source code for this implementation can be found [here](https://github.com/krentzd/unet-3d).\n","\n","<font size = 4>This notebook is laregly based on the following paper: \n","\n","<font size = 4>[**3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation**](https://arxiv.org/pdf/1606.06650.pdf), Çiçek Ö, Abdulkadir A, Lienkamp SS, Brox T, Ronneberger O. International conference on medical image computing and computer-assisted intervention 2016 Oct 17 (pp. 424-432). Springer, Cham.\n","\n","<font size = 4>The following two Python libraries play an important role in the notebook: \n","\n","1. <font size = 4>[**Elasticdeform**](https://github.com/gvtulder/elasticdeform)\n"," by Gijs van Tulder was used to augment the 3D training data using elastic grid-based deformations as described in the original 3D U-Net paper. \n","\n","2. <font size = 4>[**Tifffile**](https://github.com/cgohlke/tifffile) by Christoph Gohlke is a great library for reading and writing TIFF files. \n","\n","<font size = 4>The [example dataset](https://www.epfl.ch/labs/cvlab/data/data-em/) represents a 5x5x5µm section taken from the CA1 hippocampus region of the brain with annotated mitochondria and was acquired by Graham Knott and Marco Cantoni at EPFL.\n","\n","\n","<font size = 4>**Please also cite the original paper and relevant Python libraries when using or developing this notebook.**"]},{"cell_type":"markdown","metadata":{"id":"jWAz2i7RdxUV","colab_type":"text"},"source":["# **How to use this notebook?**\n","\n","---\n","\n","<font size = 4>Video describing how to use ZeroCostDL4Mic notebooks are available on youtube:\n","  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n","  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n","\n","\n","---\n","###**Structure of a notebook**\n","\n","<font size = 4>The notebook contains two types of cells:  \n","\n","<font size = 4>**Text cells** provide information and can be modified by double-clicking the cell. You are currently reading a text cell. You can create a new one by clicking `+ Text`.\n","\n","<font size = 4>**Code cells** contain code which can be modfied by selecting the cell. To execute the cell, move your cursor to the `[]`-symbol on the left side of the cell (a play button should appear). Click it to execute the cell. Once the cell is fully executed, the animation stops. You can create a new coding cell by clicking `+ Code`.\n","\n","---\n","###**Table of contents, Code snippets** and **Files**\n","\n","<font size = 4>Three tabs are located on the upper left side of the notebook:\n","\n","1. <font size = 4>*Table of contents* contains the structure of the notebook. Click the headers to move quickly between sections.\n","\n","2. <font size = 4>*Code snippets* provides a wide array of example code specific to Google Colab. You can ignore this when using this notebook.\n","\n","3. <font size = 4>*Files* displays the current working directory. We will mount your Google Drive in Section 1.2. so that you can access your files and save them permanently.\n","\n","<font size = 4>**Important:** All uploaded files are purged once the runtime ends.\n","\n","<font size = 4>**Note:** The directory *sample data* in *Files* contains default files. Do not upload anything there!\n","\n","---\n","###**Making changes to the notebook**\n","\n","<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive by clicking *File* -> *Save a copy in Drive*.\n","\n","<font size = 4>To **edit a cell**, double click on the text. This will either display the source code (in code cells) or the [markdown](https://colab.research.google.com/notebooks/markdown_guide.ipynb#scrollTo=70pYkR9LiOV0) (in text cells).\n","You can use `#` in code cells to comment out parts of the code. This allows you to keep the original piece of code while not executing it."]},{"cell_type":"markdown","metadata":{"id":"vNMDQHm0Ah-Z","colab_type":"text"},"source":["#**0. Before getting started**\n","---\n","\n","<font size = 4>As the network operates in three dimensions, certain consideration should be given to correctly pre-processing the data. Ensure that the structure of interest does not substantially change between slices - image volumes with isotropic pixelsizes are ideal for this architecture.\n","\n","<font size = 4>Each image volume must be provided as a **multipage TIFF file** to maintain the correct ordering of individual image slices. If more than one image volume has been annotated, source and target files must be named identically and placed in separate directories. In case only one image volume has been annotated, source and target file do not have to be placed in separate directories and can be named differently, as long as their paths are explicitly provided in Section 3. \n","\n","<font size = 4>**Prepare two datasets** (*training* and *testing*) for quality control puproses. Make sure that the *testing* dataset does not overlap with the *training* dataset and is ideally sourced from a different acquisiton and sample to ensure robustness of the trained model. \n","\n","\n","---\n","\n","\n","### **Directory structure**\n","\n","<font size = 4>Make sure to adhere to one of the following directory structures. If only one annotated training volume exists, choose the first structure. In case more than one training volume is available, choose the second structure.\n","\n","<font size = 4>**Structure 1:** Only one training volume\n","```\n","path/to/directory/with/one/training/volume\n","│--training_source.tif\n","│--training_target.tif\n","|   \n","│--testing_source.tif\n","|--testing_target.tif \n","|\n","|--data_to_predict_on.tif\n","|--prediction_results.tif\n","\n","```\n","<font size = 4>**Structure 2:** Various training volumes\n","```\n","path/to/directory/with/various/training/volumes\n","│--testing_source.tif\n","|--testing_target.tif \n","|\n","└───training\n","|   └───source\n","|   |   |--training_volume_one.tif\n","|   |   |--training_volume_two.tif\n","|   |   |--...\n","|   |   |--training_volume_n.tif\n","|   |\n","|   └───target\n","|       |--training_volume_one.tif\n","|       |--training_volume_two.tif\n","|       |--...\n","|       |--training_volume_n.tif\n","|\n","|--data_to_predict_on.tif\n","|--prediction_results.tif\n","```\n","<font size = 4>**Note:** Naming directories is completely up to you, as long as the paths are correctly specified throughout the notebook.\n","\n","\n","---\n","\n","\n","### **Important note**\n","\n","* <font size = 4>If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do so), you will need to run **Sections 1 - 4**, then use **Section 5** to assess the quality of your model and **Section 6** to run predictions using the model that you trained.\n","\n","* <font size = 4>If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **Sections 1 and 2** to set up the notebook, then use **Section 5** to assess the quality of your model.\n","\n","* <font size = 4> If you only wish to **Run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **Sections 1 and 2** to set up the notebook, then use **Section 6** to run the predictions on the desired model.\n","---"]},{"cell_type":"code","metadata":{"id":"fFdz-rHnQxld","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##**Download example dataset**\n","\n","#@markdown <font size = 4> This usually takes a few minutes. The images are saved in *example_dataset*.\n","\n","import requests  \n","import os\n","from tqdm.notebook import tqdm \n","\n","def make_directory(dir):\n","    if not os.path.exists(dir):\n","        os.makedirs(dir)\n","\n","def download_from_url(url, save_as):\n","    file_url = url\n","    r = requests.get(file_url, stream=True)  \n","  \n","    with open(save_as, 'wb') as file:  \n","        for block in tqdm(r.iter_content(chunk_size = 1024), desc = 'Downloading ' + os.path.basename(save_as),  total=126875, ncols=1000):\n","            if block:\n","                file.write(block)  \n","\n","\n","make_directory('example_dataset')\n","\n","download_from_url('https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/training.tif', 'example_dataset/training.tif')\n","download_from_url('https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/training_groundtruth.tif', 'example_dataset/training_groundtruth.tif')\n","download_from_url('https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/testing.tif', 'example_dataset/testing.tif')\n","download_from_url('https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/testing_groundtruth.tif', 'example_dataset/testing_groundtruth.tif')\n","\n","print('Example dataset successfully downloaded!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMNHVZfHmbKb","colab_type":"text"},"source":["# **1. Initialise the Colab session**\n","---\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BCPhV-pe-syw","colab_type":"text"},"source":["\n","## **1.1. Check GPU access and Python version**\n","---\n","\n","<font size = 4>By default, Colab sessions run Python 3 with GPU acceleration. You can manually set this by:\n","\n","1. <font size = 4>Going to **Runtime -> Change runtime type**\n","\n","2. <font size = 4>**Runtime type: Python 3** *(This notebook uses Python 3)*\n","\n","3. <font size = 4>**Accelator: GPU** *(Graphics Processing Unit)*\n"]},{"cell_type":"code","metadata":{"id":"r9eqe5TazD5o","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run this cell to check if you have GPU access\n","%tensorflow_version 1.x\n","\n","import tensorflow as tf\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBrnApIUBgxv","colab_type":"text"},"source":["## **1.2. Mount Google Drive**\n","---\n","<font size = 4> To use this notebook with your **own data**, place it in a folder on **Google Drive** following one of the directory structures outlined in **Section 0**.\n","\n","1. <font size = 4> **Run** the **cell** below to mount your Google Drive and follow the link. \n","\n","2. <font size = 4>**Sign in** to your Google account and press 'Allow'. \n","\n","3. <font size = 4>Next, copy the **authorization code**, paste it into the cell and press enter. This will allow Colab to read and write data from and to your Google Drive. \n","\n","4. <font size = 4> Once this is done, your data can be viewed in the **Files tab** on the top left of the notebook after hitting 'Refresh'."]},{"cell_type":"code","metadata":{"id":"01Djr8v-5pPk","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run this cell to connect your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vspvj5Q2ijd4","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Unzip pre-trained model directory\n","\n","#@markdown 1.  <font size = 4>Upload a zipped model directory using the *Files* tab\n","#@markdown 2.  <font size = 4>Run this cell to unzip your model file\n","#@markdown 3.  <font size = 4>The model directory will appear in the *Files* tab \n","\n","from google.colab import files\n","\n","zipped_model_file = \"\" #@param {type:\"string\"}\n","\n","!unzip $zipped_model_file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4yWFoJNnoin","colab_type":"text"},"source":["# **2. Install 3D U-Net dependencies**\n","---\n"]},{"cell_type":"code","metadata":{"id":"3u2mXn3XsWzd","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Install dependencies and instantiate network\n","\n","#Put the imported code and libraries here\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","try:\n","    import elasticdeform\n","except:\n","    !pip install elasticdeform\n","    import elasticdeform\n","\n","try:\n","    import tifffile\n","except:\n","    !pip install tifffile\n","    import tifffile\n","\n","import os\n","import csv\n","import random\n","import h5py\n","import imageio\n","import math\n","import shutil\n","\n","import pandas as pd\n","from glob import glob\n","from tqdm.notebook import tqdm\n","\n","from skimage import transform\n","from skimage import exposure\n","from skimage import color\n","\n","from scipy.ndimage import zoom\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","from keras import backend as K\n","\n","from keras.layers import Conv3D\n","from keras.layers import BatchNormalization\n","from keras.layers import ReLU\n","from keras.layers import MaxPooling3D\n","from keras.layers import Conv3DTranspose\n","from keras.layers import Input\n","from keras.layers import Concatenate\n","from keras.models import Model\n","from keras.utils import Sequence\n","from keras.callbacks.callbacks import ModelCheckpoint\n","from keras.callbacks.callbacks import CSVLogger\n","from keras.callbacks.callbacks import Callback\n","\n","from ipywidgets import interact\n","from ipywidgets import interactive\n","from ipywidgets import fixed\n","from ipywidgets import interact_manual \n","import ipywidgets as widgets\n","\n","print(\"Depencies installed and imported.\")\n","\n","# Define MultiPageTiffGenerator class\n","class MultiPageTiffGenerator(Sequence):\n","\n","    def __init__(self,\n","                 source_path,\n","                 target_path,\n","                 batch_size=1,\n","                 shape=(128,128,32,1),\n","                 augment=True,\n","                 val_split=0.2,\n","                 is_val=False,\n","                 random_crop=True,\n","                 downscale=1):\n","\n","        # If directory with various multi-page tiffiles is provided read as list\n","        if os.path.isfile(source_path):\n","            self.dir_flag = False\n","            self.source = tifffile.imread(source_path)\n","            self.target = tifffile.imread(target_path).astype(np.bool)\n","\n","        elif os.path.isdir(source_path):\n","            self.dir_flag = True\n","            self.source_dir_list = glob(os.path.join(source_path, '*'))\n","            self.target_dir_list = glob(os.path.join(target_path, '*'))\n","\n","            self.source_dir_list.sort()\n","            self.target_dir_list.sort()\n","\n","        self.shape = shape\n","        self.batch_size = batch_size\n","        self.augment = augment\n","        self.val_split = val_split\n","        self.is_val = is_val\n","        self.random_crop = random_crop\n","        self.downscale = downscale\n","\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # If various multi-page tiff files provided sum all images within each\n","        # Expected number of non-augmented images is 1/3 of entire training set, hence multiply lenght by 3\n","        if self.augment:\n","            augment_factor = 3\n","        else:\n","            augment_factor = 1\n","    \n","        if self.dir_flag:\n","            num_of_imgs = 0\n","            for tiff_path in self.source_dir_list:\n","                num_of_imgs += tifffile.imread(tiff_path).shape[0]\n","            xy_shape = tifffile.imread(self.source_dir_list[0]).shape[1:]\n","\n","            if self.is_val:\n","                if self.random_crop:\n","                    crop_volume = self.shape[0] * self.shape[1] * self.shape[2]\n","                    volume = xy_shape[0] * xy_shape[1] * self.val_split * num_of_imgs\n","                    return math.floor(augment_factor * crop_volume / (volume * self.batch_size))\n","                else:\n","                    return math.floor(self.val_split * num_of_imgs / self.batch_size)\n","            else:\n","                if self.random_crop:\n","                    crop_volume = self.shape[0] * self.shape[1] * self.shape[2]\n","                    volume = xy_shape[0] * xy_shape[1] * (1 - self.val_split) * num_of_imgs\n","                    return math.floor(augment_factor * crop_volume / (volume * self.batch_size))\n","\n","                else:\n","                    return math.floor(augment_factor*(1 - self.val_split) * num_of_imgs/self.batch_size)\n","        else:\n","            if self.is_val:\n","                if self.random_crop:\n","                    crop_volume = self.shape[0] * self.shape[1] * self.shape[2]\n","                    volume = self.source.shape[0] * self.source.shape[1] * self.val_split * self.source.shape[2]\n","                    return math.floor(augment_factor * volume / (crop_volume * self.batch_size))\n","                else:\n","                    return math.floor((self.val_split * self.source.shape[0] / self.batch_size))\n","            else:\n","                if self.random_crop:\n","                    crop_volume = self.shape[0] * self.shape[1] * self.shape[2]\n","                    volume = self.source.shape[0] * self.source.shape[1] * (1 - self.val_split) * self.source.shape[2]\n","                    return math.floor(augment_factor * volume / (crop_volume * self.batch_size))\n","                else:\n","                    return math.floor(augment_factor * (1 - self.val_split) * self.source.shape[0] / self.batch_size)\n","\n","    def __getitem__(self, idx):\n","\n","        source_batch = np.empty((self.batch_size,\n","                                 self.shape[0],\n","                                 self.shape[1],\n","                                 self.shape[2],\n","                                 self.shape[3]))\n","        target_batch = np.empty((self.batch_size,\n","                                 self.shape[0],\n","                                 self.shape[1],\n","                                 self.shape[2],\n","                                 self.shape[3]))\n","\n","        for batch in range(self.batch_size):\n","            # Modulo operator ensures IndexError is avoided\n","            stack_start = self.batch_list[(idx+batch*self.shape[2])%len(self.batch_list)]\n","\n","            if self.dir_flag:\n","                self.source = tifffile.imread(self.source_dir_list[stack_start[0]])\n","                self.target = tiffile.imread(self.target_dir_list[stack_start[0]]).astype(np.bool)\n","\n","            src_list = []\n","            tgt_list = []\n","            for i in range(stack_start[1], stack_start[1]+self.shape[2]):\n","                src = self.source[i]\n","                src = transform.downscale_local_mean(src, (self.downscale, self.downscale))\n","                if not self.random_crop:\n","                    src = transform.resize(src, (self.shape[0], self.shape[1]), mode='constant', preserve_range=True)\n","                src = src/255\n","                src_list.append(src)\n","\n","                tgt = self.target[i]\n","                tgt = transform.downscale_local_mean(tgt, (self.downscale, self.downscale))\n","                if not self.random_crop:\n","                    tgt = transform.resize(tgt, (self.shape[0], self.shape[1]), mode='constant', preserve_range=True)\n","                tgt_list.append(tgt)\n","\n","            if self.random_crop:\n","                if src.shape[0] == self.shape[0]:\n","                    x_rand = 0\n","                if src.shape[1] == self.shape[1]:\n","                    y_rand = 0\n","                if src.shape[0] > self.shape[0]:\n","                    x_rand = np.random.randint(src.shape[0] - self.shape[0])\n","                if src.shape[1] > self.shape[1]:\n","                    y_rand = np.random.randint(src.shape[1] - self.shape[1])\n","                if src.shape[0] < self.shape[0] or src.shape[1] < self.shape[1]:\n","                    raise ValueError('Patch shape larger than (downscaled) source shape')\n","            \n","            for i in range(self.shape[2]):\n","                if self.random_crop:\n","                    src = src_list[i]\n","                    tgt = tgt_list[i]\n","                    src_crop = src[x_rand:self.shape[0]+x_rand, y_rand:self.shape[1]+y_rand]\n","                    tgt_crop = tgt[x_rand:self.shape[0]+x_rand, y_rand:self.shape[1]+y_rand]\n","                else:\n","                    src_crop = src_list[i]\n","                    tgt_crop = tgt_list[i]\n","\n","                source_batch[batch,:,:,i,0] = src_crop\n","                target_batch[batch,:,:,i,0] = tgt_crop\n","\n","        if self.augment:\n","            # On-the-fly data augmentation\n","            rand = np.random.random()\n","            # Data augmentation by reversing stack\n","            if rand > 2/3:\n","                source_batch_rev = source_batch[::-1]\n","                target_batch_rev = target_batch[::-1]\n","\n","                return source_batch_rev, target_batch_rev\n","            # Data augmentation by elastic deformation\n","            elif rand < 2/3 and rand > 1/3:\n","                [source_batch_deform, target_batch_deform] = elasticdeform.deform_random_grid([source_batch, target_batch],\n","                                                                                              axis=(1, 2, 3),\n","                                                                                              sigma=5,\n","                                                                                              points=3,\n","                                                                                              order=4) # points=2 is better imo\n","                target_batch_deform_bin = target_batch_deform > 0.25\n","\n","                return source_batch_deform, target_batch_deform_bin\n","            else:\n","                return source_batch, target_batch\n","        else:\n","            return source_batch, target_batch\n","\n","    def on_epoch_end(self):\n","        # Validation split performed here\n","        self.batch_list = []\n","        # Create batch_list of all combinations of tifffile and stack position\n","        if self.dir_flag:\n","            for i in range(len(self.source_dir_list)):\n","                num_of_pages = tifffile.imread(self.source_dir_list[i]).shape[0]\n","                if self.is_val:\n","                    start_page = num_of_pages-math.floor(self.val_split*num_of_pages)\n","                    for j in range(start_page, num_of_pages-self.shape[2]):\n","                      self.batch_list.append([i, j])\n","                else:\n","                    last_page = math.floor((1-self.val_split)*num_of_pages)\n","                    for j in range(last_page-self.shape[2]):\n","                        self.batch_list.append([i, j])\n","        else:\n","            num_of_pages = self.source.shape[0]\n","            if self.is_val:\n","                start_page = num_of_pages-math.floor(self.val_split*num_of_pages)\n","                for j in range(start_page, num_of_pages-self.shape[2]):\n","                    self.batch_list.append([0, j])\n","\n","            else:\n","                last_page = math.floor((1-self.val_split)*num_of_pages)\n","                for j in range(last_page-self.shape[2]):\n","                    self.batch_list.append([0, j])\n","        \n","        if self.is_val and (len(self.batch_list) <= 0):\n","            raise ValueError('validation_split too small! Increase val_split or decrease z-depth')\n","        random.shuffle(self.batch_list)\n","\n","    def class_weights(self):\n","\n","        ones = 0\n","        pixels = 0\n","\n","        if self.dir_flag:\n","            for i in range(len(self.target_dir_list)):\n","                tgt = tifffile.imread(self.target_dir_list[i]).astype(np.bool)\n","                ones += np.sum(tgt)\n","                pixels += tgt.shape[0]*tgt.shape[1]*tgt.shape[2]\n","        else:\n","          ones = np.sum(self.target)\n","          pixels = self.target.shape[0]*self.target.shape[1]*self.target.shape[2]\n","        p_ones = ones/pixels\n","        p_zeros = 1-p_ones\n","\n","        # Return swapped probability to increase weight of unlikely class\n","        return p_ones, p_zeros\n","\n","# Define custom loss and dice coefficient\n","def dice_coefficient(y_true, y_pred):\n","\n","    eps = 1e-6\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f*y_pred_f)\n","\n","    return (2.*intersection)/(K.sum(y_true_f*y_true_f)+K.sum(y_pred_f*y_pred_f)+eps)\n","\n","def weighted_binary_crossentropy(zero_weight, one_weight):\n","\n","    def _weighted_binary_crossentropy(y_true, y_pred):\n","\n","        binary_crossentropy = K.binary_crossentropy(y_true, y_pred)\n","\n","        weight_vector = y_true*one_weight+(1.-y_true)*zero_weight\n","        weighted_binary_crossentropy = weight_vector*binary_crossentropy\n","\n","        return K.mean(weighted_binary_crossentropy)\n","\n","    return _weighted_binary_crossentropy\n","\n","# Custom callback showing sample prediction\n","class SampleImageCallback(Callback):\n","\n","    def __init__(self, model, sample_data, model_path, save=False):\n","        self.model = model\n","        self.sample_data = sample_data\n","        self.model_path = model_path\n","        self.save = save\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","\n","        sample_predict = self.model.predict_on_batch(self.sample_data)\n","\n","        f=plt.figure(figsize=(16,8))\n","        plt.subplot(1,2,1)\n","        plt.imshow(self.sample_data[0,:,:,0,0], interpolation='nearest', cmap='gray')\n","        plt.title('Sample source')\n","        plt.axis('off');\n","\n","        plt.subplot(1,2,2)\n","        plt.imshow(sample_predict[0,:,:,0,0], interpolation='nearest', cmap='magma')\n","        plt.title('Predicted target')\n","        plt.axis('off');\n","\n","        plt.show()\n","\n","        if self.save:\n","            plt.savefig(self.model_path + '/epoch_' + str(epoch+1) + '.png')\n","\n","\n","# Define Unet3D class\n","class Unet3D:\n","\n","    def __init__(self,\n","                 shape=(256,256,16,1)):\n","\n","        if isinstance(shape, str):\n","            shape = eval(shape)\n","\n","        self.shape = shape\n","        \n","        input_tensor = Input(self.shape, name='input')\n","\n","        self.model = self.unet_3D(input_tensor)\n","\n","    def down_block_3D(self, input_tensor, filters):\n","\n","        x = Conv3D(filters=filters, kernel_size=(3,3,3), padding='same')(input_tensor)\n","        x = BatchNormalization()(x)\n","        x = ReLU()(x)\n","\n","        x = Conv3D(filters=filters*2, kernel_size=(3,3,3), padding='same')(x)\n","        x = BatchNormalization()(x)\n","        x = ReLU()(x)\n","\n","        return x\n","\n","    def up_block_3D(self, input_tensor, concat_layer, filters):\n","\n","        x = Conv3DTranspose(filters, kernel_size=(2,2,2), strides=(2,2,2))(input_tensor)\n","\n","        x = Concatenate()([x, concat_layer])\n","\n","        x = Conv3D(filters=filters, kernel_size=(3,3,3), padding='same')(x)\n","        x = BatchNormalization()(x)\n","        x = ReLU()(x)\n","\n","        x = Conv3D(filters=filters*2, kernel_size=(3,3,3), padding='same')(x)\n","        x = BatchNormalization()(x)\n","        x = ReLU()(x)\n","\n","        return x\n","\n","    def unet_3D(self, input_tensor, filters=32):\n","\n","        d1 = self.down_block_3D(input_tensor, filters=filters)\n","        p1 = MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), data_format='channels_last')(d1)\n","        d2 = self.down_block_3D(p1, filters=filters*2)\n","        p2 = MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), data_format='channels_last')(d2)\n","        d3 = self.down_block_3D(p2, filters=filters*4)\n","        p3 = MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), data_format='channels_last')(d3)\n","\n","        d4 = self.down_block_3D(p3, filters=filters*8)\n","\n","        u1 = self.up_block_3D(d4, d3, filters=filters*4)\n","        u2 = self.up_block_3D(u1, d2, filters=filters*2)\n","        u3 = self.up_block_3D(u2, d1, filters=filters)\n","\n","        output_tensor = Conv3D(filters=1, kernel_size=(1,1,1), activation='sigmoid')(u3)\n","\n","        return Model(inputs=[input_tensor], outputs=[output_tensor])\n","\n","    def summary(self):\n","        return self.model.summary()\n","\n","    def train(self, \n","              epochs, \n","              batch_size, \n","              train_source, \n","              train_target, \n","              model_path, \n","              model_name, \n","              val_split=0.2, \n","              augment=True, \n","              ckpt_period=1, \n","              save_best_ckpt_only=False, \n","              ckpt_path=None,\n","              random_crop=True,\n","              downscaling=1):\n","\n","        train_generator = MultiPageTiffGenerator(train_source,\n","                                                 train_target,\n","                                                 batch_size=batch_size,\n","                                                 shape=self.shape,\n","                                                 augment=augment,\n","                                                 val_split=val_split,\n","                                                 random_crop=random_crop,\n","                                                 downscale=downscaling)\n","\n","        val_generator = MultiPageTiffGenerator(train_source,\n","                                               train_target,\n","                                               batch_size=batch_size,\n","                                               shape=self.shape,\n","                                               augment=False,\n","                                               val_split=val_split,\n","                                               is_val=True,\n","                                               random_crop=random_crop,\n","                                               downscale=downscaling)\n","\n","        class_weight_zero, class_weight_one = train_generator.class_weights()\n","\n","        self.model.compile(optimizer='adam',\n","                           loss=weighted_binary_crossentropy(class_weight_zero, class_weight_one),\n","                           metrics=[dice_coefficient])\n","\n","        if ckpt_path is not None:\n","            self.model.load_weights(ckpt_path)\n","\n","        full_model_path = os.path.join(model_path, model_name)\n","\n","        if not os.path.exists(full_model_path):\n","            os.makedirs(full_model_path)\n","        \n","        log_dir = full_model_path + '/Quality Control'\n","\n","        if not os.path.exists(log_dir):\n","            os.makedirs(log_dir)\n","        \n","        ckpt_dir =  full_model_path + '/ckpt'\n","\n","        if not os.path.exists(ckpt_dir):\n","            os.makedirs(ckpt_dir)\n","\n","        csv_out_name = log_dir + '/training_evaluation.csv'\n","        if ckpt_path is None:\n","            csv_logger = CSVLogger(csv_out_name)\n","        else:\n","            csv_logger = CSVLogger(csv_out_name, append=True)\n","\n","        if save_best_ckpt_only:\n","            ckpt_name = ckpt_dir + '/' + model_name + '.hdf5'\n","        else:\n","            ckpt_name = ckpt_dir + '/' + model_name + '_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.hdf5'\n","        \n","        model_ckpt = ModelCheckpoint(ckpt_name,\n","                                     verbose=1,\n","                                     period=ckpt_period,\n","                                     save_best_only=save_best_ckpt_only,\n","                                     save_weights_only=True)\n","\n","        sample_batch = val_generator[0][0]\n","        sample_img = SampleImageCallback(self.model, \n","                                         sample_batch, \n","                                         model_path)\n","\n","        self.model.fit_generator(generator=train_generator,\n","                                 validation_data=val_generator,\n","                                 validation_steps=math.floor(len(val_generator)/batch_size),\n","                                 epochs=epochs,\n","                                 callbacks=[csv_logger,\n","                                            model_ckpt,\n","                                            sample_img])\n","\n","        last_ckpt_name = ckpt_dir + '/' + model_name + '_last.hdf5'\n","        self.model.save_weights(last_ckpt_name)\n","\n","    def predict(self, input, ckpt_path, z_range=None, downscaling=None, true_patch_size=None):\n","\n","        self.model.load_weights(ckpt_path)\n","\n","        if isinstance(downscaling, str):\n","            downscaling = eval(downscaling)\n","\n","        if math.isnan(downscaling):\n","            downscaling = None\n","\n","        if isinstance(true_patch_size, str):\n","            true_patch_size = eval(true_patch_size)\n","        \n","        if not isinstance(true_patch_size, tuple): \n","            if math.isnan(true_patch_size):\n","                true_patch_size = None\n","\n","        if isinstance(input, str):\n","            src_volume = tifffile.imread(input)\n","        elif isinstance(input, np.ndarray):\n","            src_volume = input\n","        else:\n","            raise TypeError('Input is not path or numpy array!')\n","        \n","        in_size = src_volume.shape\n","\n","        if downscaling or true_patch_size is not None:\n","            x_scaling = 0\n","            y_scaling = 0\n","\n","            if true_patch_size is not None:\n","                x_scaling += true_patch_size[0]/self.shape[0]\n","                y_scaling += true_patch_size[1]/self.shape[1]\n","            if downscaling is not None:\n","                x_scaling += downscaling\n","                y_scaling += downscaling\n","\n","            src_list = []\n","            for i in range(src_volume.shape[0]):\n","                 src_list.append(transform.downscale_local_mean(src_volume[i], (int(x_scaling), int(y_scaling))))\n","            src_volume = np.array(src_list)          \n","\n","        if z_range is not None:\n","            src_volume = src_volume[z_range[0]:z_range[1]]\n","\n","        src_volume = src_volume/255\n","       \n","        src_array = np.zeros((1,\n","                              math.ceil(src_volume.shape[1]/self.shape[0])*self.shape[0], \n","                              math.ceil(src_volume.shape[2]/self.shape[1])*self.shape[1],\n","                              math.ceil(src_volume.shape[0]/self.shape[2])*self.shape[2], \n","                              self.shape[3]))\n","\n","        for i in range(src_volume.shape[0]):\n","            src_array[0,:src_volume.shape[1],:src_volume.shape[2],i,0] = src_volume[i]\n","\n","        pred_array = np.empty(src_array.shape)\n","\n","        for i in range(math.ceil(src_volume.shape[1]/self.shape[0])):\n","          for j in range(math.ceil(src_volume.shape[2]/self.shape[1])):\n","            for k in range(math.ceil(src_volume.shape[0]/self.shape[2])):\n","                pred_temp = self.model.predict(src_array[:,\n","                                                         i*self.shape[0]:i*self.shape[0]+self.shape[0],\n","                                                         j*self.shape[1]:j*self.shape[1]+self.shape[1],\n","                                                         k*self.shape[2]:k*self.shape[2]+self.shape[2]])\n","                pred_array[:,\n","                           i*self.shape[0]:i*self.shape[0]+self.shape[0],\n","                           j*self.shape[1]:j*self.shape[1]+self.shape[1],\n","                           k*self.shape[2]:k*self.shape[2]+self.shape[2]] = pred_temp\n","                           \n","        pred_volume = np.rollaxis(np.squeeze(pred_array), -1)[:src_volume.shape[0],:src_volume.shape[1],:src_volume.shape[2]]            \n","\n","        if downscaling is not None:\n","            pred_list = []\n","            for i in range(pred_volume.shape[0]):\n","                 pred_list.append(transform.resize(pred_volume[i], (in_size[1], in_size[2]), preserve_range=True))\n","            pred_volume = np.array(pred_list)\n","\n","        return pred_volume\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fw0kkTU6CsU4","colab_type":"text"},"source":["# **3. Select your model parameters**\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"CB6acvUFtWqd","colab_type":"text"},"source":["## **Paths to training data and model**\n","\n","* <font size = 4>**`training_source`** and **`training_target`** specify the paths to the training data. They can either be a single multipage TIFF file each or directories containing various multipage TIFF files in which case target and source files must be named identically within the respective directories. See Section 0 for a detailed description of the necessary directory structure.\n","\n","* <font size = 4>**`model_name`** will be used when naming checkpoints. Adhere to a `lower_case_with_underscores` naming convention and beware of using the name of an existing model within the same folder, as it will be overwritten.\n","\n","* <font size = 4>**`model_path`** specifies the directory where the model checkpoints and quality control logs will be saved.\n","\n","\n","<font size = 4>**Note:** You can copy paths from the 'Files' tab by right-clicking any folder or file and selecting 'Copy path'. \n","\n","## **Training parameters**\n","\n","* <font size = 4>**`number_of_epochs`** is the number of times the entire training data will be seen by the model. *Default: >100*\n","\n","* <font size = 4>**`batch_size`** is the number of training patches of size `patch_size` that will be bundled together at each training step. *Deafult: 1*\n","\n","* <font size = 4>**`patch_size`** specifies the size of the three-dimensional training patches that will be fed to the model. In order to avoid errors, preferably use a square aspect ratio or stick to the advanced parameters. *Default: <(512, 512, 16)*\n","\n","* <font size = 4>**`validation_split_in_percent`** is the relative amount of training data that will be set aside for validation. *Default: 20* \n","\n","* <font size = 4>**`downscaling_in_xy`** downscales the training images by the specified amount in x and y. This is useful to enforce isotropic pixel-size if the z resolution is lower than the xy resolution in the training volume or to capture a larger field-of-view while decreasing the memory requirements. *Default: 1*\n","\n","* <font size = 4>**`image_pre_processing`** selects whether the training images are randomly cropped during training or resized to `patch_size`. Choose `randomly crop to patch_size` to shrink the field-of-view of the training images to the `patch_size`. *Default: resize to patch_size* \n","\n","<font size = 4>**Note:** If a *ResourceExhaustedError* is raised in Section 4.1. during training, decrease `batch_size` and `patch_size`. Decrease `batch_size` first and if the error persists at `batch_size = 1`, reduce the `patch_size`.  \n","\n","\n","## **Data augmentation**\n"," \n","* <font size = 4>**`apply_data_augmentation`** ensures that data augmentation is randomly applied to the training data at each training step. This includes inverting the order of the slices within a training patch, as well as applying elastic grid-based deformations as described in the original 3D U-Net paper. Augmenting the training data increases robustness of the model by simulating possible variations within the training data which avoids it from overfitting on small datasets. We therefore strongly recommended selecting data augmentation. *Default: True*\n","\n","<font size = 4>**Note:** The number of steps per epoch are calculated as `floor(augment_factor * (1 - validation_split) * num_of_slices / batch_size)` if `image_pre_processing` is `resize to patch_size` where `augment_factor` is three if `apply_data_augmentation` is `True` and one otherwise. The `num_of_slices` is the overall number of slices (z-depth) in the training set across all provided image volumes. If `image_pre_processing` is `randomly crop to patch_size`, the number of steps per epoch are calculated as `floor(augment_factor * volume / (crop_volume * batch_size))` where `volume` is the overall volume of the training data in pixels accounting for the validation split and `crop_volume` is defined as the volume in pixels based on the specified `patch_size`."]},{"cell_type":"code","metadata":{"id":"ewpNJ_I0Mv47","colab_type":"code","cellView":"form","colab":{}},"source":["class bcolors:\n","  WARNING = '\\033[31m'\n","\n","#@markdown ###Path to training data:\n","training_source = \"\" #@param {type:\"string\"}\n","training_target = \"\" #@param {type:\"string\"}\n","\n","#@markdown ---\n","\n","#@markdown ###Model name and path to model folder:\n","model_name = \"\" #@param {type:\"string\"}\n","model_path = \"\" #@param {type:\"string\"}\n","\n","full_model_path = os.path.join(model_path, model_name)\n","\n","#@markdown ---\n","\n","#@markdown ###Training parameters\n","number_of_epochs =   100#@param {type:\"number\"}\n","\n","#@markdown ###Default advanced parameters\n","use_default_advanced_parameters = True #@param {type:\"boolean\"}\n","\n","#@markdown <font size = 3>If not, please change:\n","\n","batch_size =  1#@param {type:\"number\"}\n","patch_size = (512,512,16) #@param {type:\"number\"} # in pixels\n","training_shape = patch_size + (1,)\n","image_pre_processing = 'resize to patch_size' #@param [\"randomly crop to patch_size\", \"resize to patch_size\"]\n","\n","validation_split_in_percent = 20 #@param{type:\"number\"}\n","downscaling_in_xy = 1 #@param {type:\"number\"} # in pixels\n","\n","\n","if image_pre_processing == \"randomly crop to patch_size\":\n","    random_crop = True\n","else:\n","    random_crop = False\n","\n","#@markdown ---\n","\n","#@markdown ###Checkpointing\n","\n","checkpointing_period = 1 #@param {type:\"number\"}\n","\n","#@markdown  <font size = 3>If chosen only the best checkpoint is saved, otherwise a checkpoint is saved every checkpoint_period epochs:\n","save_best_only = True #@param {type:\"boolean\"}\n","\n","#@markdown <font size = 3>Choose if training was interrupted:\n","resume_training = False #@param {type:\"boolean\"}\n","\n","#@markdown <font size = 3>For transfer learning, do not select resume_training and specify a checkpoint_path below:\n","checkpoint_path = \"\" #@param {type:\"string\"}\n","\n","if resume_training and checkpoint_path != \"\":\n","    print('If resume_training is True while checkpoint_path is specified, resume_training will be set to False!')\n","    resume_training = False\n","\n","#@markdown ---\n","\n","#@markdown ###Data Augmentation\n","\n","apply_data_augmentation = True #@param {type:\"boolean\"}\n"," \n","\n","# Retrieve last checkpoint\n","if resume_training:\n","    try:\n","      ckpt_dir_list = glob(full_model_path + '/ckpt/*')\n","      ckpt_dir_list.sort()\n","      last_ckpt_path = ckpt_dir_list[-1]\n","      print('Training will resume from checkpoint:', os.path.basename(last_ckpt_path))\n","    except IndexError:\n","      last_ckpt_path=None\n","      print('CheckpointError: No previous checkpoints were found, training from scratch.')\n","elif not resume_training and checkpoint_path != \"\":\n","    last_ckpt_path = checkpoint_path\n","    assert os.path.isfile(last_ckpt_path), 'checkpoint_path does not exist!'\n","else:\n","    last_ckpt_path=None\n","\n","\n","if use_default_advanced_parameters: \n","    print(\"Default advanced parameters enabled\")\n","    batch_size = 1\n","    training_shape = (256,256,8,1)\n","    validation_split_in_percent = 20\n","    downscaling_in_xy = 1\n","    random_crop = False\n","\n","# Instantiate Unet3D \n","model = Unet3D(shape=training_shape)\n","\n","#here we check that no model with the same name already exist, if so delete\n","if not resume_training and os.path.exists(full_model_path):\n","    print('!! WARNING: Folder already exists and will be overwritten !!') \n","    shutil.rmtree(full_model_path)\n","\n","if not os.path.exists(full_model_path):\n","    os.makedirs(full_model_path)\n","\n","# Show sample image\n","if os.path.isdir(training_source):\n","    training_source = glob.glob(os.path.join(training_source, '*'))[0]\n","    training_target = glob.glob(os.path.join(training_target, '*'))[0]\n","\n","src_sample = tifffile.imread(training_source)\n","src_sample = src_sample/255\n","tgt_sample = tifffile.imread(training_target).astype(np.bool)\n","\n","src_down = transform.downscale_local_mean(src_sample[0], (downscaling_in_xy, downscaling_in_xy))\n","tgt_down = transform.downscale_local_mean(tgt_sample[0], (downscaling_in_xy, downscaling_in_xy))   \n","\n","if random_crop:\n","    true_patch_size = None\n","\n","    if src_down.shape[0] == training_shape[0]:\n","      x_rand = 0\n","    if src_down.shape[1] == training_shape[1]:\n","      y_rand = 0\n","    if src_down.shape[0] > training_shape[0]:\n","      x_rand = np.random.randint(src_down.shape[0] - training_shape[0])\n","    if src_down.shape[1] > training_shape[1]:\n","      y_rand = np.random.randint(src_down.shape[1] - training_shape[1])\n","    if src_down.shape[0] < training_shape[0] or src_down.shape[1] < training_shape[1]:\n","      raise ValueError('Patch shape larger than (downscaled) source shape')\n","else:\n","    true_patch_size = src_down.shape\n","\n","def scroll_in_z(z):\n","    src_down = transform.downscale_local_mean(src_sample[z-1], (downscaling_in_xy,downscaling_in_xy))\n","    tgt_down = transform.downscale_local_mean(tgt_sample[z-1], (downscaling_in_xy,downscaling_in_xy))       \n","    if random_crop:\n","        src_slice = src_down[x_rand:training_shape[0]+x_rand, y_rand:training_shape[1]+y_rand]\n","        tgt_slice = tgt_down[x_rand:training_shape[0]+x_rand, y_rand:training_shape[1]+y_rand]\n","    else:\n","        \n","        src_slice = transform.resize(src_down, (training_shape[0], training_shape[1]), mode='constant', preserve_range=True)\n","        tgt_slice = transform.resize(tgt_down, (training_shape[0], training_shape[1]), mode='constant', preserve_range=True)\n","\n","    f=plt.figure(figsize=(16,8))\n","    plt.subplot(1,2,1)\n","    plt.imshow(src_slice, cmap='gray')\n","    plt.title('Training source (z = ' + str(z) + ')', fontsize=15)\n","    plt.axis('off')\n","\n","    plt.subplot(1,2,2)\n","    plt.imshow(tgt_slice, cmap='magma')\n","    plt.title('Training target (z = ' + str(z) + ')', fontsize=15)\n","    plt.axis('off')\n","\n","print('This is what the training images will look like with the chosen settings')\n","interact(scroll_in_z, z=widgets.IntSlider(min=1, max=src_sample.shape[0], step=1, value=0));\n","\n","# Save model parameters\n","params =  {'training_source': training_source,\n","           'training_target': training_target,\n","           'model_name': model_name,\n","           'model_path': model_path,\n","           'number_of_epochs': number_of_epochs,\n","           'batch_size': batch_size,\n","           'training_shape': training_shape,\n","           'downscaling': downscaling_in_xy,\n","           'true_patch_size': true_patch_size,\n","           'val_split': validation_split_in_percent/100,\n","           'random_crop': random_crop,\n","           'data_augmentation': apply_data_augmentation}\n","\n","params_df = pd.DataFrame.from_dict(params, orient='index')\n","# Check if file is actually made\n","params_df.to_csv(os.path.join(full_model_path, 'params.csv'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQndJj70FzfL","colab_type":"text"},"source":["# **4. Train the network**\n","---"]},{"cell_type":"markdown","metadata":{"id":"wQPz0F6JlvJR","colab_type":"text"},"source":["## **4.1. Train the network**\n","---\n","\n","\n","<font size = 4>**CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training times must be less than 12 hours! If training takes longer than 12 hours, please decrease `number_of_epochs`."]},{"cell_type":"code","metadata":{"id":"opWPgUl7erct","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Show model summary\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZnoS3rb8BSR","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Start Training\n","\n","# Start Training\n","model.train(epochs=number_of_epochs,\n","            batch_size=batch_size,\n","            train_source=training_source,\n","            train_target=training_target,\n","            model_path=model_path,\n","            model_name=model_name,\n","            val_split=validation_split_in_percent/100,\n","            augment=apply_data_augmentation,\n","            ckpt_period=checkpointing_period,\n","            save_best_ckpt_only=save_best_only,\n","            ckpt_path=last_ckpt_path,\n","            random_crop=random_crop,\n","            downscaling=downscaling_in_xy)\n","\n","print('Training successfully completed!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQjQb_J_Qyku","colab_type":"text"},"source":["##**4.3. Download your model(s) from Google Drive**\n","\n","\n","---\n","<font size = 4>Once training is complete, the trained model is automatically saved to your Google Drive, in the **`model_path`** folder that was specified in Section 3. Download the folder to avoid any unwanted surprises, since the data can be erased if you train another model using the same `model_path`."]},{"cell_type":"markdown","metadata":{"id":"2HbZd7rFqAad","colab_type":"text"},"source":["# **5. Evaluate your model**\n","---\n","\n","<font size = 4>In this section the newly trained model can be assessed for performance. This involves inspecting the loss function in Section 5.1. and employing more advanced metrics in Section 5.2.\n","\n","<font size = 4>**We highly recommend performing quality control on all newly trained models.**\n","\n"]},{"cell_type":"code","metadata":{"id":"EdcnkCr9Nbl8","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ###Model to be evaluated:\n","#@markdown <font size = 3>If left blank, the latest model defined in Section 3 will be evaluated:\n","\n","qc_model_name = \"\" #@param {type:\"string\"}\n","qc_model_path = \"\" #@param {type:\"string\"}\n","\n","if len(qc_model_path) == 0 and len(qc_model_name) == 0:\n","    qc_model_name = model_name\n","    qc_model_path = model_path\n","\n","full_qc_model_path = os.path.join(qc_model_path, qc_model_name)\n","\n","if os.path.exists(full_qc_model_path):\n","    print(qc_model_name + ' will be evaluated')\n","else:\n","    W  = '\\033[0m'  # white (normal)\n","    R  = '\\033[31m' # red\n","    print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","    print('Please make sure you provide a valid model path and model name before proceeding further.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yDY9dtzdUTLh","colab_type":"text"},"source":["## **5.1. Inspecting loss function**\n","---\n","\n","<font size = 4>**The training loss** is the error between prediction and target after each epoch calculated across the training data while the **validation loss** calculates the error on the (unseen) validation data. During training these values should decrease until converging at which point the model has been sufficiently trained. If the validation loss starts increasing while the training loss has plateaued, the model has overfit on the training data which reduces its ability to generalise. Aim to halt training before this point.\n","\n","<font size = 4>**Note:** For a more in-depth explanation please refer to [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols et al.\n","\n","\n","<font size = 4>The accuracy is another performance metric that is calculated after each epoch. We use the [Sørensen–Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to score the prediction accuracy. \n","\n"]},{"cell_type":"code","metadata":{"id":"vMzSP50kMv5p","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Visualise loss and accuracy\n","lossDataFromCSV = []\n","vallossDataFromCSV = []\n","accuracyDataFromCSV = []\n","valaccuracyDataFromCSV = []\n","\n","with open(full_qc_model_path + '/Quality Control/training_evaluation.csv', 'r') as csvfile:\n","    csvRead = csv.reader(csvfile, delimiter=',')\n","    next(csvRead)\n","    for row in csvRead:\n","        lossDataFromCSV.append(float(row[2]))\n","        vallossDataFromCSV.append(float(row[4]))\n","        accuracyDataFromCSV.append(float(row[1]))\n","        valaccuracyDataFromCSV.append(float(row[3]))\n","\n","epochNumber = range(len(lossDataFromCSV))\n","plt.figure(figsize=(15,10))\n","\n","plt.subplot(2,1,1)\n","plt.plot(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.plot(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training and validation loss', fontsize=14)\n","plt.ylabel('Loss', fontsize=12)\n","plt.xlabel('Epochs', fontsize=12)\n","plt.legend()\n","\n","plt.subplot(2,1,2)\n","plt.plot(epochNumber,accuracyDataFromCSV, label='Training accuracy')\n","plt.plot(epochNumber,valaccuracyDataFromCSV, label='Validation accuracy')\n","plt.title('Training and validation accuracy', fontsize=14)\n","plt.ylabel('Dice', fontsize=12)\n","plt.xlabel('Epochs', fontsize=12)\n","plt.legend()\n","plt.savefig(full_qc_model_path + '/Quality Control/lossCurvePlots.png')\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZOPCVN0qcYb","colab_type":"text"},"source":["## **5.2. Error mapping and quality metrics estimation**\n","---\n","<font size = 4>This section will provide both a visual indication of the model performance by comparing the overlay of the predicted and source volume."]},{"cell_type":"code","metadata":{"id":"XbL7T9bw98Ja","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Compare prediction and ground-truth on testing data\n","\n","#@markdown <font size = 4>Provide an unseen annotated dataset to determine the performance of the model:\n","\n","testing_source = \"\" #@param{type:\"string\"}\n","testing_target = \"\" #@param{type:\"string\"}\n","\n","qc_dir = full_qc_model_path + '/Quality Control'\n","predict_dir = qc_dir + '/Prediction'\n","if os.path.exists(predict_dir):\n","    shutil.rmtree(predict_dir)\n","\n","os.makedirs(predict_dir)\n","\n","predict_path = predict_dir + '/' + os.path.splitext(os.path.basename(testing_source))[0] + '_prediction.tif'\n","\n","def last_chars(x):\n","    return(x[-11:])\n","\n","try:\n","    ckpt_dir_list = glob(full_qc_model_path + '/ckpt/*')\n","    ckpt_dir_list.sort(key=last_chars)\n","    last_ckpt_path = ckpt_dir_list[0]\n","    print('Predicting from checkpoint:', os.path.basename(last_ckpt_path))\n","except IndexError:\n","    raise CheckpointError('No previous checkpoints were found, please retrain model.')\n","\n","# Load parameters\n","params = pd.read_csv(os.path.join(full_qc_model_path, 'params.csv'), names=['val'], header=0, index_col=0)   \n","\n","model = Unet3D(shape=params.loc['training_shape', 'val'])\n","\n","prediction = model.predict(testing_source, last_ckpt_path, downscaling=params.loc['downscaling', 'val'], true_patch_size=params.loc['true_patch_size', 'val'])\n","\n","tifffile.imwrite(predict_path, prediction.astype('float32'), imagej=True)\n","\n","print('Predicted images!')\n","\n","qc_metrics_path = full_qc_model_path + '/Quality Control/QC_metrics_' + qc_model_name + '.csv'\n","\n","test_target = tifffile.imread(testing_target)\n","test_source = tifffile.imread(testing_source)\n","test_prediction = tifffile.imread(predict_path)\n","\n","def scroll_in_z(z):\n","\n","    plt.figure(figsize=(25,5))\n","    # Source\n","    plt.subplot(1,4,1)\n","    plt.axis('off')\n","    plt.imshow(test_source[z-1], cmap='gray')\n","    plt.title('Source (z = ' + str(z) + ')', fontsize=15)\n","\n","    # Target (Ground-truth)\n","    plt.subplot(1,4,2)\n","    plt.axis('off')\n","    plt.imshow(test_target[z-1], cmap='magma')\n","    plt.title('Target (z = ' + str(z) + ')', fontsize=15)\n","\n","    # Prediction\n","    plt.subplot(1,4,3)\n","    plt.axis('off')\n","    plt.imshow(test_prediction[z-1], cmap='magma')\n","    plt.title('Prediction (z = ' + str(z) + ')', fontsize=15)\n","    \n","    # Overlay\n","    plt.subplot(1,4,4)\n","    plt.axis('off')\n","    plt.imshow(test_target[z-1], cmap='Greens')\n","    plt.imshow(test_prediction[z-1], alpha=0.5, cmap='Purples')\n","    plt.title('Overlay (z = ' + str(z) + ')', fontsize=15)\n","\n","interact(scroll_in_z, z=widgets.IntSlider(min=1, max=test_source.shape[0], step=1, value=0));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lIP7AOvkg5pT","colab_type":"text"},"source":["## **5.3. Determine best Intersection over Union and threshold**\n","---\n","<font size = 4>This section will provide both a visual and a quantitative indication of the model performance by comparing the overlay of the predicted and source volume, as well as computing the highest [**Intersection over Union**](https://en.wikipedia.org/wiki/Jaccard_index) (IoU) score. The IoU is also known as the Jaccard Index.  \n","\n","<font size = 4>The best threshold is calculated using the IoU. Each threshold value from 0 to 255 is tested and the threshold with the highest score is deemed the best. The IoU is calculated for the entire volume in 3D."]},{"cell_type":"code","metadata":{"id":"1hXoooMbYvxl","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Calculate Intersection over Union and best threshold \n","prediction = tifffile.imread(predict_path)\n","prediction = np.interp(prediction, (prediction.min(), prediction.max()), (0, 255))\n","\n","target = tifffile.imread(testing_target).astype(np.bool)\n","\n","def iou_vs_threshold(prediction, target):\n","    threshold_list = []\n","    IoU_scores_list = []\n","\n","    for threshold in range(0,256): \n","        mask = prediction > threshold\n","\n","        intersection = np.logical_and(target, mask)\n","        union = np.logical_or(target, mask)\n","        iou_score = np.sum(intersection) / np.sum(union)\n","\n","        threshold_list.append(threshold)\n","        IoU_scores_list.append(iou_score)\n","\n","    return threshold_list, IoU_scores_list\n","\n","threshold_list, IoU_scores_list = iou_vs_threshold(prediction, target)\n","thresh_arr = np.array(list(zip(threshold_list, IoU_scores_list)))\n","best_thresh = int(np.where(thresh_arr == np.max(thresh_arr[:,1]))[0])\n","best_iou = IoU_scores_list[best_thresh]\n","\n","print('Highest IoU is {:.4f} with a threshold of {}'.format(best_iou, best_thresh))\n","\n","def adjust_threshold(threshold, z):\n","\n","    f=plt.figure(figsize=(25,5))\n","    plt.subplot(1,4,1)\n","    plt.imshow((prediction[z-1] > threshold).astype('uint8'), cmap='magma')\n","    plt.title('Prediction (Threshold = ' + str(threshold) + ')', fontsize=15)\n","    plt.axis('off')\n","\n","    plt.subplot(1,4,2)\n","    plt.imshow(target[z-1], cmap='magma')\n","    plt.title('Target (z = ' + str(z) + ')', fontsize=15)\n","    plt.axis('off')\n","\n","    plt.subplot(1,4,3)\n","    plt.axis('off')\n","    plt.imshow(test_source[z-1], cmap='gray')\n","    plt.imshow((prediction[z-1] > threshold).astype('uint8'), alpha=0.4, cmap='Reds')\n","    plt.title('Overlay (z = ' + str(z) + ')', fontsize=15)\n","\n","    plt.subplot(1,4,4)\n","    plt.title('Threshold vs. IoU', fontsize=15)\n","    plt.plot(threshold_list, IoU_scores_list)\n","    plt.plot(threshold, IoU_scores_list[threshold], 'ro')     \n","    plt.ylabel('IoU score')\n","    plt.xlabel('Threshold')\n","    plt.show()\n","\n","interact(adjust_threshold, \n","         threshold=widgets.IntSlider(min=0, max=255, step=1, value=best_thresh),\n","         z=widgets.IntSlider(min=1, max=prediction.shape[0], step=1, value=0));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Esqnbew8uznk"},"source":["# **6. Using the trained model**\n","\n","---\n","\n","<font size = 4>Once sufficient performance of the trained model has been established using Section 5, the network can be used to segment unseen volumetric data."]},{"cell_type":"markdown","metadata":{"id":"d8wuQGjoq6eN","colab_type":"text"},"source":["## **6.1. Generate predictions from unseen dataset**\n","---\n","\n","<font size = 4>The most recently trained model can now be used to predict segmentation masks on unseen images. If you want to use an older model, leave `model_path`  blank. Predicted output images are saved in `output_path` as Image-J compatible TIFF files.\n","\n","## **Prediction parameters**\n","\n","* <font size = 4>**`source_path`** specifies the location of the source \n","image volume.\n","\n","* <font size = 4>**`output_path`** specified where the output predictions are stored.\n","\n","* <font size = 4>**`threshold`** can be calculated in Section 5 and is used to generate binary masks from the predictions.\n","\n","* <font size = 4>**`big_tiff`** should be chosen if the expected prediction exceeds 4GB. The predictions will be saved using the BigTIFF format. Beware that this might substantially reduce the prediction speed. *Default: False* \n","\n","* <font size = 4>**`prediction_depth`** is only relevant if the prediction is saved as a BigTIFF. The prediction will not be performed in one go to not deplete the memory resources. Instead, the prediction is iteratively performed on a subset of the entire volume  with shape `(source.shape[0], source.shape[1], prediction_depth)`. *Default: 32*\n","\n","* <font size = 4>**`model_path`** specifies the path to a model other than the most  recently trained."]},{"cell_type":"code","metadata":{"id":"Ps4bbZgkmV8V","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ## Download example volume\n","\n","#@markdown <font size = 4> This can take up to an hour\n","\n","import requests  \n","import os\n","from tqdm.notebook import tqdm \n","\n","\n","def download_from_url(url, save_as):\n","    file_url = url\n","    r = requests.get(file_url, stream=True)  \n","  \n","    with open(save_as, 'wb') as file:  \n","        for block in tqdm(r.iter_content(chunk_size = 1024), desc = 'Downloading ' + os.path.basename(save_as), total=3275073, ncols=1000):\n","            if block:\n","                file.write(block)  \n","\n","download_from_url('https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/volumedata.tif', 'example_dataset/volumedata.tif')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oQr1yKyBwZS","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ### Provide the path to your dataset and to the folder where the predictions are saved, then run the cell to predict outputs from your unseen images.\n","\n","source_path = \"\" #@param {type:\"string\"}\n","output_directory = \"\" #@param {type:\"string\"}\n","\n","if not os.path.exists(output_directory):\n","    os.makedirs(output_directory)\n","\n","output_path = os.path.join(output_directory, os.path.splitext(os.path.basename(source_path))[0] + '_predicted.tif')\n","#@markdown ###Prediction parameters:\n","\n","save_probability_map = False #@param {type:\"boolean\"}\n","\n","#@markdown <font size = 3>Determine best threshold in Section 5.2.\n","\n","use_calculated_threshold = True #@param {type:\"boolean\"}\n","threshold =  200#@param {type:\"number\"}\n","\n","# Tifffile library issues means that images cannot be appended to \n","#@markdown <font size = 3>Choose if prediction file exceeds 4GB or if input file is very large (above 2GB). Image volume saved as BigTIFF.\n","big_tiff = False #@param {type:\"boolean\"}\n","\n","#@markdown <font size = 3>Reduce `prediction_depth` if runtime runs out of memory during prediction. Only relevant if prediction saved as BigTIFF\n","\n","prediction_depth = 32 #@param {type:\"number\"}\n","\n","#@markdown ###Model to be evaluated\n","#@markdown <font size = 3>If left blank, the latest model defined in Section 3 will be evaluated\n","\n","full_model_path_ = \"\" #@param {type:\"string\"}\n","\n","if len(full_model_path_) == 0:\n","    full_model_path_ = os.path.join(model_path, model_name) \n","\n","\n","\n","# Load parameters\n","params = pd.read_csv(os.path.join(full_model_path_, 'params.csv'), names=['val'], header=0, index_col=0)   \n","model = Unet3D(shape=params.loc['training_shape', 'val'])\n","\n","if use_calculated_threshold:\n","    threshold = best_thresh\n","\n","def last_chars(x):\n","    return(x[-11:])\n","\n","try:\n","    ckpt_dir_list = glob(full_model_path_ + '/ckpt/*')\n","    ckpt_dir_list.sort(key=last_chars)\n","    last_ckpt_path = ckpt_dir_list[0]\n","    print('Predicting from checkpoint:', os.path.basename(last_ckpt_path))\n","except IndexError:\n","    raise CheckpointError('No previous checkpoints were found, please retrain model.')\n","\n","src = tifffile.imread(source_path)\n","\n","if src.nbytes >= 4e9:\n","    big_tiff = True\n","    print('The source file exceeds 4GB in memory, prediction will be saved as BigTIFF!')\n","\n","if not big_tiff:\n","    prediction = model.predict(src, last_ckpt_path, downscaling=params.loc['downscaling', 'val'], true_patch_size=params.loc['true_patch_size', 'val'])\n","    prediction = np.interp(prediction, (prediction.min(), prediction.max()), (0, 255))\n","    prediction = (prediction > threshold).astype('float32')\n","\n","    tifffile.imwrite(output_path, prediction, imagej=True)\n","\n","else:\n","    with tifffile.TiffWriter(output_path, bigtiff=True) as tif:\n","        for i in tqdm(range(0, src.shape[0], prediction_depth)):\n","            prediction = model.predict(src, last_ckpt_path, z_range=(i,i+prediction_depth), downscaling=params.loc['downscaling', 'val'], true_patch_size=params.loc['true_patch_size', 'val'])\n","            prediction = np.interp(prediction, (prediction.min(), prediction.max()), (0, 255))\n","            prediction = (prediction > threshold).astype('float32')\n","            \n","            for j in range(prediction.shape[0]):\n","                tif.save(prediction[j])\n","\n","if save_probability_map:\n","    prob_map_path = os.path.splitext(output_path)[0] + '_prob_map.tif'\n","    \n","    if not big_tiff:\n","        prediction = model.predict(src, last_ckpt_path, downscaling=params.loc['downscaling', 'val'], true_patch_size=params.loc['true_patch_size', 'val'])\n","        prediction = np.interp(prediction, (prediction.min(), prediction.max()), (0, 255))\n","        tifffile.imwrite(prob_map_path, prediction.astype('float32'), imagej=True)\n","\n","    else:\n","        with tifffile.TiffWriter(prob_map_path, bigtiff=True) as tif:\n","            for i in tqdm(range(0, src.shape[0], prediction_depth)):\n","                prediction = model.predict(src, last_ckpt_path, z_range=(i,i+prediction_depth), downscaling=params.loc['downscaling', 'val'], true_patch_size=params.loc['true_patch_size', 'val'])\n","                prediction = np.interp(prediction, (prediction.min(), prediction.max()), (0, 255))\n","                \n","                for j in range(prediction.shape[0]):\n","                    tif.save(prediction[j])\n","\n","print('Predictions saved as', output_path)\n","\n","src_volume = tifffile.imread(source_path)\n","pred_volume = tifffile.imread(output_path)\n","\n","def scroll_in_z(z):\n","  \n","    f=plt.figure(figsize=(25,5))\n","    plt.subplot(1,2,1)\n","    plt.imshow(src_volume[z-1], cmap='gray')\n","    plt.title('Source (z = ' + str(z) + ')', fontsize=15)\n","    plt.axis('off')\n","\n","    plt.subplot(1,2,2)\n","    plt.imshow(pred_volume[z-1], cmap='magma')\n","    plt.title('Prediction (z = ' + str(z) + ')', fontsize=15)\n","    plt.axis('off')\n","\n","interact(scroll_in_z, z=widgets.IntSlider(min=1, max=src_volume.shape[0], step=1, value=0));\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvkd66PldsXB","colab_type":"text"},"source":["## **6.2. Download your predictions**\n","---\n","\n","<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."]},{"cell_type":"code","metadata":{"id":"anzh9w6x_sGO","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Download model directory\n","#@markdown 1.  <font size = 4>Specify the model_path\n","#@markdown 2.  <font size = 4>Run this cell to zip the model directory\n","#@markdown 3.  <font size = 4>Download the zipped file from the *Files* tab on the left\n","\n","from google.colab import files\n","\n","model_path_download = \"\" #@param {type:\"string\"}\n","\n","if len(model_path_download) == 0:\n","    model_path_download = model_path\n","\n","model_path_download = os.path.basename(model_path_download)\n","\n","print('Zipping', model_path_download)\n","\n","zip_model_path = model_path_download + '.zip'\n","\n","!zip -r $zip_model_path $model_path_download\n","\n","print('Successfully saved zipped model directory as', zip_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rn9zpWpo0xNw","colab_type":"text"},"source":["\n","#**Thank you for using 3D U-Net!**"]}]}