{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quality_Control_ZeroCostDL4Mic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9zNGvape2-I"
      },
      "source": [
        "# **Quality Control notebook**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>*Disclaimer*:\n",
        "\n",
        "<font size = 4>This notebook is part of the *Zero-Cost Deep-Learning to Enhance Microscopy* project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories.\n",
        "\n",
        "\n",
        "<font size = 4>**Please also cite this original paper when using or developing this notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWAz2i7RdxUV"
      },
      "source": [
        "# **How to use this notebook?**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>Video describing how to use our notebooks are available on youtube:\n",
        "  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n",
        "  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n",
        "\n",
        "\n",
        "---\n",
        "###**Structure of a notebook**\n",
        "\n",
        "<font size = 4>The notebook contains two types of cell:  \n",
        "\n",
        "<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n",
        "\n",
        "<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n",
        "\n",
        "---\n",
        "###**Table of contents, Code snippets** and **Files**\n",
        "\n",
        "<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n",
        "\n",
        "<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n",
        "\n",
        "<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n",
        "\n",
        "<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here. \n",
        "\n",
        "<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n",
        "\n",
        "<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n",
        "\n",
        "---\n",
        "###**Making changes to the notebook**\n",
        "\n",
        "<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n",
        "\n",
        "<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n",
        "You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNMDQHm0Ah-Z"
      },
      "source": [
        "#**0. Before getting started**\n",
        "---\n",
        "<font size = 4> To use this notebook, pay attention to the data structure. The images you want to compare need to be organised in separate folders and have the same name.\n",
        "\n",
        "<font size = 4>Here's a common data structure that can work:\n",
        "*   Experiment A\n",
        "    - **Training_source**\n",
        "        - img_1.tif, img_2.tif, ... \n",
        "    - **Training_target**\n",
        "        - img_1.tif, img_2.tif, ...        \n",
        "    - **Prediction**\n",
        "        - img_1.tif, img_2.tif, ... \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4-r1gE7Iamv"
      },
      "source": [
        "# **1. Initialise the Colab session**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqBTeLaImnU"
      },
      "source": [
        "## **1.1. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. \n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Djr8v-5pPk",
        "cellView": "form"
      },
      "source": [
        "\n",
        "#@markdown ##Run this cell to connect your Google Drive to Colab\n",
        "\n",
        "#@markdown * Click on the URL. \n",
        "\n",
        "#@markdown * Sign in your Google Account. \n",
        "\n",
        "#@markdown * Copy the authorization code. \n",
        "\n",
        "#@markdown * Enter the authorization code. \n",
        "\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n",
        "\n",
        "#mounts user's Google Drive to Google Colab.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yWFoJNnoin"
      },
      "source": [
        "# **1.2. Install the dependencies**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u2mXn3XsWzd",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Install the dependencies\n",
        "\n",
        "Notebook_version = '1.13'\n",
        "Network = 'Quality_control'\n",
        "\n",
        "!pip install tifffile # contains tools to operate tiff-files\n",
        "!pip install wget\n",
        "!pip install memory_profiler\n",
        "!pip install fpdf\n",
        "%load_ext memory_profiler\n",
        "\n",
        "# ------- Common variable to all ZeroCostDL4Mic notebooks -------\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import urllib\n",
        "import os, random\n",
        "import shutil \n",
        "import zipfile\n",
        "from tifffile import imread, imsave\n",
        "import time\n",
        "import sys\n",
        "import wget\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import csv\n",
        "from glob import glob\n",
        "from scipy import signal\n",
        "from scipy import ndimage\n",
        "from skimage import io\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from skimage.util import img_as_uint\n",
        "import matplotlib as mpl\n",
        "from skimage.metrics import structural_similarity\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from astropy.visualization import simple_norm\n",
        "from skimage import img_as_float32\n",
        "from skimage.util import img_as_ubyte\n",
        "from tqdm import tqdm \n",
        "from fpdf import FPDF, HTMLMixin\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "from pip._internal.operations.freeze import freeze\n",
        "\n",
        "from tabulate import tabulate\n",
        "from astropy.visualization import simple_norm\n",
        "\n",
        "from ipywidgets import interact\n",
        "\n",
        "# Colors for the warning messages\n",
        "class bcolors:\n",
        "  WARNING = '\\033[31m'\n",
        "\n",
        "W  = '\\033[0m'  # white (normal)\n",
        "R  = '\\033[31m' # red\n",
        "\n",
        "#Disable some of the tensorflow warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Libraries installed\")\n",
        "\n",
        "# Check if this is the latest version of the notebook\n",
        "All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n",
        "print('Notebook version: '+Notebook_version)\n",
        "Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n",
        "print('Latest notebook version: '+Latest_Notebook_version)\n",
        "if Notebook_version == Latest_Notebook_version:\n",
        "  print(\"This notebook is up-to-date.\")\n",
        "else:\n",
        "  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
        "\n",
        "\n",
        "## ------------------- Instance segmentation metrics ------------------------------\n",
        "\n",
        "# Here we load the def that perform the QC, code adapted from the StarDist repo  https://github.com/mpicbg-csbd/stardist/blob/master/stardist/matching.py\n",
        "\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "matching_criteria = dict()\n",
        "\n",
        "def label_are_sequential(y):\n",
        "    \"\"\" returns true if y has only sequential labels from 1... \"\"\"\n",
        "    labels = np.unique(y)\n",
        "    return (set(labels)-{0}) == set(range(1,1+labels.max()))\n",
        "\n",
        "\n",
        "def is_array_of_integers(y):\n",
        "    return isinstance(y,np.ndarray) and np.issubdtype(y.dtype, np.integer)\n",
        "\n",
        "\n",
        "def _check_label_array(y, name=None, check_sequential=False):\n",
        "    err = ValueError(\"{label} must be an array of {integers}.\".format(\n",
        "        label = 'labels' if name is None else name,\n",
        "        integers = ('sequential ' if check_sequential else '') + 'non-negative integers',\n",
        "    ))\n",
        "    is_array_of_integers(y) or print(\"An error occured\")\n",
        "    if check_sequential:\n",
        "        label_are_sequential(y) or print(\"An error occured\")\n",
        "    else:\n",
        "        y.min() >= 0 or print(\"An error occured\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def label_overlap(x, y, check=True):\n",
        "    if check:\n",
        "        _check_label_array(x,'x',True)\n",
        "        _check_label_array(y,'y',True)\n",
        "        x.shape == y.shape or _raise(ValueError(\"x and y must have the same shape\"))\n",
        "    return _label_overlap(x, y)\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _label_overlap(x, y):\n",
        "    x = x.ravel()\n",
        "    y = y.ravel()\n",
        "    overlap = np.zeros((1+x.max(),1+y.max()), dtype=np.uint)\n",
        "    for i in range(len(x)):\n",
        "        overlap[x[i],y[i]] += 1\n",
        "    return overlap\n",
        "\n",
        "\n",
        "def intersection_over_union(overlap):\n",
        "    _check_label_array(overlap,'overlap')\n",
        "    if np.sum(overlap) == 0:\n",
        "        return overlap\n",
        "    n_pixels_pred = np.sum(overlap, axis=0, keepdims=True)\n",
        "    n_pixels_true = np.sum(overlap, axis=1, keepdims=True)\n",
        "    return overlap / (n_pixels_pred + n_pixels_true - overlap)\n",
        "\n",
        "matching_criteria['iou'] = intersection_over_union\n",
        "\n",
        "\n",
        "def intersection_over_true(overlap):\n",
        "    _check_label_array(overlap,'overlap')\n",
        "    if np.sum(overlap) == 0:\n",
        "        return overlap\n",
        "    n_pixels_true = np.sum(overlap, axis=1, keepdims=True)\n",
        "    return overlap / n_pixels_true\n",
        "\n",
        "matching_criteria['iot'] = intersection_over_true\n",
        "\n",
        "\n",
        "def intersection_over_pred(overlap):\n",
        "    _check_label_array(overlap,'overlap')\n",
        "    if np.sum(overlap) == 0:\n",
        "        return overlap\n",
        "    n_pixels_pred = np.sum(overlap, axis=0, keepdims=True)\n",
        "    return overlap / n_pixels_pred\n",
        "\n",
        "matching_criteria['iop'] = intersection_over_pred\n",
        "\n",
        "\n",
        "def precision(tp,fp,fn):\n",
        "    return tp/(tp+fp) if tp > 0 else 0\n",
        "def recall(tp,fp,fn):\n",
        "    return tp/(tp+fn) if tp > 0 else 0\n",
        "def accuracy(tp,fp,fn):\n",
        "    return tp/(tp+fp+fn) if tp > 0 else 0\n",
        "def f1(tp,fp,fn):    \n",
        "    return (2*tp)/(2*tp+fp+fn) if tp > 0 else 0\n",
        "\n",
        "def _safe_divide(x,y):\n",
        "    return x/y if y>0 else 0.0\n",
        "\n",
        "def matching(y_true, y_pred, thresh=0.5, criterion='iou', report_matches=False):\n",
        " \n",
        "    _check_label_array(y_true,'y_true')\n",
        "    _check_label_array(y_pred,'y_pred')\n",
        "    y_true.shape == y_pred.shape or _raise(ValueError(\"y_true ({y_true.shape}) and y_pred ({y_pred.shape}) have different shapes\".format(y_true=y_true, y_pred=y_pred)))\n",
        "    criterion in matching_criteria or _raise(ValueError(\"Matching criterion '%s' not supported.\" % criterion))\n",
        "    if thresh is None: thresh = 0\n",
        "    thresh = float(thresh) if np.isscalar(thresh) else map(float,thresh)\n",
        "\n",
        "    y_true, _, map_rev_true = relabel_sequential(y_true)\n",
        "    y_pred, _, map_rev_pred = relabel_sequential(y_pred)\n",
        "\n",
        "    overlap = label_overlap(y_true, y_pred, check=False)\n",
        "    scores = matching_criteria[criterion](overlap)\n",
        "    assert 0 <= np.min(scores) <= np.max(scores) <= 1\n",
        "\n",
        "    # ignoring background\n",
        "    scores = scores[1:,1:]\n",
        "    n_true, n_pred = scores.shape\n",
        "    n_matched = min(n_true, n_pred)\n",
        "\n",
        "    def _single(thr):\n",
        "        not_trivial = n_matched > 0 and np.any(scores >= thr)\n",
        "        if not_trivial:\n",
        "            # compute optimal matching with scores as tie-breaker\n",
        "            costs = -(scores >= thr).astype(float) - scores / (2*n_matched)\n",
        "            true_ind, pred_ind = linear_sum_assignment(costs)\n",
        "            assert n_matched == len(true_ind) == len(pred_ind)\n",
        "            match_ok = scores[true_ind,pred_ind] >= thr\n",
        "            tp = np.count_nonzero(match_ok)\n",
        "        else:\n",
        "            tp = 0\n",
        "        fp = n_pred - tp\n",
        "        fn = n_true - tp\n",
        "\n",
        "\n",
        "        # the score sum over all matched objects (tp)\n",
        "        sum_matched_score = np.sum(scores[true_ind,pred_ind][match_ok]) if not_trivial else 0.0\n",
        "\n",
        "        # the score average over all matched objects (tp)\n",
        "        mean_matched_score = _safe_divide(sum_matched_score, tp)\n",
        "        # the score average over all gt/true objects\n",
        "        mean_true_score    = _safe_divide(sum_matched_score, n_true)\n",
        "        panoptic_quality   = _safe_divide(sum_matched_score, tp+fp/2+fn/2)\n",
        "\n",
        "        stats_dict = dict (\n",
        "            criterion          = criterion,\n",
        "            thresh             = thr,\n",
        "            fp                 = fp,\n",
        "            tp                 = tp,\n",
        "            fn                 = fn,\n",
        "            precision          = precision(tp,fp,fn),\n",
        "            recall             = recall(tp,fp,fn),\n",
        "            accuracy           = accuracy(tp,fp,fn),\n",
        "            f1                 = f1(tp,fp,fn),\n",
        "            n_true             = n_true,\n",
        "            n_pred             = n_pred,\n",
        "            mean_true_score    = mean_true_score,\n",
        "            mean_matched_score = mean_matched_score,\n",
        "            panoptic_quality   = panoptic_quality,\n",
        "        )\n",
        "        if bool(report_matches):\n",
        "            if not_trivial:\n",
        "                stats_dict.update (\n",
        "                    # int() to be json serializable\n",
        "                    matched_pairs  = tuple((int(map_rev_true[i]),int(map_rev_pred[j])) for i,j in zip(1+true_ind,1+pred_ind)),\n",
        "                    matched_scores = tuple(scores[true_ind,pred_ind]),\n",
        "                    matched_tps    = tuple(map(int,np.flatnonzero(match_ok))),\n",
        "                )\n",
        "            else:\n",
        "                stats_dict.update (\n",
        "                    matched_pairs  = (),\n",
        "                    matched_scores = (),\n",
        "                    matched_tps    = (),\n",
        "                )\n",
        "        return namedtuple('Matching',stats_dict.keys())(*stats_dict.values())\n",
        "\n",
        "    return _single(thresh) if np.isscalar(thresh) else tuple(map(_single,thresh))\n",
        "\n",
        "\n",
        "def matching_dataset(y_true, y_pred, thresh=0.5, criterion='iou', by_image=False, show_progress=True, parallel=False):\n",
        "    \"\"\"matching metrics for list of images, see `stardist.matching.matching`\n",
        "    \"\"\"\n",
        "    len(y_true) == len(y_pred) or _raise(ValueError(\"y_true and y_pred must have the same length.\"))\n",
        "    return matching_dataset_lazy (\n",
        "        tuple(zip(y_true,y_pred)), thresh=thresh, criterion=criterion, by_image=by_image, show_progress=show_progress, parallel=parallel,\n",
        "    )\n",
        "\n",
        "\n",
        "def matching_dataset_lazy(y_gen, thresh=0.5, criterion='iou', by_image=False, show_progress=True, parallel=False):\n",
        "\n",
        "    expected_keys = set(('fp', 'tp', 'fn', 'precision', 'recall', 'accuracy', 'f1', 'criterion', 'thresh', 'n_true', 'n_pred', 'mean_true_score', 'mean_matched_score', 'panoptic_quality'))\n",
        "\n",
        "    single_thresh = False\n",
        "    if np.isscalar(thresh):\n",
        "        single_thresh = True\n",
        "        thresh = (thresh,)\n",
        "\n",
        "    tqdm_kwargs = {}\n",
        "    tqdm_kwargs['disable'] = not bool(show_progress)\n",
        "    if int(show_progress) > 1:\n",
        "        tqdm_kwargs['total'] = int(show_progress)\n",
        "\n",
        "    # compute matching stats for every pair of label images\n",
        "    if parallel:\n",
        "        from concurrent.futures import ThreadPoolExecutor\n",
        "        fn = lambda pair: matching(*pair, thresh=thresh, criterion=criterion, report_matches=False)\n",
        "        with ThreadPoolExecutor() as pool:\n",
        "            stats_all = tuple(pool.map(fn, tqdm(y_gen,**tqdm_kwargs)))\n",
        "    else:\n",
        "        stats_all = tuple (\n",
        "            matching(y_t, y_p, thresh=thresh, criterion=criterion, report_matches=False)\n",
        "            for y_t,y_p in tqdm(y_gen,**tqdm_kwargs)\n",
        "        )\n",
        "\n",
        "    # accumulate results over all images for each threshold separately\n",
        "    n_images, n_threshs = len(stats_all), len(thresh)\n",
        "    accumulate = [{} for _ in range(n_threshs)]\n",
        "    for stats in stats_all:\n",
        "        for i,s in enumerate(stats):\n",
        "            acc = accumulate[i]\n",
        "            for k,v in s._asdict().items():\n",
        "                if k == 'mean_true_score' and not bool(by_image):\n",
        "                    # convert mean_true_score to \"sum_matched_score\"\n",
        "                    acc[k] = acc.setdefault(k,0) + v * s.n_true\n",
        "                else:\n",
        "                    try:\n",
        "                        acc[k] = acc.setdefault(k,0) + v\n",
        "                    except TypeError:\n",
        "                        pass\n",
        "\n",
        "    # normalize/compute 'precision', 'recall', 'accuracy', 'f1'\n",
        "    for thr,acc in zip(thresh,accumulate):\n",
        "        set(acc.keys()) == expected_keys or _raise(ValueError(\"unexpected keys\"))\n",
        "        acc['criterion'] = criterion\n",
        "        acc['thresh'] = thr\n",
        "        acc['by_image'] = bool(by_image)\n",
        "        if bool(by_image):\n",
        "            for k in ('precision', 'recall', 'accuracy', 'f1', 'mean_true_score', 'mean_matched_score', 'panoptic_quality'):\n",
        "                acc[k] /= n_images\n",
        "        else:\n",
        "            tp, fp, fn, n_true = acc['tp'], acc['fp'], acc['fn'], acc['n_true']\n",
        "            sum_matched_score = acc['mean_true_score']\n",
        "\n",
        "            mean_matched_score = _safe_divide(sum_matched_score, tp)\n",
        "            mean_true_score    = _safe_divide(sum_matched_score, n_true)\n",
        "            panoptic_quality   = _safe_divide(sum_matched_score, tp+fp/2+fn/2)\n",
        "\n",
        "            acc.update(\n",
        "                precision          = precision(tp,fp,fn),\n",
        "                recall             = recall(tp,fp,fn),\n",
        "                accuracy           = accuracy(tp,fp,fn),\n",
        "                f1                 = f1(tp,fp,fn),\n",
        "                mean_true_score    = mean_true_score,\n",
        "                mean_matched_score = mean_matched_score,\n",
        "                panoptic_quality   = panoptic_quality,\n",
        "            )\n",
        "\n",
        "    accumulate = tuple(namedtuple('DatasetMatching',acc.keys())(*acc.values()) for acc in accumulate)\n",
        "    return accumulate[0] if single_thresh else accumulate\n",
        "\n",
        "\n",
        "# copied from scikit-image master for now (remove when part of a release)\n",
        "def relabel_sequential(label_field, offset=1):\n",
        "    \n",
        "    offset = int(offset)\n",
        "    if offset <= 0:\n",
        "        raise ValueError(\"Offset must be strictly positive.\")\n",
        "    if np.min(label_field) < 0:\n",
        "        raise ValueError(\"Cannot relabel array that contains negative values.\")\n",
        "    max_label = int(label_field.max()) # Ensure max_label is an integer\n",
        "    if not np.issubdtype(label_field.dtype, np.integer):\n",
        "        new_type = np.min_scalar_type(max_label)\n",
        "        label_field = label_field.astype(new_type)\n",
        "    labels = np.unique(label_field)\n",
        "    labels0 = labels[labels != 0]\n",
        "    new_max_label = offset - 1 + len(labels0)\n",
        "    new_labels0 = np.arange(offset, new_max_label + 1)\n",
        "    output_type = label_field.dtype\n",
        "    required_type = np.min_scalar_type(new_max_label)\n",
        "    if np.dtype(required_type).itemsize > np.dtype(label_field.dtype).itemsize:\n",
        "        output_type = required_type\n",
        "    forward_map = np.zeros(max_label + 1, dtype=output_type)\n",
        "    forward_map[labels0] = new_labels0\n",
        "    inverse_map = np.zeros(new_max_label + 1, dtype=output_type)\n",
        "    inverse_map[offset:] = labels0\n",
        "    relabeled = forward_map[label_field]\n",
        "    return relabeled, forward_map, inverse_map\n",
        "\n",
        "\n",
        "## ------------------- Image-to-image comparaison metrics ------------------------------\n",
        "\n",
        "\n",
        "## Pearson correlation\n",
        "\n",
        "\n",
        "## lpips ?\n",
        "\n",
        "\n",
        "def ssim(img1, img2):\n",
        "  return structural_similarity(img1,img2,data_range=1.,full=True, gaussian_weights=True, use_sample_covariance=False, sigma=1.5)\n",
        "\n",
        "\n",
        "def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
        "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
        "    \"\"\"Percentile-based image normalization.\"\"\"\n",
        "\n",
        "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
        "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
        "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
        "\n",
        "\n",
        "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n",
        "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
        "    if dtype is not None:\n",
        "        x   = x.astype(dtype,copy=False)\n",
        "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
        "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
        "        eps = dtype(eps)\n",
        "\n",
        "    try:\n",
        "        import numexpr\n",
        "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
        "    except ImportError:\n",
        "        x =                   (x - mi) / ( ma - mi + eps )\n",
        "\n",
        "    if clip:\n",
        "        x = np.clip(x,0,1)\n",
        "\n",
        "    return x\n",
        "\n",
        "def norm_minmse(gt, x, normalize_gt=True):\n",
        "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    normalizes and affinely scales an image pair such that the MSE is minimized  \n",
        "     \n",
        "    Parameters\n",
        "    ----------\n",
        "    gt: ndarray\n",
        "        the ground truth image      \n",
        "    x: ndarray\n",
        "        the image that will be affinely scaled \n",
        "    normalize_gt: bool\n",
        "        set to True of gt image should be normalized (default)\n",
        "    Returns\n",
        "    -------\n",
        "    gt_scaled, x_scaled \n",
        "    \"\"\"\n",
        "    if normalize_gt:\n",
        "        gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n",
        "    x = x.astype(np.float32, copy=False) - np.mean(x)    \n",
        "    gt = gt.astype(np.float32, copy=False) - np.mean(gt)    \n",
        "    scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n",
        "    return gt, scale * x\n",
        "\n",
        "\n",
        "#--------------------- Display functions --------------------------------\n",
        "\n",
        "def visualise_image_comparison_QC(image, dimension, Source_folder, Prediction_folder, Ground_truth_folder, QC_folder, QC_scores):\n",
        "  \n",
        "  img_Source = io.imread(os.path.join(Source_folder, image))\n",
        "  img_Prediction = io.imread(os.path.join(Prediction_folder, image))\n",
        "  img_GT = io.imread(os.path.join(Ground_truth_folder, image))\n",
        "\n",
        "  if dimension == \"3D\":\n",
        "    Z_plane = int(img_GT.shape[0] / 2)+1\n",
        "  \n",
        "  img_SSIM_GTvsSource = io.imread(os.path.join(QC_folder, 'SSIM_GTvsSource_'+image))\n",
        "  img_SSIM_GTvsPrediction = io.imread(os.path.join(QC_folder, 'SSIM_GTvsPrediction_'+image))\n",
        "  img_RSE_GTvsSource = io.imread(os.path.join(QC_folder, 'RSE_GTvsSource_'+image))\n",
        "  img_RSE_GTvsPrediction = io.imread(os.path.join(QC_folder, 'RSE_GTvsPrediction_'+image))\n",
        "  \n",
        "  SSIM_GTvsP_forDisplay = QC_scores.loc[[image], 'Prediction v. GT mSSIM'].tolist()\n",
        "  SSIM_GTvsS_forDisplay = QC_scores.loc[[image], 'Input v. GT mSSIM'].tolist()\n",
        "  NRMSE_GTvsP_forDisplay = QC_scores.loc[[image], 'Prediction v. GT NRMSE'].tolist()\n",
        "  NRMSE_GTvsS_forDisplay = QC_scores.loc[[image], 'Input v. GT NRMSE'].tolist()\n",
        "  PSNR_GTvsP_forDisplay = QC_scores.loc[[image], 'Prediction v. GT PSNR'].tolist()\n",
        "  PSNR_GTvsS_forDisplay = QC_scores.loc[[image], 'Input v. GT PSNR'].tolist()\n",
        "\n",
        "  plt.figure(figsize=(15,15))\n",
        "\n",
        "#-------------------Target (Ground-truth)-------------\n",
        "  plt.subplot(3,3,1)\n",
        "  plt.axis('off')\n",
        "\n",
        "  if dimension == \"2D\":\n",
        "    plt.imshow(img_GT, norm=simple_norm(img_GT, percent = 99))\n",
        "  \n",
        "  if dimension == \"3D\":\n",
        "    plt.imshow(img_GT[Z_plane], norm=simple_norm(img_GT, percent = 99))\n",
        "  plt.title('Target',fontsize=15)\n",
        "\n",
        "#-----------------------Source---------------------\n",
        "  plt.subplot(3,3,2)\n",
        "  plt.axis('off')\n",
        "\n",
        "  if dimension == \"2D\":  \n",
        "    plt.imshow(img_Source, norm=simple_norm(img_Source, percent = 99))\n",
        "\n",
        "  if dimension == \"3D\":\n",
        "    plt.imshow(img_Source[Z_plane], norm=simple_norm(img_Source, percent = 99))\n",
        "  plt.title('Source',fontsize=15)\n",
        "\n",
        "#---------------------Prediction------------------------------\n",
        "  plt.subplot(3,3,3)\n",
        "  plt.axis('off')\n",
        "  \n",
        "  if dimension == \"2D\":\n",
        "    plt.imshow(img_Prediction, norm=simple_norm(img_Prediction, percent = 99))\n",
        "\n",
        "  if dimension == \"3D\":\n",
        "    plt.imshow(img_Prediction[Z_plane], norm=simple_norm(img_Prediction, percent = 99))\n",
        "  plt.title('Prediction',fontsize=15)\n",
        "\n",
        "  #Setting up colours\n",
        "  cmap = plt.cm.CMRmap\n",
        "\n",
        "#---------------------SSIM between GT and Source---------------------\n",
        "  plt.subplot(3,3,5)\n",
        "  #plt.axis('off')\n",
        "  plt.tick_params(\n",
        "      axis='both',      # changes apply to the x-axis and y-axis\n",
        "      which='both',      # both major and minor ticks are affected\n",
        "      bottom=False,      # ticks along the bottom edge are off\n",
        "      top=False,        # ticks along the top edge are off\n",
        "      left=False,       # ticks along the left edge are off\n",
        "      right=False,         # ticks along the right edge are off\n",
        "      labelbottom=False,\n",
        "      labelleft=False)\n",
        "     \n",
        "  if dimension == \"2D\":\n",
        "    imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource, cmap = cmap, vmin=0, vmax=1)\n",
        "  if dimension == \"3D\":\n",
        "    imSSIM_GTvsSource = plt.imshow(img_SSIM_GTvsSource[Z_plane], cmap = cmap, vmin=0, vmax=1)\n",
        "  \n",
        "  plt.colorbar(imSSIM_GTvsSource,fraction=0.046, pad=0.04)\n",
        "  plt.title('Target vs. Source',fontsize=15)\n",
        "  plt.xlabel('mSSIM: '+str(round(SSIM_GTvsS_forDisplay[0],3)),fontsize=14)\n",
        "  plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n",
        "\n",
        "#---------------------SSIM between GT and Prediction---------------------\n",
        "  plt.subplot(3,3,6)\n",
        "    #plt.axis('off')\n",
        "  plt.tick_params(\n",
        "      axis='both',      # changes apply to the x-axis and y-axis\n",
        "      which='both',      # both major and minor ticks are affected\n",
        "      bottom=False,      # ticks along the bottom edge are off\n",
        "      top=False,        # ticks along the top edge are off\n",
        "      left=False,       # ticks along the left edge are off\n",
        "      right=False,         # ticks along the right edge are off\n",
        "      labelbottom=False,\n",
        "      labelleft=False)\n",
        "  if dimension == \"2D\":    \n",
        "    imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n",
        "  \n",
        "  if dimension == \"3D\":  \n",
        "    imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction[Z_plane], cmap = cmap, vmin=0,vmax=1)\n",
        "  \n",
        "  plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n",
        "  plt.title('Target vs. Prediction',fontsize=15)\n",
        "  plt.xlabel('mSSIM: '+str(round(SSIM_GTvsP_forDisplay[0],3)),fontsize=14)\n",
        "\n",
        "#---------------------Root Squared Error between GT and Source---------------------\n",
        "  plt.subplot(3,3,8)\n",
        "    #plt.axis('off')\n",
        "  plt.tick_params(\n",
        "      axis='both',      # changes apply to the x-axis and y-axis\n",
        "      which='both',      # both major and minor ticks are affected\n",
        "      bottom=False,      # ticks along the bottom edge are off\n",
        "      top=False,        # ticks along the top edge are off\n",
        "      left=False,       # ticks along the left edge are off\n",
        "      right=False,         # ticks along the right edge are off\n",
        "      labelbottom=False,\n",
        "      labelleft=False) \n",
        "  \n",
        "  if dimension == \"2D\":  \n",
        "    imRSE_GTvsSource = plt.imshow(img_RSE_GTvsSource, cmap = cmap, vmin=0, vmax = 1)\n",
        "\n",
        "  if dimension == \"3D\": \n",
        "    imRSE_GTvsSource = plt.imshow(img_RSE_GTvsSource[Z_plane], cmap = cmap, vmin=0, vmax = 1)\n",
        "  \n",
        "  plt.colorbar(imRSE_GTvsSource,fraction=0.046,pad=0.04)\n",
        "  plt.title('Target vs. Source',fontsize=15)\n",
        "  plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsS_forDisplay[0],3))+', PSNR: '+str(round(PSNR_GTvsS_forDisplay[0],3)),fontsize=14)  \n",
        "  plt.ylabel('RSE maps',fontsize=20, rotation=0, labelpad=75)\n",
        "\n",
        "#---------------------Root Squared Error between GT and Prediction---------------------\n",
        "  plt.subplot(3,3,9)\n",
        "    #plt.axis('off')\n",
        "  plt.tick_params(\n",
        "      axis='both',      # changes apply to the x-axis and y-axis\n",
        "      which='both',      # both major and minor ticks are affected\n",
        "      bottom=False,      # ticks along the bottom edge are off\n",
        "      top=False,        # ticks along the top edge are off\n",
        "      left=False,       # ticks along the left edge are off\n",
        "      right=False,         # ticks along the right edge are off\n",
        "      labelbottom=False,\n",
        "      labelleft=False)\n",
        "  \n",
        "  if dimension == \"2D\":\n",
        "    imRSE_GTvsPrediction = plt.imshow(img_RSE_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n",
        "  \n",
        "  if dimension == \"3D\": \n",
        "    imRSE_GTvsPrediction = plt.imshow(img_RSE_GTvsPrediction[Z_plane], cmap = cmap, vmin=0, vmax=1)\n",
        "  \n",
        "  plt.colorbar(imRSE_GTvsPrediction,fraction=0.046,pad=0.04)\n",
        "  plt.title('Target vs. Prediction',fontsize=15)\n",
        "  plt.xlabel('NRMSE: '+str(round(NRMSE_GTvsP_forDisplay[0],3))+', PSNR: '+str(round(PSNR_GTvsP_forDisplay[0],3)),fontsize=14)\n",
        "  plt.savefig(QC_folder+\"/QC_example_data.png\",bbox_inches='tight',pad_inches=0)\n",
        "\n",
        "\n",
        "def visualise_segmentation_QC(image, dimension, Source_folder, Prediction_folder, Ground_truth_folder, QC_folder, QC_scores):\n",
        "\n",
        "  plt.figure(figsize=(25,5))\n",
        "  \n",
        "  source_image = io.imread(os.path.join(Source_folder, image))  \n",
        "\n",
        "  target_image = io.imread(os.path.join(Ground_truth_folder, image))\n",
        "  prediction = io.imread(os.path.join(Prediction_folder, image))\n",
        "\n",
        "  IoU_forDisplay = QC_scores.loc[[image], 'Prediction v. GT Intersection over Union'].tolist()\n",
        "\n",
        "  if dimension == \"3D\":  \n",
        "    Z_plane = int(target_image.shape[0] / 2)+1    \n",
        "      \n",
        "  target_image_mask = target_image\n",
        "  target_image_mask[target_image_mask > 0] = 255\n",
        "  target_image_mask[target_image_mask == 0] = 0\n",
        "  \n",
        "  prediction_mask = prediction\n",
        "  prediction_mask[prediction_mask > 0] = 255\n",
        "  prediction_mask[prediction_mask == 0] = 0\n",
        "\n",
        "  intersection = np.logical_and(target_image_mask, prediction_mask)\n",
        "  union = np.logical_or(target_image_mask, prediction_mask)\n",
        "  iou_score =  np.sum(intersection) / np.sum(union)\n",
        "\n",
        "  norm = simple_norm(source_image, percent = 99)\n",
        "\n",
        "  # Input\n",
        "  plt.subplot(1,4,1)\n",
        "  plt.axis('off')\n",
        "  if dimension == \"2D\":\n",
        "    n_channel = 1 if source_image.ndim == 2 else source_image.shape[-1]\n",
        "\n",
        "    if n_channel > 1:\n",
        "      plt.imshow(source_image)\n",
        "    if n_channel == 1:\n",
        "      plt.imshow(source_image, aspect='equal', norm=norm, cmap='magma', interpolation='nearest')\n",
        "\n",
        "  if dimension == \"3D\":\n",
        "    plt.imshow(source_image[Z_plane], aspect='equal', norm=norm, cmap='magma', interpolation='nearest')\n",
        "\n",
        "  plt.title('Input')\n",
        "\n",
        "    #Ground-truth\n",
        "  plt.subplot(1,4,2)\n",
        "  plt.axis('off')\n",
        "  if dimension == \"2D\":\n",
        "    plt.imshow(target_image_mask, aspect='equal', cmap='Greens')\n",
        "  \n",
        "  if dimension == \"3D\":\n",
        "    plt.imshow(target_image_mask[Z_plane], aspect='equal', cmap='Greens')\n",
        "\n",
        "  plt.title('Ground Truth')\n",
        "\n",
        "    #Prediction\n",
        "  plt.subplot(1,4,3)\n",
        "  plt.axis('off')\n",
        "  if dimension == \"2D\":\n",
        "    plt.imshow(prediction_mask, aspect='equal', cmap='Purples')\n",
        "  if dimension == \"3D\":\n",
        "    plt.imshow(prediction_mask[Z_plane], aspect='equal', cmap='Purples')\n",
        "\n",
        "  plt.title('Prediction')\n",
        "\n",
        "    #Overlay\n",
        "  plt.subplot(1,4,4)\n",
        "  plt.axis('off')\n",
        "  if dimension == \"2D\":\n",
        "    plt.imshow(target_image_mask, cmap='Greens')\n",
        "    plt.imshow(prediction_mask, alpha=0.5, cmap='Purples')\n",
        "  \n",
        "  if dimension == \"3D\":\n",
        "    plt.imshow(target_image_mask[Z_plane], cmap='Greens')\n",
        "    plt.imshow(prediction_mask[Z_plane], alpha=0.5, cmap='Purples')  \n",
        "\n",
        "  plt.title('Ground Truth and Prediction, Intersection over Union:'+str(round(IoU_forDisplay[0],3 )));\n",
        "  plt.savefig(QC_folder+\"/QC_example_data.png\",bbox_inches='tight',pad_inches=0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw0kkTU6CsU4"
      },
      "source": [
        "# **2. Error mapping and quality metrics estimation**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biT9FI9Ri77_"
      },
      "source": [
        "## **Image similarity metrics**\n",
        "---\n",
        "\n",
        "<font size = 4>**The SSIM (structural similarity) map** \n",
        "\n",
        "<font size = 4>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info). \n",
        "\n",
        "<font size=4>**mSSIM** is the SSIM value calculated across the entire window of both images.\n",
        "\n",
        "<font size=4>**The output below shows the SSIM maps with the mSSIM**\n",
        "\n",
        "<font size = 4>**The RSE (Root Squared Error) map** \n",
        "\n",
        "<font size = 4>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n",
        "\n",
        "\n",
        "<font size =4>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n",
        "\n",
        "<font size = 4>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n",
        "\n",
        "\n",
        "---\n",
        "## **Image segmentation metrics**\n",
        "---\n",
        "\n",
        "\n",
        "<font size = 4>The **Intersection over Union** (IuO) metric is a method that can be used to quantify the overlap between the target mask and your prediction output. **Therefore, the closer to 1, the better the performance.** This metric can be used to assess the quality of your model to accurately predict nuclei. \n",
        "\n",
        "<font size = 4>Here, the IuO is both calculated over the whole image and on a per-object basis. The value displayed below is the IuO value calculated over the entire image. The IuO value calculated on a per-object basis is used to calculate the other metrics displayed.\n",
        "\n",
        "<font size = 4>“n_true” refers to the number of objects present in the ground truth image. “n_pred” refers to the number of objects present in the predicted image. \n",
        "\n",
        "<font size = 4>When a segmented object has an IuO value above 0.5 (compared to the corresponding ground truth), it is then considered a true positive. The number of “**true positives**” is available in the table below. The number of “false positive” is then defined as  “**false positive**” = “n_pred” - “true positive”. The number of “false negative” is defined as “false negative” = “n_true” - “true positive”.\n",
        "\n",
        "<font size = 4>The mean_matched_score is the mean IoUs of matched true positives. The mean_true_score is the mean IoUs of matched true positives but normalized by the total number of ground truth objects. The panoptic_quality is calculated as described by [Kirillov et al. 2019](https://arxiv.org/abs/1801.00868).\n",
        "\n",
        "<font size = 4>For more information about the other metric displayed, please consult the SI of the paper describing ZeroCostDL4Mic.\n",
        "\n",
        "<font size = 4> The results can be found in the \"*Quality Control*\" folder which is located inside your \"model_folder\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLU4V3Er5leQ",
        "cellView": "form"
      },
      "source": [
        "from tabulate import tabulate\n",
        "from astropy.visualization import simple_norm\n",
        "\n",
        "from ipywidgets import interact\n",
        "\n",
        "#@markdown ##Choose the folders that contain the data to analyse\n",
        "\n",
        "Source_folder = \"\" #@param{type:\"string\"}\n",
        "Prediction_folder = \"\" #@param{type:\"string\"}\n",
        "Ground_truth_folder = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown ##Choose the type of QC you want to perform\n",
        "\n",
        "QC_type = \"Image-to-image comparison\" #@param [\"Image-to-image comparison\", \"Segmentation\", \"Instance segmentation\"]\n",
        "\n",
        "#@markdown ###Are your data 2D or 3D images?\n",
        "\n",
        "Data_type = \"2D\" #@param [\"2D\", \"3D\"]\n",
        "\n",
        "# Create a quality control in the Prediction Folder\n",
        "\n",
        "QC_folder = Prediction_folder+\"/Quality Control\"\n",
        "\n",
        "if os.path.exists(QC_folder):\n",
        "  shutil.rmtree(QC_folder)\n",
        "os.makedirs(QC_folder)\n",
        "\n",
        "# List images in Source_folder\n",
        "Z = os.listdir(Source_folder)\n",
        "print('Number of test dataset found in the folder: '+str(len(Z)))\n",
        "\n",
        "random_choice = random.choice(os.listdir(Source_folder))\n",
        "X = io.imread(Source_folder+\"/\"+random_choice)\n",
        "n_channel = 1 if X.ndim == 2 else X.shape[-1]\n",
        "\n",
        "# ------------------ Image-to-image comparison 2D -------------------------------------------------\n",
        "\n",
        "if QC_type == \"Image-to-image comparison\" and Data_type == \"2D\" :\n",
        "\n",
        "# Open and create the csv file that will contain all the QC metrics\n",
        "  with open(QC_folder+\"/QC_metrics.csv\", \"w\", newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "\n",
        "    # Write the header in the csv file\n",
        "      writer.writerow([\"image\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\", \"Prediction v. GT NRMSE\", \"Input v. GT NRMSE\", \"Prediction v. GT PSNR\", \"Input v. GT PSNR\"])  \n",
        "\n",
        "    # Let's loop through the provided dataset in the QC folders\n",
        "\n",
        "      for i in os.listdir(Source_folder):\n",
        "        if not os.path.isdir(os.path.join(Source_folder,i)):\n",
        "          print('Running QC on: '+i)\n",
        "      # -------------------------------- Target test data (Ground truth) --------------------------------\n",
        "          test_GT = io.imread(os.path.join(Ground_truth_folder, i))\n",
        "\n",
        "      # -------------------------------- Source test data --------------------------------\n",
        "          test_source = io.imread(os.path.join(Source_folder,i))\n",
        "\n",
        "      # Normalize the images wrt each other by minimizing the MSE between GT and Source image\n",
        "          test_GT_norm,test_source_norm = norm_minmse(test_GT, test_source, normalize_gt=True)\n",
        "\n",
        "      # -------------------------------- Prediction --------------------------------\n",
        "          test_prediction = io.imread(os.path.join(Prediction_folder,i))\n",
        "\n",
        "      # Normalize the images wrt each other by minimizing the MSE between GT and prediction\n",
        "          test_GT_norm,test_prediction_norm = norm_minmse(test_GT, test_prediction, normalize_gt=True)        \n",
        "\n",
        "      # -------------------------------- Calculate the metric maps and save them --------------------------------\n",
        "\n",
        "      # Calculate the SSIM maps\n",
        "          index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = ssim(test_GT_norm, test_prediction_norm)\n",
        "          index_SSIM_GTvsSource, img_SSIM_GTvsSource = ssim(test_GT_norm, test_source_norm)\n",
        "\n",
        "      #Save ssim_maps\n",
        "          img_SSIM_GTvsPrediction_32bit = np.float32(img_SSIM_GTvsPrediction)\n",
        "          io.imsave(QC_folder+'/SSIM_GTvsPrediction_'+i,img_SSIM_GTvsPrediction_32bit)\n",
        "          img_SSIM_GTvsSource_32bit = np.float32(img_SSIM_GTvsSource)\n",
        "          io.imsave(QC_folder+'/SSIM_GTvsSource_'+i,img_SSIM_GTvsSource_32bit)\n",
        "      \n",
        "      # Calculate the Root Squared Error (RSE) maps\n",
        "          img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n",
        "          img_RSE_GTvsSource = np.sqrt(np.square(test_GT_norm - test_source_norm))\n",
        "\n",
        "      # Save SE maps\n",
        "          img_RSE_GTvsPrediction_32bit = np.float32(img_RSE_GTvsPrediction)\n",
        "          img_RSE_GTvsSource_32bit = np.float32(img_RSE_GTvsSource)\n",
        "          io.imsave(QC_folder+'/RSE_GTvsPrediction_'+i,img_RSE_GTvsPrediction_32bit)\n",
        "          io.imsave(QC_folder+'/RSE_GTvsSource_'+i,img_RSE_GTvsSource_32bit)\n",
        "\n",
        "\n",
        "      # -------------------------------- Calculate the RSE metrics and save them --------------------------------\n",
        "\n",
        "      # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n",
        "          NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n",
        "          NRMSE_GTvsSource = np.sqrt(np.mean(img_RSE_GTvsSource))\n",
        "        \n",
        "      # We can also measure the peak signal to noise ratio between the images\n",
        "          PSNR_GTvsPrediction = psnr(test_GT_norm,test_prediction_norm,data_range=1.0)\n",
        "          PSNR_GTvsSource = psnr(test_GT_norm,test_source_norm,data_range=1.0)\n",
        "\n",
        "          writer.writerow([i,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource),str(NRMSE_GTvsPrediction),str(NRMSE_GTvsSource),str(PSNR_GTvsPrediction),str(PSNR_GTvsSource)])\n",
        "\n",
        "  # ------------- For display ------------\n",
        "\n",
        "  df = pd.read_csv (QC_folder+\"/QC_metrics.csv\")\n",
        "  df.set_index(\"image\", inplace=True)\n",
        "  print(tabulate(df, headers='keys', tablefmt='psql'))\n",
        "\n",
        "\n",
        "  print('--------------------------------------------------------------')\n",
        "  @interact\n",
        "  def show_QC_results(file = os.listdir(Source_folder)):\n",
        "\n",
        "    visualise_image_comparison_QC(image = file, dimension=Data_type, Source_folder=Source_folder , Prediction_folder= Prediction_folder, Ground_truth_folder=Ground_truth_folder, QC_folder=QC_folder, QC_scores= df )  \n",
        "\n",
        "  print('-----------------------------------')\n",
        "\n",
        "# ------------------ Image-to-image comparison 3D -------------------------------------------------\n",
        "\n",
        "if QC_type == \"Image-to-image comparison\" and Data_type == \"3D\" :\n",
        "\n",
        "# Open and create the csv file that will contain all the QC metrics\n",
        "  with open(QC_folder+\"/QC_metrics.csv\", \"w\", newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "\n",
        "    # Write the header in the csv file\n",
        "      writer.writerow([\"File name\",\"Slice #\",\"Prediction v. GT mSSIM\",\"Input v. GT mSSIM\", \"Prediction v. GT NRMSE\", \"Input v. GT NRMSE\", \"Prediction v. GT PSNR\", \"Input v. GT PSNR\"])  \n",
        "    \n",
        "    # These lists will be used to collect all the metrics values per slice\n",
        "      file_name_list = []\n",
        "      slice_number_list = []\n",
        "      mSSIM_GvP_list = []\n",
        "      mSSIM_GvS_list = []\n",
        "      NRMSE_GvP_list = []\n",
        "      NRMSE_GvS_list = []\n",
        "      PSNR_GvP_list = []\n",
        "      PSNR_GvS_list = []\n",
        "\n",
        "    # These lists will be used to display the mean metrics for the stacks\n",
        "      mSSIM_GvP_list_mean = []\n",
        "      mSSIM_GvS_list_mean = []\n",
        "      NRMSE_GvP_list_mean = []\n",
        "      NRMSE_GvS_list_mean = []\n",
        "      PSNR_GvP_list_mean = []\n",
        "      PSNR_GvS_list_mean = []\n",
        "\n",
        "    # Let's loop through the provided dataset in the QC folders\n",
        "      for thisFile in os.listdir(Source_folder):\n",
        "        if not os.path.isdir(os.path.join(Source_folder, thisFile)):\n",
        "          print('Running QC on: '+thisFile)\n",
        "\n",
        "          test_GT_stack = io.imread(os.path.join(Ground_truth_folder, thisFile))\n",
        "          test_source_stack = io.imread(os.path.join(Source_folder,thisFile))\n",
        "          test_prediction_stack = io.imread(os.path.join(Prediction_folder, thisFile))\n",
        "          n_slices = test_GT_stack.shape[0]\n",
        "\n",
        "        # Calculating the position of the mid-plane slice\n",
        "          z_mid_plane = int(n_slices / 2)+1\n",
        "\n",
        "          img_SSIM_GTvsPrediction_stack = np.zeros((n_slices, test_GT_stack.shape[1], test_GT_stack.shape[2]))\n",
        "          img_SSIM_GTvsSource_stack = np.zeros((n_slices, test_GT_stack.shape[1], test_GT_stack.shape[2]))\n",
        "          img_RSE_GTvsPrediction_stack = np.zeros((n_slices, test_GT_stack.shape[1], test_GT_stack.shape[2]))\n",
        "          img_RSE_GTvsSource_stack = np.zeros((n_slices, test_GT_stack.shape[1], test_GT_stack.shape[2]))\n",
        "\n",
        "          for z in range(n_slices): \n",
        "          # -------------------------------- Normalising the dataset --------------------------------\n",
        "\n",
        "            test_GT_norm, test_source_norm = norm_minmse(test_GT_stack[z], test_source_stack[z], normalize_gt=True)\n",
        "            test_GT_norm, test_prediction_norm = norm_minmse(test_GT_stack[z], test_prediction_stack[z], normalize_gt=True)\n",
        "\n",
        "          # -------------------------------- Calculate the SSIM metric and maps --------------------------------\n",
        "\n",
        "          # Calculate the SSIM maps and index\n",
        "            index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = structural_similarity(test_GT_norm, test_prediction_norm, data_range=1.0, full=True, gaussian_weights=True, use_sample_covariance=False, sigma=1.5)\n",
        "            index_SSIM_GTvsSource, img_SSIM_GTvsSource = structural_similarity(test_GT_norm, test_source_norm, data_range=1.0, full=True, gaussian_weights=True, use_sample_covariance=False, sigma=1.5)\n",
        "\n",
        "          #Calculate ssim_maps\n",
        "            img_SSIM_GTvsPrediction_stack[z] = img_as_float32(img_SSIM_GTvsPrediction, force_copy=False)\n",
        "            img_SSIM_GTvsSource_stack[z] = img_as_float32(img_SSIM_GTvsSource, force_copy=False) \n",
        "\n",
        "          # -------------------------------- Calculate the NRMSE metrics --------------------------------\n",
        "\n",
        "          # Calculate the Root Squared Error (RSE) maps\n",
        "            img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n",
        "            img_RSE_GTvsSource = np.sqrt(np.square(test_GT_norm - test_source_norm))\n",
        "\n",
        "          # Calculate SE maps\n",
        "            img_RSE_GTvsPrediction_stack[z] = img_as_float32(img_RSE_GTvsPrediction, force_copy=False)\n",
        "            img_RSE_GTvsSource_stack[z] = img_as_float32(img_RSE_GTvsSource, force_copy=False)\n",
        "\n",
        "          # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n",
        "            NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n",
        "            NRMSE_GTvsSource = np.sqrt(np.mean(img_RSE_GTvsSource))\n",
        "\n",
        "          # Calculate the PSNR between the images\n",
        "            PSNR_GTvsPrediction = psnr(test_GT_norm,test_prediction_norm,data_range=1.0)\n",
        "            PSNR_GTvsSource = psnr(test_GT_norm,test_source_norm,data_range=1.0)\n",
        "\n",
        "            writer.writerow([thisFile, str(z),str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsSource),str(NRMSE_GTvsPrediction),str(NRMSE_GTvsSource), str(PSNR_GTvsPrediction), str(PSNR_GTvsSource)])\n",
        "          \n",
        "          # Collect values to display in dataframe output\n",
        "            slice_number_list.append(z)\n",
        "            mSSIM_GvP_list.append(index_SSIM_GTvsPrediction)\n",
        "            mSSIM_GvS_list.append(index_SSIM_GTvsSource)\n",
        "            NRMSE_GvP_list.append(NRMSE_GTvsPrediction)\n",
        "            NRMSE_GvS_list.append(NRMSE_GTvsSource)\n",
        "            PSNR_GvP_list.append(PSNR_GTvsPrediction)\n",
        "            PSNR_GvS_list.append(PSNR_GTvsSource)\n",
        "        \n",
        "        # If calculating average metrics for dataframe output\n",
        "          file_name_list.append(thisFile)\n",
        "          mSSIM_GvP_list_mean.append(sum(mSSIM_GvP_list)/len(mSSIM_GvP_list))\n",
        "          mSSIM_GvS_list_mean.append(sum(mSSIM_GvS_list)/len(mSSIM_GvS_list))\n",
        "          NRMSE_GvP_list_mean.append(sum(NRMSE_GvP_list)/len(NRMSE_GvP_list))\n",
        "          NRMSE_GvS_list_mean.append(sum(NRMSE_GvS_list)/len(NRMSE_GvS_list))\n",
        "          PSNR_GvP_list_mean.append(sum(PSNR_GvP_list)/len(PSNR_GvP_list))\n",
        "          PSNR_GvS_list_mean.append(sum(PSNR_GvS_list)/len(PSNR_GvS_list))\n",
        "\n",
        "         # ----------- Change the stacks to 32 bit images -----------\n",
        "\n",
        "          img_SSIM_GTvsSource_stack_32 = img_as_float32(img_SSIM_GTvsSource_stack, force_copy=False)\n",
        "          img_SSIM_GTvsPrediction_stack_32 = img_as_float32(img_SSIM_GTvsPrediction_stack, force_copy=False)\n",
        "          img_RSE_GTvsSource_stack_32 = img_as_float32(img_RSE_GTvsSource_stack, force_copy=False)\n",
        "          img_RSE_GTvsPrediction_stack_32 = img_as_float32(img_RSE_GTvsPrediction_stack, force_copy=False)\n",
        "\n",
        "        # ----------- Saving the error map stacks -----------\n",
        "          io.imsave(QC_folder+\"/SSIM_GTvsSource_\"+thisFile,img_SSIM_GTvsSource_stack_32)\n",
        "          io.imsave(QC_folder+\"/SSIM_GTvsPrediction_\"+thisFile,img_SSIM_GTvsPrediction_stack_32)\n",
        "          io.imsave(QC_folder+\"/RSE_GTvsSource_\"+thisFile,img_RSE_GTvsSource_stack_32)\n",
        "          io.imsave(QC_folder+\"/RSE_GTvsPrediction_\"+thisFile,img_RSE_GTvsPrediction_stack_32)\n",
        "\n",
        "#Averages of the metrics per stack as dataframe output\n",
        "  pdResults = pd.DataFrame(file_name_list, columns = [\"image\"])\n",
        "  pdResults[\"Prediction v. GT mSSIM\"] = mSSIM_GvP_list_mean\n",
        "  pdResults[\"Input v. GT mSSIM\"] = mSSIM_GvS_list_mean\n",
        "  pdResults[\"Prediction v. GT NRMSE\"] = NRMSE_GvP_list_mean\n",
        "  pdResults[\"Input v. GT NRMSE\"] = NRMSE_GvS_list_mean\n",
        "  pdResults[\"Prediction v. GT PSNR\"] = PSNR_GvP_list_mean\n",
        "  pdResults[\"Input v. GT PSNR\"] = PSNR_GvS_list_mean\n",
        "\n",
        "  print('Here are the average scores for the stacks you tested in Quality control. To see values for all slices, open the .csv file saved in the Quality Control folder.')\n",
        "  pdResults.set_index(\"image\", inplace=True)\n",
        "  pdResults.head()\n",
        "  print(tabulate(pdResults, headers='keys', tablefmt='psql'))\n",
        "\n",
        "  print('--------------------------------------------------------------')\n",
        "  @interact\n",
        "  def show_QC_results(file = os.listdir(Source_folder)):\n",
        "    \n",
        "    visualise_image_comparison_QC(image = file, dimension=Data_type, Source_folder=Source_folder , Prediction_folder= Prediction_folder, Ground_truth_folder=Ground_truth_folder, QC_folder=QC_folder, QC_scores= pdResults )  \n",
        "    \n",
        "\n",
        "  print('-----------------------------------')\n",
        "\n",
        "# ------------------ Segmentation 2D -------------------------------------------------\n",
        "\n",
        "if QC_type == \"Segmentation\" and Data_type == \"2D\":\n",
        "\n",
        "  with open(QC_folder+\"/QC_metrics.csv\", \"w\", newline='') as file:\n",
        "    writer = csv.writer(file, delimiter=\",\")\n",
        "    writer.writerow([\"image\",\"Prediction v. GT Intersection over Union\"])  \n",
        "\n",
        "    for n in os.listdir(Source_folder):\n",
        "    \n",
        "      if not os.path.isdir(os.path.join(Source_folder,n)):\n",
        "        print('Running QC on: '+n)\n",
        "        test_input = io.imread(os.path.join(Source_folder,n))\n",
        "        test_prediction = io.imread(os.path.join(Prediction_folder,n))\n",
        "        test_ground_truth_image = io.imread(os.path.join(Ground_truth_folder, n))\n",
        "\n",
        "       #Convert pixel values to 0 or 255\n",
        "        test_prediction_0_to_255 = test_prediction\n",
        "        test_prediction_0_to_255[test_prediction_0_to_255>0] = 255\n",
        "\n",
        "      #Convert pixel values to 0 or 255\n",
        "        test_ground_truth_0_to_255 = test_ground_truth_image\n",
        "        test_ground_truth_0_to_255[test_ground_truth_0_to_255>0] = 255\n",
        "\n",
        "      # Intersection over Union metric\n",
        "        intersection = np.logical_and(test_ground_truth_0_to_255, test_prediction_0_to_255)\n",
        "        union = np.logical_or(test_ground_truth_0_to_255, test_prediction_0_to_255)\n",
        "        iou_score =  np.sum(intersection) / np.sum(union)\n",
        "        writer.writerow([n, str(iou_score)])\n",
        "\n",
        "  df = pd.read_csv (QC_folder+\"/QC_metrics.csv\")\n",
        "  df.set_index(\"image\", inplace=True)\n",
        "  print(tabulate(df, headers='keys', tablefmt='psql'))\n",
        "\n",
        "\n",
        "  # ------------- For display ------------\n",
        "  print('--------------------------------------------------------------')\n",
        "  @interact\n",
        "  def show_QC_results(images = os.listdir(Source_folder)):  \n",
        "\n",
        "    visualise_segmentation_QC(image=images, dimension=Data_type, Source_folder=Source_folder, Prediction_folder=Prediction_folder, Ground_truth_folder=Ground_truth_folder, QC_folder=QC_folder, QC_scores=df)\n",
        "      \n",
        "  print('-----------------------------------')\n",
        "\n",
        "\n",
        "# ------------------ Segmentation 3D -------------------------------------------------\n",
        "\n",
        "if QC_type == \"Segmentation\" and Data_type == \"3D\":\n",
        "\n",
        "  with open(QC_folder+\"/QC_metrics.csv\", \"w\", newline='') as file:\n",
        "    writer = csv.writer(file, delimiter=\",\")\n",
        "    writer.writerow([\"image\",\"Slice #\",\"Prediction v. GT Intersection over Union\"])\n",
        "\n",
        "    file_name_list = []\n",
        "    slice_number_list = []\n",
        "    iou_score_list = []  \n",
        "\n",
        "    # These lists will be used to display the mean metrics for the stacks\n",
        "    iou_score_list_mean = []\n",
        "\n",
        "    for n in os.listdir(Source_folder):\n",
        "    \n",
        "      if not os.path.isdir(os.path.join(Source_folder,n)):\n",
        "        print('Running QC on: '+n)\n",
        "        test_input = io.imread(os.path.join(Source_folder,n))\n",
        "        test_prediction = io.imread(os.path.join(Prediction_folder,n))\n",
        "        test_ground_truth_image = io.imread(os.path.join(Ground_truth_folder, n))\n",
        "\n",
        "        for z in range(n_slices):\n",
        "\n",
        "       #Convert pixel values to 0 or 255\n",
        "          test_prediction_0_to_255 = test_prediction[z]\n",
        "          test_prediction_0_to_255[test_prediction_0_to_255>0] = 255\n",
        "\n",
        "      #Convert pixel values to 0 or 255\n",
        "          test_ground_truth_0_to_255 = test_ground_truth_image[z]\n",
        "          test_ground_truth_0_to_255[test_ground_truth_0_to_255>0] = 255\n",
        "\n",
        "      # Intersection over Union metric\n",
        "          intersection = np.logical_and(test_ground_truth_0_to_255, test_prediction_0_to_255)\n",
        "          union = np.logical_or(test_ground_truth_0_to_255, test_prediction_0_to_255)\n",
        "          iou_score =  np.sum(intersection) / np.sum(union)\n",
        "\n",
        "          slice_number_list.append(z)\n",
        "          iou_score_list.append(iou_score)\n",
        "\n",
        "          writer.writerow([n, str(z), str(iou_score)])\n",
        "\n",
        "        iou_score_array = np.array(iou_score_list)\n",
        "        iou_score_array[iou_score_array==0.0] = np.nan          \n",
        "\n",
        "        # If calculating average metrics for dataframe output\n",
        "        file_name_list.append(n)\n",
        "        iou_score_list_mean.append(np.nanmean(iou_score_array))\n",
        "\n",
        "  df = pd.read_csv (QC_folder+\"/QC_metrics.csv\")\n",
        "\n",
        "#Averages of the metrics per stack as dataframe output\n",
        "  pdResults = pd.DataFrame(file_name_list, columns = [\"image\"])\n",
        "  pdResults[\"Prediction v. GT Intersection over Union\"] = iou_score_list_mean\n",
        "\n",
        "  print('Here are the average scores for the stacks you tested in Quality control. To see values for all slices, open the .csv file saved in the Quality Control folder.')\n",
        "  pdResults.set_index(\"image\", inplace=True)\n",
        "  pdResults.head()\n",
        "  print(tabulate(pdResults, headers='keys', tablefmt='psql'))\n",
        "\n",
        "\n",
        "  # ------------- For display ------------\n",
        "  print('--------------------------------------------------------------')\n",
        "  @interact\n",
        "  def show_QC_results(images = os.listdir(Source_folder)):  \n",
        "\n",
        "    visualise_segmentation_QC(image=images, dimension=Data_type, Source_folder=Source_folder, Prediction_folder=Prediction_folder, Ground_truth_folder=Ground_truth_folder, QC_folder=QC_folder, QC_scores=pdResults)\n",
        "      \n",
        "  print('-----------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ Instance Segmentation 2D -------------------------------------------------\n",
        "\n",
        "if QC_type == \"Instance segmentation\" and Data_type == \"2D\":\n",
        "\n",
        "  with open(QC_folder+\"/QC_metrics.csv\", \"w\", newline='') as file:\n",
        "    writer = csv.writer(file, delimiter=\",\")\n",
        "    writer.writerow([\"image\",\"Prediction v. GT Intersection over Union\", \"false positive\", \"true positive\", \"false negative\", \"precision\", \"recall\", \"accuracy\", \"f1 score\", \"n_true\", \"n_pred\", \"mean_true_score\", \"mean_matched_score\", \"panoptic_quality\"])  \n",
        "\n",
        "  # define the images\n",
        "\n",
        "    for n in os.listdir(Source_folder):\n",
        "    \n",
        "      if not os.path.isdir(os.path.join(Source_folder,n)):\n",
        "        print('Running QC on: '+n)\n",
        "        test_input = io.imread(os.path.join(Source_folder,n))\n",
        "        test_prediction = io.imread(os.path.join(Prediction_folder,n))\n",
        "        test_ground_truth_image = io.imread(os.path.join(Ground_truth_folder, n))\n",
        "\n",
        "        # Calculate the matching (with IoU threshold `thresh`) and all metrics\n",
        "\n",
        "        stats = matching(test_ground_truth_image, test_prediction, thresh=0.5)      \n",
        "      \n",
        "\n",
        "       #Convert pixel values to 0 or 255\n",
        "        test_prediction_0_to_255 = test_prediction\n",
        "        test_prediction_0_to_255[test_prediction_0_to_255>0] = 255\n",
        "\n",
        "      #Convert pixel values to 0 or 255\n",
        "        test_ground_truth_0_to_255 = test_ground_truth_image\n",
        "        test_ground_truth_0_to_255[test_ground_truth_0_to_255>0] = 255\n",
        "\n",
        "      # Intersection over Union metric\n",
        "        intersection = np.logical_and(test_ground_truth_0_to_255, test_prediction_0_to_255)\n",
        "        union = np.logical_or(test_ground_truth_0_to_255, test_prediction_0_to_255)\n",
        "        iou_score =  np.sum(intersection) / np.sum(union)\n",
        "        writer.writerow([n, str(iou_score), str(stats.fp), str(stats.tp), str(stats.fn), str(stats.precision), str(stats.recall), str(stats.accuracy), str(stats.f1), str(stats.n_true), str(stats.n_pred), str(stats.mean_true_score), str(stats.mean_matched_score), str(stats.panoptic_quality)])\n",
        "\n",
        "  df = pd.read_csv (QC_folder+\"/QC_metrics.csv\")\n",
        "  df.set_index(\"image\", inplace=True)\n",
        "  print(tabulate(df, headers='keys', tablefmt='psql'))\n",
        "\n",
        "\n",
        "  # ------------- For display ------------\n",
        "  print('--------------------------------------------------------------')\n",
        "  @interact\n",
        "  def show_QC_results(images = os.listdir(Source_folder)):\n",
        "        \n",
        "    visualise_segmentation_QC(image=images,dimension=Data_type, Source_folder=Source_folder, Prediction_folder=Prediction_folder, Ground_truth_folder=Ground_truth_folder, QC_folder=QC_folder, QC_scores=df)  \n",
        "\n",
        "  print('-----------------------------------')\n",
        "\n",
        "\n",
        "# ------------------ Instance Segmentation 3D in progress -------------------------------------------------\n",
        "\n",
        "if QC_type == \"Instance segmentation\" and Data_type == \"3D\":\n",
        "\n",
        "  with open(QC_folder+\"/QC_metrics.csv\", \"w\", newline='') as file:\n",
        "    writer = csv.writer(file, delimiter=\",\")\n",
        "    writer.writerow([\"image\",\"Slice #\",\"Prediction v. GT Intersection over Union\", \"false positive\", \"true positive\", \"false negative\", \"precision\", \"recall\", \"accuracy\", \"f1 score\", \"n_true\", \"n_pred\", \"mean_true_score\", \"mean_matched_score\", \"panoptic_quality\"])  \n",
        "\n",
        "    # These lists will be used to collect all the metrics values per slice\n",
        "    file_name_list = []\n",
        "    slice_number_list = []\n",
        "    iou_score_list = []\n",
        "    fp_list = []\n",
        "    tp_list = []\n",
        "    fn_list = []\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    accuracy_list = []\n",
        "    f1_list = []\n",
        "    n_true_list = []\n",
        "    n_pred_list = []\n",
        "    mean_true_score_list = []\n",
        "    mean_matched_score_list = []\n",
        "    panoptic_quality_list = []\n",
        "\n",
        "    # These lists will be used to display the mean metrics for the stacks\n",
        "    iou_score_list_mean = []\n",
        "    fp_list_mean = []\n",
        "    tp_list_mean = []\n",
        "    fn_list_mean = []\n",
        "    precision_list_mean = []\n",
        "    recall_list_mean = []\n",
        "    accuracy_list_mean = []\n",
        "    f1_list_mean = []\n",
        "    n_true_list_mean = []\n",
        "    n_pred_list_mean = []\n",
        "    mean_true_score_list_mean = []\n",
        "    mean_matched_score_list_mean = []\n",
        "    panoptic_quality_list_mean = []\n",
        "\n",
        "    for n in os.listdir(Source_folder):\n",
        "    \n",
        "      if not os.path.isdir(os.path.join(Source_folder,n)):\n",
        "        print('Running QC on: '+n)\n",
        "        test_input = io.imread(os.path.join(Source_folder,n))\n",
        "        test_prediction = io.imread(os.path.join(Prediction_folder,n))\n",
        "        test_ground_truth_image = io.imread(os.path.join(Ground_truth_folder, n))\n",
        "\n",
        "        n_slices = test_ground_truth_image.shape[0]\n",
        "\n",
        "        for z in range(n_slices):\n",
        "\n",
        "        # Calculate the matching (with IoU threshold `thresh`) and all metrics\n",
        "\n",
        "          stats = matching(test_ground_truth_image[z], test_prediction[z], thresh=0.5) \n",
        "      \n",
        "\n",
        "       #Convert pixel values to 0 or 255\n",
        "          test_prediction_0_to_255 = test_prediction[z]\n",
        "          test_prediction_0_to_255[test_prediction_0_to_255>0] = 255\n",
        "\n",
        "      #Convert pixel values to 0 or 255\n",
        "          test_ground_truth_0_to_255 = test_ground_truth_image[z]\n",
        "          test_ground_truth_0_to_255[test_ground_truth_0_to_255>0] = 255\n",
        "\n",
        "      # Intersection over Union metric\n",
        "          intersection = np.logical_and(test_ground_truth_0_to_255, test_prediction_0_to_255)\n",
        "          union = np.logical_or(test_ground_truth_0_to_255, test_prediction_0_to_255)\n",
        "          iou_score =  np.sum(intersection) / np.sum(union)\n",
        "          \n",
        "          # Collect values to display in dataframe output\n",
        "          slice_number_list.append(z)\n",
        "          iou_score_list.append(iou_score)\n",
        "          fp_list.append(stats.fp)\n",
        "          tp_list.append(stats.tp)\n",
        "          fn_list.append(stats.fn)\n",
        "          precision_list.append(stats.precision)\n",
        "          recall_list.append(stats.recall)\n",
        "          accuracy_list.append(stats.accuracy)\n",
        "          f1_list.append(stats.f1)\n",
        "          n_true_list.append(stats.n_true)\n",
        "          n_pred_list.append(stats.n_pred)\n",
        "          mean_true_score_list.append(stats.mean_true_score)\n",
        "          mean_matched_score_list.append(stats.mean_matched_score)\n",
        "          panoptic_quality_list.append(stats.panoptic_quality)\n",
        "  \n",
        "\n",
        "          writer.writerow([n, str(z), str(iou_score), str(stats.fp), str(stats.tp), str(stats.fn), str(stats.precision), str(stats.recall), str(stats.accuracy), str(stats.f1), str(stats.n_true), str(stats.n_pred), str(stats.mean_true_score), str(stats.mean_matched_score), str(stats.panoptic_quality)])\n",
        "        \n",
        "        #Here we transform the lists into arrays so that 0 can be removed when computing the average over the stack\n",
        "\n",
        "        iou_score_array = np.array(iou_score_list)\n",
        "        iou_score_array[iou_score_array==0.0] = np.nan\n",
        "        precision_array = np.array(precision_list)\n",
        "        precision_array[precision_array==0.0] = np.nan        \n",
        "        recall_array = np.array(recall_list)\n",
        "        recall_array[recall_array==0.0] = np.nan\n",
        "        accuracy_array = np.array(accuracy_list)\n",
        "        accuracy_array[accuracy_array==0.0] = np.nan \n",
        "        f1_array = np.array(f1_list)\n",
        "        f1_array[f1_array==0.0] = np.nan\n",
        "        mean_true_score_array = np.array(mean_true_score_list)\n",
        "        mean_true_score_array[mean_true_score_array==0.0] = np.nan          \n",
        "        mean_matched_score_array = np.array(mean_matched_score_list)\n",
        "        mean_matched_score_array[mean_matched_score_array==0.0] = np.nan\n",
        "        panoptic_quality_array = np.array(panoptic_quality_list)\n",
        "        panoptic_quality_array[panoptic_quality_array==0.0] = np.nan\n",
        "\n",
        "        # If calculating average metrics for dataframe output\n",
        "        file_name_list.append(n)\n",
        "        iou_score_list_mean.append(np.nanmean(iou_score_array))\n",
        "        fp_list_mean.append(sum(fp_list))\n",
        "        tp_list_mean.append(sum(tp_list))\n",
        "        fn_list_mean.append(sum(fn_list))\n",
        "        precision_list_mean.append(np.nanmean(precision_array))\n",
        "        recall_list_mean.append(np.nanmean(recall_array))\n",
        "        accuracy_list_mean.append(np.nanmean(accuracy_array))\n",
        "        f1_list_mean.append(np.nanmean(f1_array))\n",
        "        n_true_list_mean.append(sum(n_true_list))\n",
        "        n_pred_list_mean.append(sum(n_pred_list))\n",
        "        mean_true_score_list_mean.append(np.nanmean(mean_true_score_array))\n",
        "        mean_matched_score_list_mean.append(np.nanmean(mean_matched_score_array))\n",
        "        panoptic_quality_list_mean.append(np.nanmean(panoptic_quality_array))\n",
        "\n",
        "  df = pd.read_csv (QC_folder+\"/QC_metrics.csv\")\n",
        "\n",
        "#Averages of the metrics per stack as dataframe output\n",
        "  pdResults = pd.DataFrame(file_name_list, columns = [\"image\"])\n",
        "  pdResults[\"Prediction v. GT Intersection over Union\"] = iou_score_list_mean\n",
        "  pdResults[\"false positive\"] = fp_list_mean\n",
        "  pdResults[\"true positive\"] = tp_list_mean\n",
        "  pdResults[\"false negative\"] = fn_list_mean\n",
        "  pdResults[\"precision\"] = precision_list_mean\n",
        "  pdResults[\"recall\"] = recall_list_mean\n",
        "  pdResults[\"accuracy\"] = accuracy_list_mean\n",
        "  pdResults[\"f1 score\"] = f1_list_mean\n",
        "  pdResults[\"n_true\"] = n_true_list_mean\n",
        "  pdResults[\"n_pred\"] = n_pred_list_mean\n",
        "  pdResults[\"mean_true_score\"] = mean_true_score_list_mean\n",
        "  pdResults[\"mean_matched_score\"] = mean_matched_score_list_mean\n",
        "  pdResults[\"panoptic_quality\"] = panoptic_quality_list_mean\n",
        "\n",
        "  print('Here are the average scores for the stacks you tested in Quality control. To see values for all slices, open the .csv file saved in the Quality Control folder.')\n",
        "  pdResults.set_index(\"image\", inplace=True)\n",
        "  pdResults.head()\n",
        "  print(tabulate(pdResults, headers='keys', tablefmt='psql'))\n",
        "\n",
        "\n",
        "\n",
        "  # ------------- For display ------------\n",
        "  print('--------------------------------------------------------------')\n",
        "  @interact\n",
        "  def show_QC_results(images = os.listdir(Source_folder)):\n",
        "        \n",
        "    visualise_segmentation_QC(image=images,dimension=Data_type, Source_folder=Source_folder, Prediction_folder=Prediction_folder, Ground_truth_folder=Ground_truth_folder, QC_folder=QC_folder, QC_scores=pdResults)  \n",
        "\n",
        "  print('-----------------------------------')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shum2WdMfVLt"
      },
      "source": [
        "# **3. Version log**\n",
        "---\n",
        "<font size = 4>**v1.13**:  \n",
        "\n",
        "\n",
        "*  This version now includes built-in version check and the version log that that you're reading now.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4pcBe8Z3T2J"
      },
      "source": [
        "#**Thank you for using our Quality Control notebook!**"
      ]
    }
  ]
}